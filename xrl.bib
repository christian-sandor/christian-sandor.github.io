% Encoding: UTF-8
@inproceedings{Cassinelli2007,
abstract = {We are developing a modular electronic device to allow users to perceive and respond simultaneously to multiple spatial information sources using haptic stimulus. Each module of this wearable "haptic radar" acts as an artificial hair capable of sensing obstacles, measuring their range and transducing this information as a vibro-tactile cue on the skin directly beneath the module. Our first prototype (a headband) provides the wearer with 360 degrees of spatial awareness thanks to invisible, insect-like antennas. During a proof-of-principle experiment, a significant proportion (87{\%}, p=1.26* 10-5) of participants moved to avoid an unseen object approaching from behind without any previous training. Participants reported the system as more of a help, easy, and intuitive. Among the numerous potential applications of this interface are electronic travel aids and visual prosthetics for the blind, augmentation of spatial awareness in hazardous working environments, as well as enhanced obstacle awareness for motorcycle or car drivers (in this case the sensors may cover the surface of the car). {\textcopyright} 2006 IEEE.},
author = {Cassinelli, Alvaro and Reynolds, Carson and Ishikawa, Masatoshi},
booktitle = {Proceedings - International Symposium on Wearable Computers, ISWC},
doi = {10.1109/ISWC.2006.286344},
isbn = {1424405971},
issn = {15504816},
title = {{Augmenting spatial awareness with haptic radar}},
year = {2007}
}
@inproceedings{Cassinelli2005,
author = {Cassinelli, Alvaro and Ishikawa, Masatoshi},
booktitle = {ACM SIGGRAPH 2005 Emerging Technologies, SIGGRAPH 2005},
doi = {10.1145/1187297.1187308},
title = {{Khronos projector}},
year = {2005}
}
@inproceedings{Cassinelli2005a,
abstract = {The problem of tracking hands and fingers on natural scenes has received much attention using passive acquisition vision systems and computationally intense image processing. We are currently studying a simple active tracking system using a laser diode, steering mirrors, and a single non-imaging detector, which is capable of acquiring three dimensional coordinates in real time without the need of any image processing at all. Essentially, it is a smart rangefinder scanner that instead of continuously scanning over the full field of view restricts its scanning area, on the basis of a real-time analysis of the backscattered signal, to a very narrow window precisely the size of the target. The complexity of the whole setup is equivalent to that of a portable laser-based barcode reader, making the system compatible with wearable computers.},
author = {Cassinelli, {\'{A}}lvaro and Perrin, St{\'{e}}phane and Ishikawa, Masatoshi},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/1056808.1056851},
isbn = {1595930027},
keywords = {3D Acquisition,Active Tracking,Device-less Computer Interaction,Smart Laser Scanner},
title = {{Smart laser-scanner for 3d human-machine interface}},
year = {2005}
}
@inproceedings{Puig2013,
author = {Puig, Jordi and Perkis, Andrew and Hoel, Aud Sissel and Cassinelli, Alvaro},
doi = {10.1145/2542256.2542264},
title = {{A-me}},
year = {2013}
}
@inproceedings{Watanabe2014,
abstract = {Though we live in the era of the touchscreen (tablet PCs and smart phones providing a rigid and flat interface) people and the industry are getting excited about the world of tangible 3D interfaces. This may be explained for two reasons: first, the emergence of cheap vision-based gestural interfaces conquering the space above and below the screen (but without haptic feedback), and second -and perhaps more important for the present discussion - the explosion of the 3D printing industry and the possibility for the end user to not only customise the layout of icons on a screen, but also of designing their own physical, deformable interface from scratch. Mass-produced smartphones could then be seen as bare-bone electronics devices whose shape can be physically augmented, personalised and crafted. Now, in order to introduce DIY techniques in the world of deformable input-output interfaces, it is necessary to provide a generic manufacturing/sensing method for such arbitrarily designed shapes. The goal of this paper is to demonstrate a minimally invasive method (i.e. no wiring) to physically augment rigid tablet PCs or smartphones. By putting a deformable object over the front or rear camera - this 'object' can be part of the smartphone case itself - and by making the inside of the object partially transparent, the complex light reflections can be used to recognise patterns of deformation/grasping and map them to different UI actions. A machine learning algorithm allows object shape and deformation to be designed arbitrarily, bringing the device physical personalisation at a level never reached before, with minimal interference with its original hardware.},
author = {Watanabe, Chihiro and Cassinelli, Alvaro and Watanabe, Yoshihiro and Ishikawa, Masatoshi},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/2559206.2581307},
isbn = {9781450324748},
keywords = {3D interfaces,DIY,Deformable interfaces},
title = {{Generic method for crafting deformable interfaces to physically augment smartphones}},
year = {2014}
}
@inproceedings{Reynolds2008,
abstract = {Meta-perception is both an interaction design concept and the theme of a research group at the University of Tokyo. As a design concept, meta-perception is used to describe experience of novel phenomena made possible by devices that extend the human percepts. As a research group, our goal is to develop methods for capturing and manipulating information that is normally inaccessible to humans and machines. In this paper we describe various displays and devices that exemplify meta-perception. These include: several displays with which the human bodily interacts and wearable haptic devices that act as an extended skin. We reflect upon a design approach which borrows from elements of philosophy and media art to describe a different relationship between humans and technology.},
author = {Reynolds, Carson and Cassinelli, Alvaro and Ishikawa, Masatoshi},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/1358628.1358910},
keywords = {Augmented reality,Displays,Haptics,Input devices},
title = {{Meta-percept ion: Reflexes and bodies as part of the interface}},
year = {2008}
}
@inproceedings{Cassinelli2012,
abstract = {Recent developments in computer vision hardware have popularized the use of (free hand) gestures as well as full body posture as a form of input control in commercial gaming applications. However, the computer screen remains the place where the eyes must be placed at all times. Freeing graphic output from that rectangular cage is a hot topic in Spatial Augmented Reality (SAR). Using static or dynamic projection mapping and 'smart projectors', it is possible to recruit any surface in the surrounding for displaying the game's graphics. The present work introduces an original interaction paradigm building on kinetic interfaces and SAR: in 'Skin Games' the body acts simultaneously as the controller and as the (wildly deformable) projection surface on which to display the game's output. {\textcopyright} 2012 Authors.},
author = {Cassinelli, Alvaro and Angesleva, Jussi and Watanabe, Yoshihiro and Frasca, Gonzalo and Ishikawa, Masatoshi},
booktitle = {ITS 2012 - Proceedings of the ACM Conference on Interactive Tabletops and Surfaces},
doi = {10.1145/2396636.2396690},
isbn = {9781450312097},
issn = {00158259},
keywords = {computer games,dynamic projection mapping,entertainment,gestural control,laser sensing display,proprioception,spatial augmented reality},
title = {{Skin games}},
year = {2012}
}
@inproceedings{Cassinelli2009,
author = {Cassinelli, Alvaro and Ishikawa, Masatoshi},
doi = {10.1145/1665137.1665207},
title = {{Volume slicing display}},
year = {2009}
}
@inproceedings{Cassinelli2010,
abstract = {Smart Laser Projector' (SLP) is a modified laser-based projector capable of displaying while simultaneously using the laser beam (at the same or different wavelength or polarization) as a LIDAR probe gathering information about the projection surface (its borders, 3d shape, relative position and orientation, as well as fine texture and spectral reflectance). This information can then be used to correct perspective warp, perform per-pixel contrast compensation, or even reroute the scanning/projecting path altogether (for tracking, feature discovery or barcode reading for instance). We demonstrate here raster-scan and vector graphics applications on two different prototypes. The first relies on a pair of galvanomirrors, and is used for demonstrating simultaneous tracking and display on the palm of the hand, depth-discriminating active contours (for spatially augmented reality surveying), and interactive games. The other relies on a single 2-axis MEMS mirror working in resonant mode, and is used to demonstrate edge enhancement of printed material and 'artificial fluorescence' - all with perfect projection-to-real-world registration by construction.},
author = {Cassinelli, Alvaro and Zerroug, Alexis and Watanabe, Yoshihiro and Ishikawa, Masatoshi and Angesleva, Jussi},
booktitle = {ACM SIGGRAPH 2010 Emerging Technologies, SIGGRAPH '10},
doi = {10.1145/1836821.1836830},
isbn = {9781450303927},
title = {{Camera-less Smart Laser Projector}},
year = {2010}
}
@inproceedings{Perrin2004,
abstract = {This paper describes a finger gesture recognition system based on an active tracking mechanism. The simplicity of this tracking system is such that it would be possible to integrate the whole system on a chip, making it an interesting input interface for portable computing devices. In this context, recognition of gestural characters allows information to be input in a natural way. The recognition of three dimensional gestures is also studied, opening the way to a more complex interaction mode and to other kinds of applications.},
author = {Perrin, St{\'{e}}phane and Cassinelli, Alvaro and Ishikawa, Masatoshi},
booktitle = {Proceedings - Sixth IEEE International Conference on Automatic Face and Gesture Recognition},
doi = {10.1109/AFGR.2004.1301589},
isbn = {0769521223},
title = {{Gesture recognition using laser-based tracking system}},
year = {2004}
}
@inproceedings{Cassinelli2017,
abstract = {Latest hardware improvements on transceivers supporting Low PowerWide Area Networks (LPWAN) make it feasible to connect small battery powered devices hundred of meters or even km away. In this paper, we propose a physical computing paradigm fully exploiting this novel technology. Spatial physical computing (SPC) leverages not only natural manipulation typically used on TUI and construction kits but also integrates the necessary deambulation (around a building or a city) in the process of creating, testing and tuning a distributed smart sensor/actuator network. The overall system is a compound of (an unlimited) set of independent data-processing nodes supporting an event-driven data-flow programming scheme. We demonstrate in a few examples how such networks can be deployed-spatially programmed-through intuitive physical actions, and discuss the unique qualities and challenges of Spatial Physical Computing.},
author = {Cassinelli, Alvaro and Saakes, Daniel},
booktitle = {TEI 2017 - Proceedings of the 11th International Conference on Tangible, Embedded, and Embodied Interaction},
doi = {10.1145/3024969.3024978},
isbn = {9781450346764},
keywords = {Physical Computing,Programming,Spatial,Tangible},
title = {{Data flow, spatial physical computing}},
year = {2017}
}
@article{Kastrup2017,
abstract = {Background: Visually disabled people increasingly use computers in everyday life, thanks to novel assistive technologies better tailored to their cognitive functioning. Like sighted people, many are interested in computer games – videogames and audio-games. Tactile-games are beginning to emerge. The Tactile Radar is a device through which a visually disabled person is able to detect distal obstacles. In this study, it is connected to a computer running a tactile-game. The game consists in finding and collecting randomly arranged coins in a virtual room. Methods: The study was conducted with nine congenital blind people including both sexes, aged 20–64 years old. Complementary methods of first and third person were used: the debriefing interview and the quasi-experimental design. Results: The results indicate that the Tactile Radar is suitable for the creation of computer games specifically tailored for visually disabled people. Conclusions: Furthermore, the device seems capable of eliciting a powerful immersive experience. Methodologically speaking, this research contributes to the consolidation and development of first and third person complementary methods, particularly useful in disabled people research field, including the evaluation by users of the Tactile Radar effectiveness in a virtual reality context.Implications for rehabilitationDespite the growing interest in virtual games for visually disabled people, they still find barriers to access such games.Through the development of assistive technologies such as the Tactile Radar, applied in virtual games, we can create new opportunities for leisure, socialization and education for visually disabled people.The results of our study indicate that the Tactile Radar is adapted to the creation of video games for visually disabled people, providing a playful interaction with the players.},
author = {Kastrup, Virg{\'{i}}nia and Cassinelli, Alvaro and Qu{\'{e}}rette, Paulo and Bergstrom, Niklas and Sampaio, Eliana},
doi = {10.1080/17483107.2017.1378391},
issn = {17483115},
journal = {Disability and Rehabilitation: Assistive Technology},
keywords = {Tactile Radar,assistive technology,tactile-games,virtual reality,visual disability},
title = {{Tactile Radar: experimenting a computer game with visually disabled}},
year = {2017}
}
@inproceedings{Reynolds2010,
abstract = {Evidence suggests participants in games may evaluate invasive technologies differently. Accordingly, special care ought to be taken with the use of video games as research instruments. We propose a range of methods which help protect participants of video game experiments.},
author = {Reynolds, Carson and Hertrich, Susanna and Cassinelli, Alvaro and Smith, Marshall},
booktitle = {Video Games as Research Instruments Workshop in conjunction with Conference on Human Factors in Computing Systems (CHI 2010)},
title = {{Ethical Aspects of Video Game Experiments}},
year = {2010}
}
@inproceedings{Wilde2012,
abstract = {The Light Arrays project explores the extension of the body through an array of visible light beams projecting on the environment a dynamic representation of the body, its movement and posture. Interestingly, these light cues are visible both for the user wearing the device as well as for others. The result is an experiential bridge between what we see and what we feel or know about the dynamic, moving body. The Light Arrays afford augmented proprioception, generated through the artificial visual feedback system; enhanced body interaction prompted by the interactively augmented body image (in time and space); as well as a clear visual representation of interpersonal and inter-structural | architectural space. {\textcopyright} 2012 Authors.},
author = {Wilde, Danielle and Cassinelli, Alvaro and Zerroug, Alexis},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/2212776.2212367},
isbn = {9781450310161},
keywords = {embodied engagement,light,performative research,soft electronics,wearable technologies},
title = {{Light arrays}},
year = {2012}
}
@inproceedings{Watanabe2008,
abstract = {We propose a variant of the multi-touch display technology that introduces an original way of manipulating three-dimensional data. The underlying metaphor is that of a deformable screen that acts as a boundary surface between the real and the virtual worlds. By doing so, the interface can create the illusion of continuity between the user's real space and the virtual three-dimensional space. The prototype system presented here enables this by employing three key technologies: a tangible and deformable projection screen, a real-time three-dimensional sensing mechanism, and an algorithm for dynamic compensation for anamorphic projection. This paper introduces the concept of the deformable tangible workspace, and describes the required technologies for implementing it. Also, several applications developed on a prototype system are detailed and demonstrated. {\textcopyright} 2008 IEEE.},
author = {Watanabe, Yoshihiro and Cassinelli, Alvaro and Komuro, Takashi and Ishikawa, Masatoshi},
booktitle = {2008 IEEE International Workshop on Horizontal Interactive Human Computer System, TABLETOP 2008},
doi = {10.1109/TABLETOP.2008.4660197},
isbn = {9781424428984},
title = {{The deformable workspace: A membrane between real and virtual space}},
year = {2008}
}
@inproceedings{Steimle2013,
abstract = {This workshop provides a forum for discussing emerging trends in interactive surfaces that leverage alternative display types and form factors to enable more expressive interaction with information. The goal of the workshop is to push the current discussion forward towards a synthesis of emerging visualization and interaction concepts in the area of improvised, minimal, curved and malleable interactive surfaces. By doing so, we aim to generate an agenda for future research and development in interactive surfaces.},
author = {Steimle, J{\"{u}}rgen and Benko, Hrvoje and Cassinelli, Alvaro and Leithinger, Daniel and Maes, Pattie and Poupyrev, Ivan and Ishii, Hiroshi},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/2468356.2479667},
isbn = {9781450318990},
keywords = {Everywhere display,Flexible display,Improvised display,Input,Interactive surface,Minimal display,On-body display,Projected interfaces,Tracking},
title = {{Displays Take New Shape: An Agenda for Future Interactive Surfaces}},
year = {2013}
}
@inproceedings{Cassinelli2006,
abstract = {We present a device whose goal is to allow wearers to perceive and respond to range information from multiple sensors using haptic cues. It uses an array of modules, each of which senses range information and transduces it as vibro-tactile cues on the skin directly beneath the module. Moreover, this modular interface can cover precise skin regions, be distributed in a discrete manner over the skin surface, or span the entire body surface (and then function as a sort of double skin). Among the numerous applications of this interface are visual prosthetics for the blind, augmentation of spatial awareness in hazardous working environments, as well as enhanced obstacle awareness for car drivers. In an experiment, a significant proportion (87{\%}, p=1.26*10 -5) of subjects moved to avoid an unseen object. On a questionnaire, subjects reported the system as more of a help, easy, and intuitive.},
author = {Cassinelli, Alvaro and Reynolds, Carson and Ishikawa, Masatoshi},
booktitle = {ACM SIGGRAPH 2006: Sketches, SIGGRAPH '06},
doi = {10.1145/1179849.1179892},
isbn = {1595933646},
keywords = {Haptics,Sensory substitution,Visual prosthetics},
title = {{Haptic radar/extended skin project}},
year = {2006}
}
@inproceedings{Niikura2010,
abstract = {Recently the miniaturization of mobile devices has progressed and such devices are difficult to have input interface that has wide operation area on their surface. Conventional input interface on a cell phone, such as a touch panel or keypad, has limited operation area. There has been many approaches to handle this problem, but they require users to wear some physical devices[Harrison and Hudson 2009] or to use in some specific environments[Roeber et al. 2003].},
author = {Niikura, Takehiro and Hirobe, Yuki and Cassinelli, Alvaro and Watanabe, Yoshihiro and Komuro, Takashi and Ishikawa, Masatoshi},
booktitle = {ACM SIGGRAPH 2010 Emerging Technologies, SIGGRAPH '10},
doi = {10.1145/1836821.1836836},
isbn = {9781450303927},
title = {{In-air typing interface for mobile devices with vibration feedback}},
year = {2010}
}
@inproceedings{Vaananen-Vainio-Mattila2013,
abstract = {Mobile and ubiquitous systems create opportunities for new kinds of interactivity in public spaces. Examples of human-technology interactions in public spaces include interactive displays on different scales; mobile systems enabling projection in public environments; smart interactive and reactive objects; tangible interfaces; and public media arts. Human-system and mediated human-human interactions become public and visible to the people around the same space. This creates many possibilities and challenges for designing the user experience that arise primarily from the social and physical context. This workshop will bring together researchers, designers, practitioners and media artists to discuss elements and viewpoints of such new forms of experiences. The results of the workshop will be an “experience design space” and a research agenda for experiences with interactive systems used in public spaces.},
author = {V{\"{a}}{\"{a}}n{\"{a}}nen-Vainio-Mattila, Kaisa and H{\"{a}}kkil{\"{a}}, Jonna and Cassinelli, Alvaro and M{\"{u}}ller, J{\"{o}}rg and Rukzio, Enrico and Schmidt, Albrecht},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/2468356.2479665},
isbn = {9781450318990},
keywords = {Interaction,Public spaces,User Experience},
title = {{Experiencing Interactivity in Public Spaces}},
year = {2013}
}
@inproceedings{Zerroug2009,
abstract = {We present here a first prototype of the Virtual Haptic Radar (VHR), a wearable device helping actors become aware of the presence of invisible virtual objects in their path when evolving in a virtual studio (such as a "bluescreen" filming stage [Figure 1]). The VHR is a natural extension of the Haptic Radar (HR) and its principle [Cassinelli et al. 2006] in the realm of virtual reality: while each module of the HR had a small vibrator and a rangefinder to measure distance to real obstacles, the VHR module lacks the rangefinder but accommodates instead a (cheap) ultrasound-based indoor positioning system that gives it the ability to know exactly where it is situated relatively to an external frame of reference.},
author = {Zerroug, Alexis and Cassinelli, Alvaro and Ishikawa, Masatoshi},
booktitle = {ACM SIGGRAPH ASIA 2009 Sketches, SIGGRAPH ASIA '09},
doi = {10.1145/1667146.1667158},
isbn = {9781605588582},
title = {{Virtual haptic radar}},
year = {2009}
}
@phdthesis{Cassinelli2001,
abstract = {The purpose of this work is to highlight the interest in using dedicated optoelectronic stochastic parallel processors (OSPP) within real-time low-level image processing systems. An OSPP (a sort of Smart Pixel Array sometimes called stochastic artificial retina) is composed of a (regular) matrix of simple processing elements (PE) whose binary state depends on a short-range neighborhood. The update of each processing element is made stochastic thanks to a laser speckle based random number generator. A first VLSI prototype was built on standard silicon technology (CMOS 0,8 $\mu$m), providing mesh-like electronic linking between PEs. The optical interconnection issue was explored afterwards and showed it to be a powerful tool for providing both easy extension and rearrangement of the interconnection pattern. Experimental results using a compact prototype demonstrator (21x35x14 cm3) based on a silicon chip, two spatial light modulators and a convolution setup relying on interchangeable Dammann gratings (in charge of the interconnection pattern) enabled us to simulate stochastic relaxation in a lattice of spins (2-D Ising model), and then successfully demonstrate cleaning noise on binary images and motion detection in a sequence of gray level images using stochastic optimization techniques (simulated annealing). Further results on this topic are reported in a separated paper [Cassin01], where a discussion about the advantages of using a hybrid CMOS/SEED Smart Pixel Array to monolithically integrate photodetectors and modulators on the same chip, providing compact, high bandwidth intrachip optoelectronic interconnects can be found. Modelling of this device clearly showed system performance improvement (the prototype should be able to process more than ten thousands images per second while 2 to 5 seconds where needed to process each on the previous prototype). Equally important is the fact that the size of the whole system would be compatible with the standards of electronic packaging technologies.},
author = {Cassinelli, Alvaro},
booktitle = {Nutrition Clinique et M{\'{e}}tabolisme},
issn = {09850562},
keywords = {SIMD,artificial retina,laser,optoelectronics,parallel processor,speckle},
title = {{Processeurs Parall{\`{e}}les Opto{\'{e}}lectroniques Stochastiques pour le traitement d'images en temps r{\'{e}}el}},
year = {2001}
}
@inproceedings{Perrin2003,
abstract = {This paper describes a proof-of-principle demonstration of a novel and simple active tracking mechanism using a laser diode, steering mirrors, and a single non-imaging photodetector. Tracking is based on the analysis of a temporal signal corresponding to the amount of backscattered light produced during a rapid, local circular scan (or saccade) around the presumed object position. The simplicity of the system is such that, using state-of-the-art Micro-Opto-Electro-Mechanical-System (MOEMS) technology, it would be possible to integrate the whole system on a chip, making it an interesting input interface for portable computing devices.},
author = {Perrin, Stephane and Cassinelli, Alvaro and Ishikawa, Masatoshi},
booktitle = {Image and Vision Computing, New Zealand (IVCNZ 2003)},
keywords = {active tracking,human machine interface,moems},
title = {{Laser-Based Finger Tracking System Suitable for MOEMS Integration}},
year = {2003}
}
@inproceedings{Cassinelli2011,
abstract = {We propose a wearable device capable of translating bio-sensed data into cartoon-like graphics projected in the physical surrounding. Such 'expressive Laser Aura' (LA) may serve for biofeedback purposes; but more interestingly, as the display extends past the wearer's personal space it could complement non-verbal social communication by giving others an instant cue about a person's real inner state. In this preliminary work, we explore a proof-of-principle scenario: the possibility of enhancing empathic behavior at the working place, where people have little or no time to explicitly communicate a need for help, or on the contrary a need for isolation. The subject is sitting at the office desk; the LA (presently non-wearable) is projected on a nearby wall or on the floor. It takes the shape of an halo that changes its behavior as a function of the subject stress level. (The LA is also capable of displaying icons and text messages in an autonomous way, or as prompted by the user.) To measure stress, we first tried the commercial 'Mind Flex' head-worn device (combining simple EEG and EMG data). This proved to be too much dependent on test subjects, so we opted for a simpler strategy consisting on measuring physical restlessness by fitting the chair with an accelerometer. The custom laser projector unit measures about 20×20×10cm. The LA graphic output is inspired from cartoon conventions, paranormal folklore and synesthetic considerations. When the subject is relaxed, the halo is nearly circular, uniform and smooth; restlessness disturbs the aura by introducing dynamic spikes. This mapping seems very intuitive - in our preliminary experiment, casual observers never failed to recognize the intended meaning at first glance. When projected on the floor (either from the back of the chair or, in the future, using a laser projection belt [Wilde et al. 2010]), the LA may represent the limits of the personal space. By enlarging it, the subject can claim personal space and signify a need for isolation; on the contrary, while shrinking (or fading or changing its color) the aura may indicate the user readiness to engage in closer social interaction.},
author = {Cassinelli, Alvaro and Zhou, Yuko and Zerroug, Alexis and Ishikawa, Masatoshi},
booktitle = {SIGGRAPH Asia 2011 Posters, SA'11},
doi = {10.1145/2073304.2073330},
isbn = {9781450311373},
title = {{The Laser Aura: A prosthesis for emotional expression}},
year = {2011}
}
@inproceedings{Wang2014,
author = {Wang, Lihui and Cassinelli, Alvaro and Oku, Hiromasa and Ishikawa, Masatoshi},
booktitle = {Novel Optical Systems Design and Optimization XVII},
doi = {10.1117/12.2061659},
isbn = {9781628412208},
issn = {1996756X},
title = {{A pair of diopter-adjustable eyeglasses for presbyopia correction}},
year = {2014}
}
@inproceedings{Reynolds2007,
abstract = {We propose economically autonomous behavior as a novel goal for robotic systems. Currently examples of robotic autonomy are often limited to restricted physical environments such as a factory or road. In this paper we instead restrict the notion of autonomy to a social environment: the economy. We define economically autonomous behavior and describe different levels of independence culminating with hypothetical examples of economically autonomous robotic systems. I.},
author = {Reynolds, Carson and Cassinelli, Alvaro and Ishikawa, Masatoshi},
booktitle = {Workshop on Roboethics in conjunction with IEEE International Conference on Robotics and Automation (ICRA'07)},
title = {{Economically Autonomous Robotic Entities}},
year = {2007}
}
@inproceedings{Puig2013a,
abstract = {A-me is a fictitious memory-evoking apparatus at the intersection of science, art and technology. The system enables users to experience other people's memories as well as store their own by interacting with a volumetric representation (MR) of a human brain. The user retrieves or stores memories (audio traces) by pointing and clicking at precise voxels locations. Triggered by their exploratory action, a story is slowly revealed and recomposed in the form of whispering voices revealing intimate stories. A-me it's a public receptacle for private memories, thus exploring the possibility of a collective physical brain. The installation introduces an original optical see-through AR setup for neuronavigation capable of overlaying a volume rendered MR scan onto a physical dummy head. Implementing such a system also forced us to address technical questions on quality assessment of AR systems for brain visualization. Copyright {\textcopyright} ACM.},
author = {Puig, Jordi and Perkis, Andrew and Hoel, Aud Sissel and Cassinelli, Alvaro},
booktitle = {SIGGRAPH Asia 2013 Art Gallery, SA 2013},
doi = {10.1145/2542256.2542264},
isbn = {9781450326285},
keywords = {Augmented reality,Collective experience,Memory retrieval,Visualization},
title = {{A-me: Augmented memories}},
year = {2013}
}
@inproceedings{Steimle2013a,
abstract = {This workshop provides a forum for discussing emerging trends in interactive surfaces that leverage alternative display types and form factors to enable more expressive interaction with information. The goal of the workshop is to push the current discussion forward towards a synthesis of emerging visualization and interaction concepts in the area of improvised, minimal, curved and malleable interactive surfaces. By doing so, we aim to generate an agenda for future research and development in interactive surfaces.},
author = {Steimle, J{\"{u}}rgen and Benko, Hrvoje and Cassinelli, Alvaro and Ishii, Hiroshi and Leithinger, Daniel and Maes, Pattie and Poupyrev, Ivan},
doi = {10.1145/2468356.2479667},
title = {{Displays take new shape}},
year = {2013}
}
@inproceedings{Cassinelli2004,
author = {Cassinelli, Alvaro and Perrin, Stephane and Ishikawa, Masatoshi},
booktitle = {ACM SIGGRAPH 2004 Posters, SIGGRAPH 2004},
doi = {10.1145/1186415.1186513},
isbn = {1581138962},
title = {{Markerless laser-based tracking for real-time 3D gesture acquisition}},
year = {2004}
}
@inproceedings{Cassinelli2006a,
abstract = {We present a device whose goal is to allow wearers to perceive and$\backslash$nrespond to range information from multiple sensors using haptic$\backslash$ncues. It uses an array of modules, each of which senses range information$\backslash$nand transduces it as vibro-tactile cues on the skin directly$\backslash$nbeneath the module. Moreover, this modular interface can cover$\backslash$nprecise skin regions, be distributed in a discrete manner over the$\backslash$nskin surface, or span the entire body surface (and then function as a$\backslash$nsort of double skin). Among the numerous applications of this interface$\backslash$nare visual prosthetics for the blind, augmentation of spatial$\backslash$nawareness in hazardous working environments, as well as enhanced$\backslash$nobstacle awareness for car drivers. In an experiment, a significant$\backslash$nproportion (87{\%}, p=1:26 � 10�5) of subjects moved to avoid an unseen$\backslash$nobject. On a questionnaire, subjects reported the system as$\backslash$nmore of a help, easy, and intuitive.},
author = {Cassinelli, Alvaro and Reynolds, Carson and Ishikawa, Masatoshi},
booktitle = {SIGGRAPH '06: ACM SIGGRAPH 2006 Sketches},
isbn = {1595933646},
keywords = {haptic,hci,sensors},
title = {{Haptic Radar}},
year = {2006}
}
@article{Shulman2009,
abstract = {The developing academic field of machine ethics seeks to make artificial agents safer as they become more pervasive throughout society. Motivated by planned next-generation robotic systems, machine ethics typically explores solutions for agents with autonomous capacities intermediate between those of current artificial agents and humans, with de- signs developed incrementally by and embedded in a society of human agents. These assumptions substantially simplify the problem of designing a desirable agent and re- flect the near-term future well, but there are also cases in which they do not hold. In particular, they need not apply to artificial agents with human-level or greater capabili- ties. The potentially very large impacts of such agents suggest that advance analysis and research is valuable. We describe some of the additional challenges such scenarios pose for machine ethics.},
author = {Shulman, Carl and Jonsson, Henrik and Tarleton, Nick},
journal = {AP-CAP 2009: The fifth Asia-Pacific computing and philosophy conference, October 1st-2nd, University of Tokyo, Japan, proceedings, ed. Carson Reynolds and Alvaro Cassinelli},
title = {{Machine Ethics and Superintelligence}},
year = {2009}
}
@article{Reynolds2010a,
abstract = {Sensors, actuators, implants, wearable computers, and neural interfaces can do more than simply observe our bodies: these devices can also alter and manipulate our perceptions. This workshop will promote the design and critique of systems whose explicit purpose is to alter human percepts. Participants will be asked to present abstracts, images, videos and demonstrations that focus on devices that shape perceptual phenomena. The goals of the workshop are to: (1) document an emerging field of device design; (2) facilitate the development of these devices by sharing designs; (3) better understand the process of perception and how it informs the design of devices; and (4) debate the aesthetics, perceptual change, social and ethical issues as well as functional transformation the presented works envision for the future.},
author = {Reynolds, Carson and Cassinelli, Alvaro and Hayashi, Tomoko and Wilde, Danielle},
journal = {devices-alter.me},
keywords = {2 input,4,augmented,b,h5,index t erms,information,interaction,interfaces and presentation,m,media art,output devices,perception,phenomenology,prosthetics,reality,sensors,transhumanism},
title = {{Devices that Alter Perception (DAP 2010) 4th CFP}},
year = {2010}
}
@inproceedings{Cassinelli2012a,
abstract = {"scoreLight" and "scoreBots" are two experimental platforms for performative sound design and manipulation. Both are essentially synesthetic interfaces - synesthetic musical instruments - capable of translating free-hand drawings into a sonic language of beats and pitches, all in real time. While scoreLight uses a modified "smart" laser scanner to track the figure's relevant features (in particular contours), scoreBots rely on one or more tiny line-follower robots to do the same. We present here some of our latest experimentations in an informal way. {\textcopyright} 2012 Authors.},
author = {Cassinelli, Alvaro and Manabe, Daito and Perrin, Stephane and Zerroug, Alexis and Ishikawa, Masatoshi},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/2212776.2212373},
isbn = {9781450310161},
keywords = {laser,line-follower robot,musical instrument,physical interaction,sound manipulation,synesthesia},
title = {{scoreLight {\&} scoreBots}},
year = {2012}
}
@article{Puig2013b,
abstract = {Recent advances in neuroimaging over the last 15 years leaded to an explosion of knowledge in neuroscience and to the emergence of international projects and consortiums. Integration of existing knowledge as well as efficient communication between scientists are now challenging issues into the understanding of such a complex subject [Yarkoni et al., 2010]. Several Internet based tools are now available to provide databases and meta-analysis of published results (Neurosynth, Braimap, NIF, SumsDB, OpenfMRI.). These projects are aimed to provide access to activation maps and/or peak coordinates associated to semantic descriptors (cerebral mechanism, cognitive tasks, experimental stimuli.). However, these interfaces suffer from a lack of interactivity and do not allow real-time exchange of data and knowledge between authors. Moreover, classical modes of scientific communication (articles, meetings, lectures.) do not allow to create an active and updated view of the field for members of a specific community (large scientific structure, international work group.). In this view, we propose here to develop an interface designed to provide a direct mapping between neuroscientific knowledge and 3D brain anatomical space. 2013 Copyright held by the Owner/Author.},
author = {Puig, Jordi and Perkis, Andrew and Pinel, Philippe and Cassinelli, Alvaro and Ishikawa, Masatoshi},
doi = {10.1145/2542302.2542327},
isbn = {9781450326346},
journal = {SIGGRAPH Asia 2013 Posters, SA 2013},
title = {{The neuroscience social network project}},
year = {2013}
}
@article{Cassinelli2004a,
abstract = {Plane-to-plane guided-wave-based interconnection modules are proposed as building blocks for scalable optoelectronic multistage interconnection networks (MINs). This approach leads naturally to a MIN paradigm based not on cascading switching stages containing several size-reduced crossbars, as in the shuffle-exchange (SE) networks, but on cascading permutation-reduced crossbars instead, one per stage. The interest of such an architecture lies in the control simplicity and scalability potential. Transparent circuit switching for permutation routing is possible in such an unbuffered "globally switched" multistage interconnection network (GSMIN). Preliminary experiments using fiber-based interconnection modules are presented. Performance analysis and simulation of a buffered GSMIN is also studied for packet routing purposes.},
author = {Cassinelli, Alvaro and Naruse, Makoto and Ishikawa, Masatoshi},
doi = {10.1109/JLT.2004.824385},
issn = {07338724},
journal = {Journal of Lightwave Technology},
keywords = {Circuit switching,Column control,Mechanically reconfigurable optical switch,Optical multistage interconnection network,Packet switching,Parallel optical interconnection,Permutation routing,Stacked optical planar waveguides},
title = {{Multistage network with globally controlled switching stages and its implementation using optical multi-interconnection modules}},
year = {2004}
}
@article{Cassinelli2001a,
abstract = {We report experimental results and performance analysis of a dedicated optoelectronic processor implementing stochastic optimization-based image processing tasks in real time. We first show experimental results using a proof of principle prototype demonstrator based on standard Si-CMOS technology and liquid crystal spatial light modulators; we then elaborate on the advantages of using a hybrid CMOS/SEED Smart Pixel Array to monolithically integrate photodetectors and modulators on the same chip, providing compact, high bandwidth intra-chip optoelectronic interconnects. We have modeled the operation of the monolithic processor, clearly showing system performance improvement. OCIS codes : 200.0200 (optical computing), 200.4650 (optical interconnects), 250.3140 (integrated optoelectronic circuits), 030.6140 (speckle), 100.0100 (image processing), 330.4150 (motion detection), 999.999 : Boltzmann machines, stochastic processors, random number generation, simulated annealing.},
author = {Cassinelli, Alvaro and Chavel, Pierre and Desmulliez, Marc P.Y.},
journal = {Applied Optics},
keywords = {image processing,integrated optoelectronic circuits,optical computing,optical interconnects,speckle},
title = {{Dedicated Optoelectronic Stochastic Parallel Processor ( OSPP ) for real-time image processing : motion detection demonstration and design of a hybrid CMOS / SEED based prototype .}},
year = {2001}
}
@inproceedings{Cassinelli2011a,
abstract = {We propose a wearable device capable of translating bio-sensed data into cartoon-like graphics projected in the physical surrounding. Such 'expressive Laser Aura' (LA) may serve for biofeed-back purposes; but more interestingly, as the display extends past the wearer's personal space it could complement non-verbal social communication by giving others an instant cue about a person's real inner state. In this preliminary work, we explore a proof-of-principle scenario: the possibility of enhancing empathic behavior at the working place, where people have little or no time to explicitly communicate a need for help, or on the contrary a need for isolation. The subject is sitting at the office desk; the LA (presently non-wearable) is projected on a nearby wall or on the floor. It takes the shape of an halo that changes its behavior as a function of the subject stress level. (The LA is also capable of displaying icons and text messages in an autonomous way, or as prompted by the user.) To measure stress, we first tried the commercial 'Mind Flex' head-worn device (combining simple EEG and EMG data). This proved to be too much dependent on test subjects, so we opted for a simpler strategy consisting on measuring physical restlessness by fitting the chair with an accelerometer. The custom laser projector unit measures about 20x20x10cm. The LA graphic output is inspired from cartoon conventions, paranormal folklore and synesthetic considerations. When the subject is relaxed, the halo is nearly circular, uniform and smooth; restlessness disturbs the aura by introducing dynamic spikes. This mapping seems very intuitive - in our preliminary experiment, casual observers never failed to recognize the intended meaning at first glance. When projected on the floor (either from the back of the chair or, in the future, using a laser projection belt [Wilde et al. 2010]), the LA may represent the limits of the personal space. By enlarging it, the subject can claim personal space and signify a need for isolation; on the contrary, while shrinking (or fading or changing its color) the aura may indicate the user readiness to engage in closer social interaction.},
author = {Cassinelli, Alvaro and Zhou, Yuko and Zerroug, Alexis and Ishikawa, Masatoshi},
doi = {10.1145/2073304.2073330},
title = {{The Laser Aura}},
year = {2011}
}
@article{Cassinelli1999,
abstract = {We present a device whose goal is to allow wearers to perceive and$\backslash$nrespond to range information from multiple sensors using haptic$\backslash$ncues. It uses an array of modules, each of which senses range information$\backslash$nand transduces it as vibro-tactile cues on the skin directly$\backslash$nbeneath the module. Moreover, this modular interface can cover$\backslash$nprecise skin regions, be distributed in a discrete manner over the$\backslash$nskin surface, or span the entire body surface (and then function as a$\backslash$nsort of double skin). Among the numerous applications of this interface$\backslash$nare visual prosthetics for the blind, augmentation of spatial$\backslash$nawareness in hazardous working environments, as well as enhanced$\backslash$nobstacle awareness for car drivers. In an experiment, a significant$\backslash$nproportion (87{\%}, p=1:26 � 10�5) of subjects moved to avoid an unseen$\backslash$nobject. On a questionnaire, subjects reported the system as$\backslash$nmore of a help, easy, and intuitive.},
author = {Cassinelli, Alvaro and Reynolds, Carson},
journal = {Biomedical Engineering},
keywords = {haptics,sensory substitution,visual prosthetics},
title = {{Haptic Radar}},
year = {1999}
}
@article{Shulman2009a,
abstract = {Some researchers in the field of machine ethics have suggested consequentialist or util- itarian theories as organizing principles for Artificial Moral Agents (AMAs) (Wallach, Allen, and Smit 2008) that are ‘full ethical agents' (Moor 2006), while acknowledging extensive variation among these theories as a serious challenge(Wallach, Allen, and Smit 2008). This paper develops that challenge, beginning with a partial taxonomy of conse- quentialisms proposed by philosophical ethics. We discuss numerous ‘free variables' of consequentialism where intuitions conflict about optimal values, and then consider spe- cial problems of human-level AMAs designed to implement a particular ethical theory, by comparison to human proponents of the same explicit principles. In conclusion, we suggest that if machine ethics is to fully succeed, itmust draw upon the developing field of moral psychology.},
author = {Shulman, Carl and Jonsson, Henrik and Tarleton, Nick},
journal = {AP-CAP 2009: The fifth Asia-Pacific computing and philosophy conference, October 1st-2nd, University of Tokyo, Japan, proceedings, ed. Carson Reynolds and Alvaro Cassinelli},
title = {{Which Consequentialism ? Machine Ethics and Moral Divergence}},
year = {2009}
}
@article{Puig2018,
abstract = {This article reports on two art-science collaborations, A-me: Augmented Memories and BrainCloud, that interrogate the central role of localization in neuroscience-including the use of technologies that augment sociability using localization as a central reference point. The two projects result from a series of interactions where a science/technology development fostered art, which in turn led to a science application, which potentially may lead to further artistic activity. A-me is an art installation that repurposes navigation and visualization tools normally reserved for medical clinicians and scientists, inviting reflection on the ongoing endeavor of neuroscience to explain and map cognitive functions such as memory. BrainCloud is a software prototype that provides neuroscientists with an interface for interacting with existing data and knowledge about the brain. Organized visually as a brain atlas, it forms a social network that allows neuroscientists to connect and share their ongoing research and ideas.},
author = {Puig, Jordi and Carusi, Annamaria and Cassinelli, Alvaro and Pinel, Philippe and Hoel, Aud Sissel},
doi = {10.1162/LEON_a_01382},
issn = {0024094X},
journal = {Leonardo},
title = {{A-me and braincloud: Art-science interrogations of localization in neuroscience}},
year = {2018}
}
@article{Ishikawa2008,
author = {Ishikawa, Masatoshi and Cassinelli, Alvaro and Reynolds, Carson},
doi = {10.2184/lsj.36.199},
issn = {0387-0200},
journal = {The Review of Laser Engineering},
title = {{Meta Perception}},
year = {2008}
}
@inproceedings{Chavel2001,
author = {Chavel, Pierre H. and Cassinelli, Alvaro and Glaser, I.},
booktitle = {ROMOPTO 2000: Sixth Conference on Optics},
doi = {10.1117/12.432874},
issn = {0277786X},
title = {{Optoelectronic implementation of cellular automata for complex vision algorithms}},
year = {2001}
}
@inproceedings{Naruse2002,
abstract = {A two-dimensional fiber array is proposed for short-distance parallel optical interconnections. In contrast to conventional fiber bundles, interconnection topology is integrated by configuring the spatial position of the input and output end of each fiber.},
author = {Naruse, Makoto and Cassinelli, Alvaro and Ishikawa, Masatoshi},
booktitle = {Conference Proceedings - Lasers and Electro-Optics Society Annual Meeting-LEOS},
doi = {10.1109/leos.2002.1159510},
issn = {10928081},
title = {{Two-dimensional fiber array with integrated topology for short-distance optical interconnections}},
year = {2002}
}
@inproceedings{Parshakova2017,
author = {Parshakova, Tetiana and Cho, Minjoo and Cassinelli, Alvaro and Saakes, Daniel},
doi = {10.1145/3027063.3049778},
title = {{Furniture that Learns to Move Itself}},
year = {2017}
}
@inproceedings{Cassinelli1998,
author = {Cassinelli, Alvaro and Lalanne, Philippe and Chavel, Pierre H. and Glaser, I.},
booktitle = {Optics in Computing '98},
doi = {10.1117/12.308912},
title = {{Demonstration of video-rate optoelectronic parallel processors for noise cleaning in binary images by simulated annealing}},
year = {1998}
}
@article{Ogawa2008,
abstract = {We propose a novel system that enables us to interact with microorganisms in the real world. It is a kind of a new interface linking the macro- and micro-world over the scale. The status of the microorganism is projected onto the avatar robot in the real world, and the user interacts with microorganism through the robot. The experimental system demonstrates the feasibility of human-microorganism interaction.},
author = {Ogawa, Naoko and Kikuta, Kyohei and Oku, Hiromasa and Hasegawa, Takeshi and Cassinelli, Alvaro and Ishikawa, Masatoshi},
issn = {03875806},
journal = {Transactions of Information Processing Society of Japan},
keywords = {特集:メディアインタラクション研究の発展(テクニカルノート)},
title = {{Proposal for Real-world-oriented Interaction System with Microorganisms and Its Preliminary Study}},
year = {2008}
}
@inproceedings{Cassinelli2009a,
author = {Cassinelli, Alvaro and Kuribara, Yusaku and Ishikawa, Masatoshi and Manabe, Daito},
doi = {10.1145/1665137.1665144},
title = {{scoreLight}},
year = {2009}
}
@inproceedings{Ando2008,
author = {Ando, Hideyuki and Cassinelli, Alvaro and XWatanabe, Alvaro},
booktitle = {ACM SIGGRAPH ASIA 2008 Artgallery: Emerging Technologies, SIGGRAPH Asia'08},
doi = {10.1145/1504229.1504265},
isbn = {9781605583747},
title = {{Ghostly images appearing in moving human eyes and still machine eyes}},
year = {2008}
}
@article{Reynolds2011,
abstract = {Sensors, actuators, implants, wearable computers, and neural interfaces can do more than simply observe our bodies: these devices can also alter and manipulate our perceptions. This workshop will promote the design and critique of systems whose explicit purpose is to alter human percepts. Participants will be asked to present abstracts, images, videos and demonstrations that focus on devices that shape perceptual phenomena. The goals of the workshop are to: (1) document an emerging field of device design; (2) facilitate the development of these devices by sharing designs; (3) better understand the process of perception and how it informs the design of devices; and (4) debate the aesthetics, perceptual change, philosophical issues as well as functional transformation the presented works envision for the future.},
author = {Reynolds, Carson and Cassinelli, Alvaro and Auger, James and College, Royal},
journal = {devices-alter.me},
keywords = {2 input,4,augmented,b,h5,i ndex t erms,information,interaction,interfaces and presentation,m,media art,output devices,perception,phenomenology,prosthetics,reality,sensors,transhumanism},
title = {{Devices that Alter Perception ( DAP 2011 ) 5th CFP}},
year = {2011}
}
@article{Cassinelli2001b,
abstract = {We report experimental results and performance analysis of a dedicated optoelectronic processor that implements stochastic optimization-based image-processing tasks in real time. We first show experimental results using a proof-of-principle-prototype demonstrator based on standard silicon–complementary-metal-oxide-semiconductor (CMOS) technology and liquid-crystal spatial light modulators. We then elaborate on the advantages of using a hybrid CMOS–self-electro-optic-device-based smart-pixel array to monolithically integrate photodetectors and modulators on the same chip, providing compact, high-bandwidth intrachip optoelectronic interconnects. We have modeled the operation of the monolithic processor, clearly showing system-performance improvement.},
author = {Cassinelli, Alvaro and Chavel, Pierre and Desmulliez, Marc P. Y.},
doi = {10.1364/ao.40.006479},
issn = {0003-6935},
journal = {Applied Optics},
title = {{Dedicated optoelectronic stochastic parallel processor for real-time image processing: motion-detection demonstration and design of a hybrid complementary-metal-oxide semiconductor– self-electro-optic-device-based prototype}},
year = {2001}
}
@article{Watanabe2010,
author = {Watanabe, Yoshihiro and Cassinelli, Alvaro and Komuro, Takashi and Ishikawa, Masatoshi},
doi = {10.18974/tvrsj.15.2_173},
issn = {1344-011X},
journal = {Transactions of the Virtual Reality Society of Japan},
keywords = {manipulation of virtual data,projection display,tangible screen,three-dimensional sensing,visualization},
title = {{Interactive Display System based on Adaptive Image Projection to a Deformable Tangible Screen}},
year = {2010}
}
@inproceedings{Puig2013c,
abstract = {Recent advances in neuroimaging over the last 15 years leaded to an explosion of knowledge in neuroscience and to the emergence of international projects and consortiums. Integration of existing knowledge as well as efficient communication between scientists are now challenging issues into the understanding of such a complex subject [Yarkoni et al., 2010]. Several Internet based tools are now available to provide databases and meta-analysis of published results (Neurosynth, Braimap, NIF, SumsDB, OpenfMRI...). These projects are aimed to provide access to activation maps and/or peak coordinates associated to semantic descriptors (cerebral mechanism, cognitive tasks, experimental stimuli...). However, these interfaces suffer from a lack of interactivity and do not allow real-time exchange of data and knowledge between authors. Moreover, classical modes of scientific communication (articles, meetings, lectures...) do not allow to create an active and updated view of the field for members of a specific community (large scientific structure, international work group...). In this view, we propose here to develop an interface designed to provide a direct mapping between neuroscientific knowledge and 3D brain anatomical space.},
author = {Puig, Jordi and Perkis, Andrew and Pinel, Philippe and Cassinelli, Alvaro and Ishikawa, Masatoshi},
booktitle = {SIGGRAPH Asia 2013 Posters on - SA '13},
doi = {10.1145/2542302.2542327},
title = {{SIGGRAPH ASIA Posters - The neuroscience social network project}},
year = {2013}
}
@article{Zufferey2012,
abstract = {This work describes our efforts in creating a general object interaction framework for dynamic collaborative virtual environments. Furthermore, we increase the realism of the interactive world by using a rigid body simulator to calculate all actor and object movements. The main idea behind our interactive platform is to construct a virtual world using only objects that contain their own interaction information. As a result, the object interactions are application independent and only a single scheme is required to handle all interactions in the virtual world. In order to have more dynamic interactions, we also created a new and efficient way for human users to dynamically interact within virtual worlds through their avatar. In particular, we show how inverse kinematics can be used to increase the interaction possibilities and realism in collaborative virtual environments. This results in a higher feeling of presence for connected users and allows for easy, on-the-fly creation of new interactions. For the distribution of both the interactive objects and the dynamic avatar interactions, we keep the network load as low as possible. To demonstrate the effectiveness of our techniques, we incorporate them into an existing CVE framework.},
author = {Zufferey, Guillaume and Jermann, Patrick and Dillenbourg, Pierre and F, Ecole Polytechnique and Zook, Matthew and Graham, Mark and Shelton, Taylor and Z{\l}otowski, Jakub and Bleeker, Timo and Bartneck, Christoph and Reynolds, Ryan and Zinman, Aaron and Donath, Judith and Zimmermann, Andreas and Henze, Niels and Righetti, Xavier and Rukzio, Enrico and Zhu, Changyun and Li, Kevin A Kang Kun and Lv, Qin and Shang, Li and Dick, Robert P and Zhang, Shaojie and Banerjee, P Pat and Luciano, Cristian and Zavala, Luis Armer{\'{i}}a and Gallardo, Sara C Hern{\'{a}}ndez and Garc{\'{i}}a-ru{\'{i}}z, Miguel {\'{A}}ngel and Zamith, Marcelo P M and Clua, Esteban W G and Conci, Aura and Montenegro, Anselmo and Leal-toledo, Regina C P and Fluminense, Universidade Federal and Pagliosa, Paulo A and Valente, Luis and Feij, Bruno and Zabulis, Xenophon and Koutlemanis, P and Baltzakis, H and Grammenos, D and Yohn, Denise Lee and Yee-king, Matthew and Confalonieri, Roberto and Jonge, Dave De and Hazelden, Katina and Yarin, Paul and Ishii, Hiroshi and Yankelovich, Nicole and Kaplan, Jonathan and Simpson, Nigel and Yang, Fan and Baber, Christopher and Yang, Christopher C and Chen, Hsinchun and Wactlar, Howard and Combi, Carlo K and Yamakawa, Hisako Kroiden and Xu, Yan Yongchun and Stojanovic, Nenad and Stojanovic, Ljiljana and Cabrera, Ana and Schuchert, Tobias and Mendenhall, Sam and Hu, Vu and Tillery, Paul and Cohan, Joshua and Xin, Min and Xglr, D Q G and Aoki, Paul M and Grinter, Rebecca E and Hurst, Amy and Szymanski, Margaret H and Thornton, James D and Woodruff, Allison and Wyeld, Theodor G and Wu, Qiong and Kazakevich, Maryia and Boulanger, Pierre and Taylor, Robyn and Wu, Andy and Jog, Jayraj and Hoag, Loring Scotty and Mazalek, Ali and World, Walt Disney and Work, Related and Wolfe, Christopher and Graham, T C Nicholas and Phillips, W Greg and Roy, Banani and Kl, Canada and Kk, Canada and Willis, Karl D D and Whittaker, Steve and Swanson, Jerry and Kucan, Jakov and Sidner, Candy and Hirschberg, Julia and White, Sean and Feiner, Steven and Wetzel, Richard and Blum, Lisa and Oppermann, Leif and West, David and Quigley, Aaron and Kay, Judy and Wen, James and Helton, William S and Billinghurst, Mark and Weiser, Mark and Wei, Chen and Marsden, Gary and Wecker, Alan J and Carmel, Mount and Weber, Matthias Michael and Jung, Bernhard and Weaver, Kimberly A and Tech, Georgia and Bremen, D- and Iben, Hendrik Hendrick and Lawo, Michael and Ward, Gregory J and Rubinstein, Francie M and Clear, Robert D and Wang, Robert Y and Wang, Qi and Ding, Xianghua and Lu, Tun and Xia, Huanhuan and Gu, Ning and Wang, By Lihui and Wong, Brian and Wakkary, Ron and Hatala, Marek and Waechter, Christian A L and Pustka, Daniel and Wadley, Greg and Vetere, Frank and Kulik, Lars and Hopkins, Liza and Green, Jonathan Julie and Vyas, Dhaval and Heylen, Dirk and Nijholt, Anton and van der Veer, Gerrit and Voida, Amy and Voida, Stephen and Greenberg, Saul and He, Helen Ai and Vlist, Bram and Niezen, Gerrit and Rapp, Stefan and Hu, Jun and Feijs, Loe and Vincze, D{\'{a}}vid and Kov{\'{a}}cs, Szilveszter and Niitsuma, Mihoko and Hashimoto, Hideki and Korondi, P{\'{e}}ter and G{\'{a}}csi, M{\'{a}}rta and Mikl{\'{o}}si, {\'{A}}d{\'{a}}m and Viller, Stephen and Verhaegh, Janneke and Soute, Iris and Kessels, Angelique and Markopoulos, Panos and Vera, C S N and Varesano, Fabio and Vernero, Fabiana and Informatica, Dipartimento and Varcholik, Paul D and van den Hoven, Elise and Frens, Joep and Aliakseyeu, Dima and Martens, Jean-Bernard and Overbeeke, Kees and Peters, Peter and van Dam, Andries and Valtolina, S and Mazzoleni, P and Franzoni, S and Bertino, E and Vallg{\aa}rda, Anna and Vallg, Anna and Redstr, Johan and Vallance, Michael and Martin, Stewart and Wiz, Charles and Schaik, Prof Paul Van and Vainio, Teija and Sirkkunen, Esa and V{\"{a}}{\"{a}}n{\"{a}}nen-Vainio-Mattila, Kaisa and H{\"{a}}kkil{\"{a}}, Jonna and Cassinelli, Alvaro and M{\"{u}}ller, J{\"{o}}rg and Schmidt, Albrecht and Ursu, Marian F and Groen, Martin and Falelakis, Manolis and Frantzis, Michael and Zsombori, Vilmos and Kaiser, Rene and Uriu, Daisuke and Namai, Mizuki and Tokuhisa, Satoru and Kashiwagi, Ryo and Inami, Masahiko and Okude, Naohito and Urbanowicz, Katarzyna and Universitv, Keio and Underkoffler, John and Ullmer, Brygg and Umakatsu, Atsushi and Ulriksson, J and Ayani, R and Jacob, Robert J K and Dell, Christian and Gill, Claudia and {Toole Jr.}, Cornelius and Wiley, Cole and Dever, Zachary and Rogge, Landon and Bradford, Rachel and Riviere, Guillaume and Sankaran, Rajesh and Liu, Kexi and Freeman, Chase and Wallace, Alvin and Delatin, Michael and Washington, Christian and Reeser, Alex and Branton, Christopher W and Parker, Rod and Ueda, Tetsuya and Hanai, Ayako and Kamei, Keiko and Turnhout, Koen Van and Craenmehr, Sabine and Holwerda, Robert and Menijn, Mike and Zwart, Jan-Pieter and Bakker, Ren{\'{e}} and Tuite, Kathleen and Snavely, Noah and Hsiao, Dun-Yu and Tabing, Nadine and Popovi{\'{c}}, Zoran and Tuikka, Tuomo and Tsinaraki, Chrisa and Skevakis, Giannis and Trochatou, Ioanna and Christodoulakis, Stavros and Tsandilas, Theophanis and Letondal, Catherine and Mackay, Wendy E and Trevisan, Daniela G and Nedel, Luciana P and Macq, Beno{\^{i}}t Benoit and Tost, L Pujol and Economou, Maria and Tompkin, James and Pece, Fabrizio and Shah, Rajvi and Izadi, Shahram and Kautz, Jan and Theobalt, Christian and Tomlinson, Bill and Tolvanen, Juha-pekka and Games, Multiplayer Online and Tolmie, Peter and Benford, Steve and Flintham, Martin and Brundell, Patrick and Adams, Matt and Tandavantij, Nicholas and Row-Farr, Ju and Giannachi, Gabriella and Tolk, Andreas and Shumaker, Randall and Waite, William F and Drive, Jan Davis and Sullivan, Joseph A and Tinapple, David and Ingalls, Todd and Tien, Geoffrey and Atkins, M Stella and Zheng, Bin and Th{\'{o}}risson, Kristinn R and Pennock, Christopher and List, Thor and DiPirro, John and Thoresson, Johan and Thomsen, Mette Ramsgard and Thieme, Anja and Wallace, Jenelle Jayne and Johnson, Paula and McCarthy, John and Lindley, Si{\^{a}}n and Wright, Peter Paul and Olivier, Patrick and Meyer, Thomas D and Tetteroo, Daniel and Tessendorf, B and Roggen, D and Spuhler, M and Stiefmeier, T and Tr{\"{o}}ster, G and Gr{\"{a}}mer, T and Derleth, P and Feilner, M and Terrenghi, Lucia and Dix, Alan and Tax{\'{e}}n, Gustav and Tanriverdi, Vildan and Tanin, Egemen and Zhang, Rui and Tang, Lei and Yu, Zhiwen and Zhou, Xingshe and Wang, Hanbo Heidi Hongmei Haibin and Becker, Christian and Tang, John C and Tanaka, Katsumi and Takeuchi, Yuichiro and Takayama, Leila and Harris, Helen and Go, Janet and Takahashi, Hidenori and Usami, Masashi and Symposium, Ieee International and Reality, Augmented and Sundstedt, Veronica and Chalmers, Alan and Martinez, Philippe and Summary, Workshop and Sumi, Yasuyuki and Mase, Kenji and Subasi, {\"{O}}zge and Su, Norman Makoto and Mark, Gloria and Studer, Corinne and Shave, John and Streitz, Norbert and Stone, Linda M and Erickson, Thomas and Bederson, Benjamin B and Rothman, Peter and Muzzy, Raymond and Stuehmer, Roland and Stoev, Stanislav L and Feurer, Matthias and Ruckaberle, Michael and Stock, Ingo and Stewart, John A and Dumoulin, Sarah and No{\"{e}}l, Sylvie and Stetten, George and Shelton, Damion and Chang, Wilson and Chib, Vikram and Tamburo, Robert and Hildebrand, Daniel and Lobes, Louis and Sumkin, Jules and Steinicke, Frank and Ropinski, Timo and Hinrichs, Klaus and Steimle, J{\"{u}}rgen and Benko, Hrvoje and Leithinger, Daniel and Maes, Pattie and Poupyrev, Ivan and Steel, Peter and Staudek, Tom{\'{a}}{\v{s}} and Machala, Petr and State, Andrei and Keller, Kurtis P and Fuchs, Henry and Stapleton, Christopher B and Smith, Eileen and Hughes, Charles E and Florida, Central and Moshell, J Michael and Stafford-Fraser, Quentin and Robinson, Peter and Staff and Contributors, eLearn Magazine and Sridharan, Srinivas Srikanth Kirshnamachari and Hincapi{\'{e}}-Ramos, Juan David and Flatla, David R and Irani, Pourang and Sparacino, Flavia and Soro, Alessandro and Iacolina, Samuel A and Scateni, Richardo and Uras, Selene and Song, Peng and Goh, Wooi Boon and Fu, Chi-Wing and Meng, Qiang and Heng, Pheng-Ann and Song, Meehae and Elias, Thomas and M{\"{u}}ller-Wittig, Wolfgang and Chan, Tony K Y and Song, Hyunyoung and Grossman, Tovi and Fitzmaurice, George and Guimbreti{\`{e}}re, Fran{\c{c}}ois and Khan, Azam and Attar, Ramtin and Kurtenbach, Gordon and Soga, Asako and Shiba, Masahito and Salz, Jonah and Sodhi, Rajinder and Wilson, Andrew D and Smailagic, Asim and Siewiorek, Dan and Iannucci, Bob and Dahbura, Anton and Bass, Len and Slay, J and Quirchmayr, G and Kurzel, F and Hagenus, K and Slay, Hannah and Thomas, Bruce H and Singh, Vivek and Pirsiavesh, Hamed and Rishabh, Ish and Jain, Ramesh and Singh, Ramesh and Singh, Anubhav Kumar and Silvers, Aaron and Siltanen, Sanni and Aikala, Maiju and Shirehjini, Ali Asghar Nazari and Albayrak, Sahin and Yassine, Abdulsalam and Shin, Dong Hee and Kim, Tayang and Shi, Jane Junhao and Zhang, Mingmin and Pan, Zhigeng and Jimmerson, Glenn and Pearson, Tom and Menassa, Roland and Sheridan, Jennifer G and Maciel, Abel and Roussos, George and Shelley, Tia and Lyons, Leilah and Zellner, Moira and Minor, Emily and Shell, Jeffrey S and Vertegaal, Roel and Cheng, Daniel Derrick and Skaburskis, Alexander W and Sohn, Changuk and Stewart, A James and Aoudeh, Omar and Dickie, Connor and Shaer, Orit and Strait, Megan and Valdes, Consuelo and Feng, Taili and Lintz, Michael and Luyten, Kris and Green, Mark and Seyed, Teddy and Burns, Chris and {Costa Sousa}, Mario and Maurer, Frank and Tang, Anthony and Setkov, Aleksandr and Gouiff{\`{e}}s, Mich{\`{e}}le and Jacquemin, Christian and Servat, David and Drogoul, Alexis and {Seif El-Nasr}, Magy and Aghabeigi, Bardia and Milam, David and Erfani, Mona and Lameman, Beth and Maygoli, Hamid and Mah, Sang and Schwerdtfeger, Bj{\"{o}}rn and Hofhauser, Andreas and Klinker, Gudrun and Schweitzer, J{\"{o}}rg and D{\"{o}}rner, Ralf and Schuhmann, Stephan and Herrmann, Klaus and Rothermel, Kurt and Boshmaf, Yazan and Schrader, Andreas and Carlson, Darren V and Busch, Dominik and Schougaard, Kari Rye and Gr{\o}nb{\ae}k, Kaj and Scharling, Tejs and Schoor, Wolfram and Masik, Steffen and R{\"{u}}diger, Mecke and M{\"{u}}ller, Gerhard and Schenk, Michael and Deter, Andreas and Scholtz, Brenda and Calitz, Andre and Snyman, Irene and Schneider, Bertrand and Pea, Roy and Blikstein, Paulo and Schmitz, Michael and Endres, Christoph and Butz, Andreas and Schmitt, B{\'{e}}n{\'{e}}dicte and Bach, C{\'{e}}dric Cedric and Dubois, Emmanuel and Duranthon, Francis and Schmidt, Matthew and Galyen, Krista and Laffey, James and Ding, Nan and Wang, Xuan Xianhui and Schloss, W Andrew and Stammen, Dale and Schlienger, C{\'{e}}line and Val{\`{e}}s, St{\'{e}}phane and Chatty, St{\'{e}}phane and Schkolne, Steven and Pruett, Michael and Schr{\"{o}}der, Peter and Scherrer, Camille and Pilet, Julien and Fua, Pascal and Lepetit, Vincent and Scher, Steven and Liu, Joyce Jing and Vaish, Rajan and Gunawardane, Prabath and Davis, James Juliet and Schall, Gerhard and Zollmann, Stefanie and Reitmayr, Gerhard and Schafer, George J and Green, Keith Evan and Walker, Ian D and Lewis, Elise and Saupe, Dietmar and Alexa, Marc and Sato, Daisuke and Kobayashi, Masatomo and Takagi, Hironobu and Asakawa, Chieko and Tanaka, Jiro and Santos, Pedro and Gierlinger, Thomas and Machui, Oliver and Stork, Andr{\'{e}} and Sanneblad, Johan and Holmquist, Lars Erik and Russo, Margherita and Ghose, Ruchira and Mattioli, Mauro and Rusdorf, Stephan and Brunnett, Guido and Rowe, Anthony and Birtles, Liam and Ross, James and Rosenbloom, Andrew and Rogers, Yvonne and Scaife, Mike and Harris, Eric and Phelps, Ted and Price, Sara and Smith, Hilary and Muller, Henk L and Randell, Cliff and Moss, Andrew and Taylor, Ian and Staton, Danae and O'Malley, Claire and Corke, Greta and Gabrielli, Silvia and Rogers, Ralph V and Zeigler, Bernard P and Rodrigues, Maria Andr{\'{e}}ia F and Barbosa, Rafael Garcia and Mendon{\c{c}}a, Nabor C and Roden, Timothy and Parberry, Ian and Robinson, Simon and Robert, David and Wistort, Ryan and Gray, Jesse and Breazeal, Cynthia and Rioux, Fran{\c{c}}ois and Bernier, Fran{\c{c}}ois and Laurendeau, Denis and Riek, Laurel D and Riche, Yann and Simpson, Matthew and Rhyne, Theresa-Marie Marie and MacEachern, Alan and Rhee, Youngho and Lee, Jaemin Jaedong Ju-Hwan Jenq-Kuen Juyeon and Repo, Pertti and Riekki, Jukka and Renner, Rebekka S and Velichkovsky, Boris M and Helmert, Jens R and Renevier, Philippe and Nigay, Laurence and Reitinger, Bernhard and Bornik, Alexander and Beichel, Reinhard and Reilly, Derek and MacKay, Bonnie and Reif, Rupert and G{\"{u}}nthner, Willibald A and Reid, Josephine and Hull, Richard and Clayton, Ben and Melamed, Tom and Stenton, Phil and Regenbrecht, Holger and Collins, Jonny and Hoermann, Simon and Rega, Elizabeth A and Sumida, Stuart S and Edwards, Dave and City, Salt Lake and Computer, International and Lew, Jack and Reeves, Stuart and Fraser, Mike and Realinho, Valentim and Rom{\~{a}}o, Teresa and Dias, A Eduardo and Birra, Fernando and Raybourn, Elaine M and Waern, Annika and Ravasio, Pamela and Sch{\"{a}}r, Sissel Guttormsen and Krueger, Helmut and Rauschert, Ingmar and Agrawal, Pyush and Sharma, Rajeev and Fuhrmann, Sven and Brewer, Isaac and Maceachren, Alan and Cai, Guoray and Ramakers, Raf and Sch, Johannes and Rabaey, Jan M and Quek, Francis and Ehrich, Roger and Lockhart, Thurmon and Quah, Chee Kwang and Gagalowicz, Andre and Roussel, Richard and Seah, Hock Soon and Qi, Wen and van Liere, Robert and Kok, Arjan and Huber, Manuel and Bauer, Martin and Puig, Jordi and Perkis, Andrew and Pinel, Philippe and Ishikawa, Masatoshi and Prasolova-Forland, Ekaterina and Lindas, Anders E and Pleuss, Andreas and Vitzthum, Arnd and Hussmann, Heinrich and Plaisant, Catherine and Clamage, Aaron and Hutchinson, Hilary Browne and Druin, Allison and Piumsomboon, Thammathip and Clark, Adrian and Cockburn, Andy and Piper, Ben and Ratti, Carlo and Pier, Marissa D{\'{i}}az and Goldberg, Isaac Rudom{\'{i}}n and Rivera, Daniel and Pichlmair, Martin and Pham, Thai-Lai and Schneider, Georg and Goose, Stuart and Petersen, Marianne Graves and Peternier, Achille and Vexo, Frederic Fr{\'{e}}d{\'{e}}ric and Thalmann, Daniel and Hopmann, Mathieu and Repetto, Matteo and Papagiannakis, George and Davy, Pierre and Lim, Mingyu and Magnenat-Thalmann, Nadia and Barsocchi, Paolo and Fragopoulos, Tasos and Serpanos, Dimitrios and Gialelis, Yiannis and Kirykou, Anna and Peschel, Joshua M and Paulson, Brandon and Hammond, Tracy and Perritaz, Damien and Salzmann, Christophe and Gillet, Denis and Pekkola, Samuli and Pedras, Bernardo F V and Raposo, Alberto B and Santos, Ismael H F and Peck, Evan M and Solovey, Erin T and Patten, James and Pasman, Wouter and Woodward, Charles and Hakkarainen, Mika and Honkamaa, Petri and Hyv{\"{a}}kk{\"{a}}, Jouko and Pascucci, Federica and Setola, Roberto and Pascoe, Jason and Thomson, Kirsten and Parvinen, Petri and Tiainen, Olli and Salo, Jari and P{\"{o}}yry, Essi and Blakaj, Hedon and Parkes, Amanda and Klemmer, Scott R and Lee, Brian and Rosenfeld, Dan and Corporation, Microsoft and Park, Kyoung Shin and Cho, Yongjoo and Park, Soyon and Papagiannis, Helen and Paleari, Marco and Huet, Benoit and Paiva, Paulo Vinicius F and Machado, Liliane S and Valenca, Ana Maria Gondim and de Oliveira, Jauvane C and Paelke, Volker and Nebe, Karsten and Paay, Jeni and Kjeldskov, Jesper and Christensen, Anders and Ibsen, Andreas and Jensen, Dan and Nielsen, Glen and Vutborg, Ren{\'{e}} and Oomen, Johan and Aroyo, Lora and Marchand-Maillet, St{\'{e}}phane and Douglass, Jeremy and Olsson, Thomas and Saari, Timo and Lucero, Andr{\'{e}}s and Arrasvuori, Juha and {Olsen Jr.}, Dan R and Nielsen, S Travis and Parslow, David and Okura, Fumio and Kanbara, Masayuki and Yokoya, Naokazu and Okamoto, Shin and Kawasaki, Hiroki and Iizuka, Hiroyuki and Yokosaka, Takumi and Yonemura, Tomoko and Hashimoto, Yuki and Ando, Hideyuki and Maeda, Taro and Ogawa, Takefumi and Kiyokawa, Kiyoshi and Takemura, Haruo and Ocnarescu, Ioana and Pain, Fr{\'{e}}d{\'{e}}rique and Bouchard, Carole and Aoussat, Am{\'{e}}ziane and Sciamma, Dominique and Occhialini, Valentina and van Essen, Harm and Eggen, Berry and Obrenovic, {\v{Z}}eljko and Nussbaum, Miguel and Susaeta, Heinz and Jimenez, Felipe and Gajardo, Ignacio and Andreu, Juan Jose and Villalta, Marco and Nordlinger, John and Numbering, Page and Norrie, Moira C and Palinginis, Alexios and Signer, Beat and Noma, Haruo and Miyasato, Tsutomu and Kishino, Fumio and Nitsche, Michael and Niewiadomski, Radoslaw and Bevacqua, Elisabetta and Anh, Le Quoc and Obaid, Mohammad and Looser, Julian and Pelachaud, Catherine and Niebling, Florian and Kopecki, Andreas and Becker, Martin and Newton-Dunn, Henry and Nakano, Hiroaki and Gibson, James and Negulescu, Matei and Inamura, Tetsunari and Neal, Lisa and Navab, Nassir and Naukkarinen, Anne and Sutela, Jenna and Botero, Andrea and Kommonen, Kari-Hans and Narayanasamy, Viknashvaran and Wong, Kok Wai and Fung, Chun Che and Depickere, Arnold and Street, South and Narayanan, Dushyanth and Satyanarayanan, Mahadev and Nakakoji, Kumiyo and Yamamoto, Yasuhiro and Atsushi, Aoki and Nakajima, Tatsuo and Satoh, Ichiro and Naef, Martin and Boyd, Cathie and Nacenta, Miguel A and Pinelle, David and Stuckel, Dane and Gutwin, Carl and Murphy, Robin R and Pratt, Kevin S and Burke, Jennifer L and M{\"{u}}ller-Tomfelde, Christian and Paris, C{\'{e}}cile and M{\"{u}}ller-Birn, Claudia and Lehmann, Janeete and Jeschke, Sabine and Muise, Kevin and Mueller-Wittig, Wolfgang and Jegathese, Reginald and Quick, Jochen and Zhong, Yongmin and Mueller, Florian 'Floyd' and Cole, Luke and O'Brien, Shannon and Walmink, Wouter and Mudge, Mark and Arnold, David and Barcelo, Juan Antonio and Beacham, Richard and Mrazovi{\'{c}}, Petar and Pilipovi{\'{c}}, Marko and Volarevi{\'{c}}, Mario and Mihajlovi{\'{c}}, {\v{Z}}eljka and Moussa, Wafaa Abou and Bortolaso, Christophe and Salembier, Pascal and Jessel, Jean-Pierre and Mottola, Luca and Murphy, Amy L and Picco, Gian Pietro and Mossel, Annette and Venditti, Benjamin and Kaufmann, Hannes and Morrison, Gerald D and Morrison, Ann and Mitchell, Peta and Morita, Tomoyuki and Hirano, Yasushi and Kajita, Shoji and Morial, Ernest N and Orleans, New and Mooser, Jonathan and You, Suya and Neumann, Ulrich and Montemayor, Jaime and Farber, Allison and Simms, Sante and Churaman, Wayne and D'Amour, Allison and Mollet, Nicolas and Chellali, Ryad and Brayda, Luca Giulio and Fontaine, Jean-Guy and M{\"{o}}llers, Max and Borchers, Jan and Molinari, Francesco and Moher, Tom and Mohammed-Amin, R K and Levy, R M and Boyd, J E and Moehrmann, Julia and Heidemann, Gunther and Misuraca, Gianluca and Broster, David and Centeno, Clara and Miller, Robert and Evankovich, Carl and Team, Ben Bostwick and Micire, Mark and Desai, Munjal and Courtemanche, Amanda and Tsui, Katherine M and Yanco, Holly A and Merritt, Tim and Kow, Weiman and Ng, Christopher and Mcgee, Kevin and Wyse, Lonce and Merabti, Madjid and Mend{\'{i}}vil, Eduardo Gonz{\'{a}}lez and Sol{\'{i}}s, Roc{\'{i}}o Esmeralda Naranjo and Rios, Horacio and Memarovic, Nemanja and Elhart, Ivan and Langheinrich, Marc and Mehendran, Aravindh and Dewan, Ayush and Soni, Nikhil and Krishna, K Madhava and Meese, Rupert and Ali, Shakir and Thorne, Emily-Clare and Quinn, Anthony and Mortier, Richard and Koleva, Boriana and Pridmore, Tony and Baurley, Sharon and Medeiros, Daniel and Ribeiro, Eduardo and Dam, Peter and Pinheiro, Rodrigo and Motta, Thiago and Loaiza, Manuel and McNely, Brian J and McNamara, Ann and Mania, Katerina and Gutierrez, Diego and Banks, Marty and Healey, Christopher and McKenzie, Frederic D and Garcia, Hector M and Castelino, Reynel J and Hubbard, Thomas W and Ullian, John A and Gliva, Gayle A and McGarry, Ben and Matthews, Ben and Brereton, Margot and McCrickard, D Scott and Chewar, C M and Somervell, Jacob P and Ndiwalana, Ali and McCaw, Caroline and Oliver, Morgan and Leyton and Maya, Isaac and May, Daniel C-M. and Kristensen, Bent Bruun and Nowack, Palle and Mart{\'{i}}nez-Reyes, Fernando and Hern{\'{a}}ndez-Santana, Israel and Marquardt, Nicolai and Hinckley, Ken and Marcu, Gabriela and Tassini, Kevin and Carlson, Quintin and Goodwyn, Jillian and Rivkin, Gabrielle and Schaefer, Kevin J and Dey, Anind K and Kiesler, Sara and Maquil, Val{\'{e}}rie and Psik, Thomas and Wagner, Ina and Mann, Steve and Janzen, Ryan and Huang, Jason and Mancilla-Amaya, Leonardo and San{\'{i}}n, Cesar and Szczerbicki, Edward and Manches, Andrew and O'Malley, Claire and Malik, Shahzad and Laszlo, Joe and Malacria, Sylvain and Lecolinet, Eric and Foni, Alessandro Enrico and Cadi-Yazli, Nedjma and MacIntyre, Blair and Gandy, Maribeth and Dow, Steven and Bolter, Jay David and Moreno, Emmanuel and Hannigan, Brendan and Moraes, Ronei M and Lyra, Olga and Karapanos, Evangelos and Gouveia, R{\'{u}}ben and Barreto, Mary and Nisi, Valentina and Nunes, Nuno J and Zimmerman, John and Forlizzi, Jodi and Lugrin, Jean-Luc and Wiebusch, Dennis and Latoschik, Marc Erich and Strehler, Alexander and Lugmayr, Artur and Risse, Thomas and Stockleben, Bjorn and Kaario, Juha and Laurila, Kari and L{\o}vlie, Anders Sundnes and Lorenzi, David and Vaidya, Jaideep and Chun, Soon and Shafiq, Basit and Nabi, Ghulam and Atluri, Vijayalakshmi and Lok, Benjamin and Naik, Samir and Whitton, Mary and {Brooks Jr.}, Frederick P and Liu, Pangfeng and Lee, Greg C and Lin, Cheng-Yen and Linders, Dennis and Lindeman, Robert W and Lin, Su-hui and Lim, Youn-Kyung and Stolterman, Erik and Tenenberg, Josh and Liikkanen, Lassi A and Kuikkaniemi, Kai and Lievonen, Petri and Ojala, Pauli and Lieberman, Henry and Espinosa, Jos{\'{e}} and Liao, Chunyuan and Tang, Hao and Liu, Qiong and Chiu, Patrick and Chen, Francine and Li, Yang Yuefeng and Zhang, Yanging and Cao, Xiang and Everitt, Katherine and Dixon, Morgan and Landay, James A and Sohn, Timothy and Huang, Steven and Griswold, William G and Leutert, Florian and Herrmann, Christian and Schilling, Klaus and Lehikoinen, Juha and Salminen, Ilkka and Aaltonen, Antti and Huuskonen, Pertti and Lee, Shang Ping Sangyong and Cheok, Adrian David and Soon, James Teh Keng and Lyn, Goah Pae Debra and Jie, Chio Wen and Chuang, Wang and Farbiz, Farzam and Lee, Ryong and Kitayama, Daisuke and Kwon, Yong-Jin and Sumiya, Kazutoshi and Lee, Youngwon and Kim, Gerard J and Lee, Eric and Karren, Thorsten and Kiel, Henning and Wolf, Marius and Dedenbach, Saskia and Gr{\"{u}}ll, Ingo and Leach, Matthew and Benyon, David and Layton-James, Laura and Lawson, Jean-Yves Lionel and Al-Akkad, Ahmad-Amr and Vanderdonckt, Jean and {Laviola Jr}, Joseph J and Keefe, Daniel F and Laurel, Brenda and Lalioti, Vail Vali and Garcia, Christophe and Hasenbrink, Frank and Lai, Danbo and Sourin, Alexei and Labrune, Jean-Baptiste and Kurz, Daniel and Kumaragurubaran, Viswanathan and Kumar, Neeraj and Chilamkurti, Naveen and Park, Jong Hyuk and Kucukyilmaz, Ayse and Sezgin, Tevfik Metin and Basdogan, Cagatay and Kronqvist, Juha and Salmi, Anna and Krogh, Peter Gall and Kristoffersen, Steinar and Ljungberg, Fredrik and Kosola, Heikki and Vuorela, Timo and Palovuori, Karri and Kortbek, Karen Johanne and Korostelev, Michael and Knauth, Kathryn and Bai, Li and ten Koppel, Maurice and Bailly, Gilles and Walter, Robert and Konkel, Miriam and Leung, Vivian and Hu, Catherine and Kollenberg, Tobit and Hermann, Thomas and Neumann, Alexander and Ritter, Helge and Dierker, Angelika and Tews, Tessa-Karina and Koesling, Hendrik and Koh, Eunyee and Kobayashi, Hiroki and Hirose, Michitaka and Fujiwara, Akio and Nakamura, Kazuhiro and Sezaki, Kaoru and Saito, Kaoru and Knoerlein, B and Harders, M and Knoblauch, Daniel and Kuester, Falko and Klopfer, Eric and Zhang, Chuan and Perry, Judy and Sheldon, Josh and Newman, Mark W and Farrell, Ryan and Bilezikjian, Mark and Li, Jack and Lin, James and Klein, Georg and Murray, David and Cheverst, Keith and de S{\'{a}}, Marco and Jones, Matt and Murray-Smith, Roderick and Kimber, Don and Shingu, Jun and Vaughan, Jim and Kim, Sehwan and Coffin, Christopher and H{\"{o}}llerer, Tobias and Kim, David and Hilliges, Otmar and Butler, Alex and Chen, Jiawen Jessie Y C and Oikonomidis, Iason and Kientz, Julie A and Hayes, Gillian R and Abowd, Gregory D and Khelil, Abdelmajid and Shaikh, Faisal Karim and Ayari, Brahim and Suri, Neeraj and Kharbat, Faten and Bull, Larry and Odeh, Mohammed and Khanzada, Tariq J S and Ali, Ali R and Napoleon, Sameh A and Omar, Abbas S and Kelly, Lorelei and Reeder, Sarah and Morse, Susan Coleman and Kelleher, Caitlin and Kauko, Jarmo and Karnik, Abhijit and Mayol-Cuevas, Walterio and Subramanian, Sriram and Karami, Abir-Beatrice and Jeanpierre, Laurent and Mouaddib, Abdel-Illah and Kane, Shaun K and Frey, Brian and Wobbrock, Jacob O and Kan, Alexander and Gibbs, Martin and Ploderer, Bernd and Kalatzis, Nikos and Liampotis, Nicolas and Roussaki, Ioanna and Kosmides, Pavlos and Papaioannou, Ioannis and Xynogalas, Stavros and Zhang, Daqing and Anagnostou, Miltiades and Kabisch, Eric and Williams, Amanda and Dourish, Paul and Junglas, Iris A and Johnson, Norman A and Steel, Douglas J and Abraham, D Chon and Loughlin, Paul Mac and Joseph, Sam and Uther, Maria and Jorissen, Pieter and Wijnants, Maarten and Lamotte, Wim and Jonsson, Martin and Mattsson, Magnus and Jones, Brett and Forysth, David and Bailey, Brian P and Maciocci, Giuliano and Jones, Andrew and Lang, Magnus and Fyffe, Graham and Yu, Xueming and Busch, Jay and Mcdowall, Ian and Bolas, Mark and Debevec, Paul and Jo, Hyungeun and Hwang, Sungjae and Jin, Li and Wen, Zhigang and Jetter, Hans-Christian and Geyer, Florian and Reiterer, Harald and Dachselt, Raimund and Fischer, Gerhard and Groh, Rainer and Haller, Michael and Herrmann, Thomas and Jeon, Myounghoon and Riener, Andreas and Schuett, Jonathan and Walker, Brendan Bruce N and Jennings, Pamela and Jara, Antonio J and Lopez, Pablo and Fernandez, David and Castillo, Jose F and Zamora, Miguel A and Skarmeta, Antonio F and Jankowski, Jacek and Ressler, Snady and Sons, Kristian and Jung, Yvonne and Behr, Johannes and Slusallek, Philipp and Kruk, Sebastian Ryszard and Decker, Stefan and Jamont, Jean-Paul and Occello, Michel and Jaimes, Alejandro and Sebe, Nicu and Gatica-Perez, Daniel and Jaffe, Elliot and Dayan, Aviva and Dekel, Amnon and Planes, Bertrand and Ajaj, Rami and Jacobs, Margot and Worbin, Linda and Girouard, Audrey and Horn, Michael and Zigelbaum, Jamie and Isola, Sara and Fails, Jerry Alan and Irawati, Sylvia and Green, Scott A and Duenser, Andreas and Ko, Heedong and Ip, Barry and Capey, Martin and Imber, James and Volino, Marco and Guillemaut, Jean-Yves and Fenney, Simon and Hilton, Adrian and Ikeda, Sei and Asghar, Zeeshan and Hyry, Jaakko and Pulli, Petri and Pitkanen, Antti and Kato, Hirokazu and Haumann, Hannes and Ruthenbeck, Carman and Klug, Tobias and Hvannberg, Ebba Thora and Halld{\'{o}}rsd{\'{o}}ttir, Gyda and Rudinsky, Jan and Hutter, Hans-Peter and M{\"{u}}ggler, Thomas and Jung, Udo and Hutchison, David and Mitchell, John C and Hurter, Christophe and Lesbordes, R{\'{e}}mi and Vinot, Jean-Luc and Conversy, St{\'{e}}phane and Edouard, Paul and Gaits, Vincent and Nadfaoui, Hasna and Pailler, J{\'{e}}rome and Hunter, Seth and Kalanithi, Jeevan and Merrill, David and Hung, Pham Phuoc and Bui, Tuan-Anh and Morales, Mauricio Alejandro G{\'{o}}mez and Nguyen, Mui Van and Huh, Eui-Nam and Geelhoed, Erik and Hui, Pan and Pi{\'{o}}rkowski, Michal and Henderson, Tristan and Crowcroft, Jon and Huerta-Canepa, Gonzalo and Lee, Dongman and Huang, Weidong and James, Craig and Alem, Leila and Widzyk-Capehart, Eleonora and Haustein, Kerstin and Hua, Hong and Gao, Chunyu and Hu, Wenjun and Gu, Hao and Pu, Qifan and Hosoi, Kazuhiro and Dao, Vinh Ninh and Mori, Akihiro and Sugimoto, Masanori and Hoseini-tabatabaei, Seyed Amir and Gluhak, Alexander and Tafazolli, Rahim and Hornecker, Eva and Horiuchi, Toshiharu and Sankoh, Hiroshi and Kato, Tsuneo and Naito, Sei and Hoppen, Martin and Schluse, Michael and Ro{\ss}mann, J{\"{u}}rgen and Weitzig, Bj{\"{o}}rn and Hong, Jason I and Holman, David and Altosaar, Mark and Troje, Nikolaus and Johns, Derek and Hohl, Fritz and Kubach, Uwe and Leonhardi, Alexander and Schwehm, Markus and Hoang, Thuong N and Porter, Shane R and Ho, Karen and Weng, Hanley and Hinze, Annika and Bainbridge, David and Hindmarsh, Jon and Heath, Christian and vom Lehn, Dirk and Cleverly, Jason and Weiss, Malte and Heuveline, Vincent and Hertz, Garnet and Herschel, Melanie and Manolescu, Ioana and Henrysson, Anders and Ollila, Mark and Hengeveld, Bart and Hummels, Caroline and van Balkom, Hans and Voort, Riny and de Moor, Jan and Helfer, Barb and Cunningham, Steve and McGrath, Mike and Rosenblum, Larry and Bax, Ingo and Bekel, Holger and Hecht, Louis and Buehler, Greg and Johnston, Doug and Moeller, John and Hecht, Brent and Sch{\"{o}}ning, Johannes and Priedhorsky, Reid and Hauswiesner, Stefan and Straka, Matthias and Hartmann, Bj{\"{o}}rn and Follmer, Sean and Ricciardi, Antonio and Cardenas, Timothy and Hartman, Nathan W and Connolly, Patrick E and Gilger, Jeffrey W and Bertoline, Gary R and Heisler, Justin and Harrison, Steve and Haro, B{\'{a}}rbara Paola Muro and Santana, Pedro C and Maga{\~{n}}a, Martha A and Harboe, Gunnar and Harada, Yutaka and Nazir, Napoleon and Shiote, Yoshinori and Ito, Tomotaka and Halvey, Martin and Azzopardi, Leif and Halskov, Kim and Dalsg{\aa}rd, Peter and Halln{\"{a}}s, Lars and Johan, Redstr{\"{o}}m and Hakulinen, Lasse and Haffegee, Adrian and Ramsamy, Priscilla and Jamieson, Ronan and Alexandrov, Vassil and Anthes, Christoph and Gwilt, Ian and Gueddana, Sofiane and Roussel, Nicolas and Gualtieri, Lisa and Gr{\"{u}}ter, Barbara and M{\"{u}}gge, Holger and Grudin, Jonathan and Grubert, Jens and Grasset, Rapha{\"{e}}l and Wigdor, Daniel and Balakrishnan, Ravin and Gross, Markus and W{\"{u}}rmlin, Stephan and Lamboray, Edouard and Spagno, Christian and Kunz, Andreas and Koller-Meier, Esther and Svoboda, Tomas and van Gool, Luc and Lang, Silke and Strehlke, Kai and van de Moere, Andrew and Staadt, Oliver and Grosch, Thorsten and Eble, Tobias and Mueller, Stefan and Greengard, Samuel and Chase, J Geoffrey and Chen, XiaoQi and Graham, Jamey and Moraleda, Jorge and Hull, Jonathan J and Bailloeul, Timothee and Liu, Xu and Mariotti, Andrea and Go{\ss}mann, Joachim and Specht, Marcus and Gordon, Steven C "Flash" and Gogouvitis, Spyridon V and Kousiouris, George and Konstanteli, Kleopatra and Polychniatis, Theodoros and Menychtas, Andreas and Kyriazis, Dimosthenis and Varvarigou, Theodora and Goebbels, Gernot and G{\"{o}}bel, Martin and Glinert, Eitan and Gl{\"{a}}ser, Thomas and Franke, Jens and Wintergerst, G{\"{o}}tz and Jagodzinski, Ron and Ginige, Athula and Romano, Marco and Sebillo, Monica and Vitiello, Giuliana and di Giovanni, Passquale and Gilroy, Stephen W and Cavazza, Marc and Chaignon, Remi and Gillette, Daniel and Cassell, Justine and el Kaliouby, Rana and Strickland, Dorothy and Weiss, Patrice (Tamar) and Gill, Zann and Giese, Holger and Kindler, Ekkart and Klein, Florian and Wagner, Robert and Ghiletiuc, Johannes and F{\"{a}}rber, Markus and Br{\"{u}}derlin, Beat and Gentile, Anna Lisa and Lanfranchi, Vitaveska and Mazumdar, Suvodeep and Ciravegna, Fabio and Gellersen, Hans W and Fischer, Carl and Guinard, Dominique and Gostner, Roswitha and Kortuem, Gerd and Kray, Christian and Streng, Sara and Beigl, Michael and Garzotto, Franca and Forfori, Matteo and Garrahan, Deirdre and Piplica, Andreya and Gold, Kevin and Galantay, Roderick and Torpus, Jan and Engeli, Maia and Galambos, P{\'{e}}ter and Weidig, Christian and Baranyi, P{\'{e}}ter and Aurich, Jan C and Hamann, Bernd and Kreylos, Oliver and Fukuchi, Kentaro and Jo, Kazuhiro and Fuhrmann, Anton and Grijller, Eduard and Freeman, R and Steed, Anthony and Freed, Natalie and Qi, Jie and Sylla, Cristina and Branco, Pedro and Cater, Kirsten and Duff, Paul and Francken, Yannick and Huysmans, Johan and Bekaert, Philippe and Forte, Maurizio and Pietroni, Eva and Rufa, Claudio and Bizzarro, Angela and Tilia, Alessandro and Tilia, Stefano and Foote, Jonathan and Fonseca, David and Ramzan, Naeem and Kurti, Arianit and Pileggi, Salvatore Flavio and Olwal, Alex and Hogge, Akimitsu and Fjeld, Morten and Fredriksson, Jonas and Ejdestig, Martin and Duca, Florin and B{\"{o}}tschi, Kristina and Voegtli, Benedikt and Juchli, Patrick and Fisher, Brian and Fels, Sidney and MacLean, Karon and Munzner, Tamara and Rensink, Ronald and Giaccardi, E and Ye, Y and Sutcliffe, A G and Mehandiev, N and Fikar, Peter and Schoenauer, Christian and Fields, Robert and Patern{\`{o}}, Fabio and Santoro, Carmen and Tahmassebi, Sophie and Felismino, Mar{\'{i}}lia and Luiz, Ricardo and Castela, Marta and Marques, Pedro and Felger, Wolfgang and Fahlen, Lennart E and Loftin, R Bowen and Macedonia, Michael R and Singh, Gurminder and Ganapathy, S Kicha and Lanier, Jaron and Levin, Golan and White, Duffie and Pingali, Gopal and Fechteler, P and Hilsmann, A and Eisert, P and Broeck, S V and Stevens, C and Wall, J and Sanna, M and Mauro, D A and Kuijk, F and Mekuria, R and Cesar, P and Monaghan, D and O'Connor, N E and Daras, Petros and Alexiadis, Dimitrios S and Zahariadis, T and Favre-Brun, Aur{\'{e}}lie and de Luca, Livio and Caye, V{\'{e}}ronique and Faste, Haakon and Rachmel, Nir and Essary, Russell and Sheehan, Evan and Farringdon, Jonny and Oni, Vanessa and Fantauzzacoffin, Jill and Candy, Linda and Chenzira, Ayoka and Edmonds, Ernest and England, David and Schiphorst, Thecla and Tanaka, Atau and Falk, Jennica and Bj{\"{o}}rk, Staffan and Ernst, Andreas and Papst, Anton and Ruf, Tobias and Garbas, Jens-uwe and Eriksson, Eva and Hansen, Thomas Riisgaard and Lykke-Olesen, Andreas and Emond, Bruno and Ehrlich, Kate and Henderson, Austin and Ehnes, Jochen and Hirota, Koichi and Edwards, Liz and Dean, Graham and Mullagh, Louise and Blair, Gordon and Edge, Darren and Searle, Elly and Chiu, Kevin and Zhao, Jing and Economou, Daphne and Gavalas, Damianos and Kenteris, Michael and Tsekouras, George E and Echtler, Florian and Nestler, Simon and Dippon, Andreas and H{\"{a}}ussler, Maximilian and Gudrun, Klinker and Eber, Dena and Betz, Brian and Crockett, Tobey and Dutoit, D and Guthmuller, E and Miro-Panades, I and Mansoux, Beno{\^{i}}t and Scapin, Dominique L and Masserey, Guillaume and Viala, Jo{\"{e}}l and Drozd, Adam and Tandavanitj, Nick and Wright, Matthew Michael and Chamberlain, Alan and Drettakis, George and Dragone, Mauro and Holz, Thomas and O'Hare, Gregory M P and Mehta, Manish and Mateas, Michael and Harmon, Ellie and Oezbek, Christopher and Doursat, Ren{\'{e}} and Ulieru, Mihaela and Dong, Suyang and Kamat, Vineet R and Doherty, Gavin and Sharry, John and Bang, Magnus and Alca{\~{n}}iz, Mariano and Ba{\~{n}}os, Rosa and DiVerdi, Stephen and Schreyer, Richard and Dietz, Paul and Raskar, Ramesh and Booth, Shane and van Baar, Jeroen and Wittenburg, Kent and Knep, Brian and D{\'{i}}az, Oscar and Arellano, Crist{\'{o}}bal and Puente, Gorka and Diaz, Marissa and Rudomin, Isaac and Dhar, Subhankar and Varshney, Upkar and Desanctis, Gerardine and Jiang, Lu and Denef, Sebastian and Ramirez, Leonardo and Dyrks, Tobias and Schwartz, Tobias and Demeure, Alexandre and Calvary, Ga{\"{e}}lle and Demeulemeester, Aljosha and Kilpi, Katriina and Elprama, Shirley A and Lievens, Sammy and Hollemeersch, Charles-Frederik and Jacobs, An and Lambert, Peter and de Walle, Rik Van and Debackere, Boris and Shamma, David A and Churchill, Elizabeth F and de Amicis, Raffaele and Conti, Giuseppe and Fiorentino, Michele and de Alwis, Brian and Dangelmaier, Wilhelm and Fischer, Matthias and Huber, Daniel and Laroque, Christoph and S{\"{u}}{\ss}, Tim and Dang, Chi Tai and Andr{\'{e}}, Elisabeth and Damiano, Rossana and Gena, Cristina and Lombardo, Vincenzo and Nunnari, Fabrizio and Damala, Areti and Cubaud, Pierre and Bationo, Anne and Houlier, Pascal and Marchal, Isabelle and Dalsgaard, Peter and Cunningham, Andrew and Close, Ben and Hutterer, Peter and Cuccurullo, Stefania and Francese, Rita and Murad, Sharefa and Passero, Ignazio and Tucci, Maurizio and Crease, Murray and Coutrix, C{\'{e}}line and Cotting, Daniel and Costanza, Enrico and Inverso, Samuel A and Pavlov, Elan and Allen, Rebecca and Costa, Carlos J and Apar{\'{i}}cio, Manuela and Corrales, J A and Candelas, F A and Torres, F and Coors, Volker and Collins, Karen and Kanev, Kamen and Kapralos, Bill and Cohen, Philip R and McGee, David R and Coffey, Dane and Malbraaten, Nicholas and Le, Trung and Borazjani, Iman and Sotiropoulos, Fotis and Cocciolo, Anthony and Rabina, Debbie and Clark, Brendon and Ciger, Jan and Gutierrez, Mario and Dray, Susan and Elliott, Ame and Larvie, Patrick and Siegel, David and Churcher, Neville and Irwin, Warwick and Kriz, Ron and Church, Karen and Augusta, Via and Smyth, Barry and Chui, Yim-Pan and Christensen, Ulrik and Chow, Jonathan and Feng, Haoyang and Amor, Robert and W{\"{u}}nsche, Burkhard C and Choi, Stephen H and Ch'ng, Eugene and Chipman, Gene and Guha, Mona Leigh and Beer, Dianne and Chi, Pei-Yu Peggy and Linder, Jason and Dontcheva, Mira and Li, Wilmot and Koh, Jeffrey Tze Kwan Valino and Peiris, Roshan Lalintha and Fernando, Owen Noel Newton and Cheng, Li-Te and de Souza, Cleidson R B and Hupfer, Susanne and Patterson, John and Ross, Steven and Kwak, Taeil and Chen, Vivian Hsueh-hua and Duh, Henry Been-Lirn and Kolko, Beth and Whang, Leo Sang-Min and Fu, Michael Ching-Hui and Chen, Sicheng and Chen, Miao Milton and Yanta{\c{c}}, Asim Evren and Bergmark, Mathias and Sundin, Anders and Barnes, Michael J and Chehimi, Fadi and Coulton, Paul and Edwards, Reuben and Chattopadhyay, Siddhartha and Bhandarkar, Suchendra M and Charfi, Syrine and Chan, Leith Hin Yip and Kenderdine, Sarah and Shaw, Jeffrey and Farr, Ju Row and Marshall, Joe and Rodden, Tom and Chalmers, Matthew and MacColl, I and Bell, M and Chaboissier, Jonathan and Vernier, Fr{\'{e}}d{\'{e}}ric and Cercone, Nick and Yasif, Amir and Cavagna, Romain and Abdallah, Maha and Buyukkaya, Eliya and Bouville, Christian and Carrino, Stefano and P{\'{e}}clat, Alexandre and Mugellini, Elena and Khaled, Omar Abou and Ingold, Rolf and Caon, Maurizio and Yue, Yong and Andreoni, Giuseppe and Callaway, Charles and Stock, Oliviero and Dekoven, Elyon and Noy, Kinneret and Citron, Yael and Dobrin, Yael and Cakmakci, Ozan and Ha, Yonggang and Rolland, Jannick P and Cadiz, J J and Czerwinski, Mary and Mccrickard, Scott and Stasko, John and Caball{\'{e}}, Santi and Mora, N{\'{e}}stor and Dunwell, Ian and Ga{\~{n}}an, David and Buur, Jacob and Fraser, Euan and Oinonen, Soila and Rolfstam, Max and Hodges, Steve and Molyneaux, David and Kong, Danny and B{\"{u}}scher, Monika and Kramp, Gunnar and Bunt, Brogan and Bucolo, Sam and Sickinger, David and Br{\"{u}}kheimer, Alessandro Diogo and Hounsell, Marcelo Da Silva and Soares, Ant{\^{o}}nio Vin{\'{i}}cius and Brown, Barry and Perry, Mark and Browall, Carolina and Lindquist, Kristina and Brodersen, Christina and Kristensen, Jannie Friis and Brinkman, Willem-Paul and Gorini, Alessandra and Gaggioli, Andrea and Neerincx, Mark and Breslin, John G and Burg, Thomas N and Kim, Hong-Gee and Raftery, Tom and Schmidt, Jan-Hinrik and Donovan, Jared and Bratteteig, Tone and Boyle, Michael and Edwards, Christopher and Bott, Jared N and Crowley, James G and Bort, Heather and Brylow, Dennis and Booth, Thomas and Grimm, Cindy and Bailey, Reynold and Booker, John E and Bonomi, Flavio and Milito, Rodolfo and Zhu, Jiang and Addepalli, Sateesh and Bongers, Bert and Bolliger, Philipp and K{\"{o}}hler, Moritz and R{\"{o}}mer, Kay and Birnholtz, Jeremy and Bi, Nanyi and Fussell, Susan and Bimber, Oliver and Bilandzic, Mark and Bernstein, Mark and Bernardes, Jo{\~{a}}o and Nakamura, Ricardo and Calife, Daniel and Tokunaga, Daniel and Tori, Romero and Beristain, Joseba and Arenaza, Xabier and Prieto, I{\~{n}}aki and Bennes, Lionel and Bazzaro, Florence and Sagot, Jean-Claude and Schn{\"{a}}delbach, Holger and Anastasi, Rob and Greenhalgh, Chris and Ghali, Ahmed and Gaver, Bill and Boucher, Andy and Pennington, Sarah and Bec, R{\'{e}}mi and Beaudouin-Lafon, Michel and Bayoumi, Sahar and Bau, Olivier and Basu, Aryabrata and Raij, Andrew and Johnsen, Kyle and Barca, Jan Carlo and Li, Raymond Koon and Rodridges, Maria Andr{\'{e}}ia Formico and Barba, Evan and Ban{\^{a}}tre, Michel and Couderc, Paul and Menaud, Jean-Marc and Weis, Fr{\'{e}}d{\'{e}}ric and Baljko, Melanie and Tenhaaf, Nell and Bakopoulos, Menelaos and Tsekeridou, Sofia and Giannaka, Eri and Tan, Zheng-Hua and Prasad, Ramjee and Bakker, Saskia and Antle, Alissa N and Bahl, Paramvir and Philipose, Matthai and Zhong, Lin and Han, Richard Y and Li, Li Erran and Atzenbeck, Claus and Asuncion, Hazeline and Socha, David and Sung, Kelvin and Berfield, Scott and Gregory, Wanda and Asteriadis, Stylianos and Chatzitofis, Anargyros and Zarpalas, Dimitrios and Assenmacher, I and Hentschel, B and Ni, C and Kuhlen, T and Bischof, C and Ashoori, Maryam and Miao, Chunyan and Cai, Yundong and Rushmeier, Holly and Ikeuchi, Katsushi and Scopigno, Roberto and Ardito, Carmelo and Buono, Paolo and Costabile, Maria F and Ardaiz, Oscar and Arroyo, Ernesto and Righi, Valeria and Galimany, Oriol and Blat, Josep and Aoki, Kota and Sakuraba, Yoshihiko and Nagahashi, Hiroshi and Anderson, Ian and Maitland, Julie and Sherwood, Scott and Barkhuus, Louise and Hall, Malcolm and Anabuki, Mahoro and Ampatzoglou, Apostolos and Kritikos, Apostolos and Arvanitou, Elvira-Maria and Gortzis, Antonis and Chatziasimidis, Fragkiskos and Stamelos, Ioannis and Alexanderson, Petter and Tollmar, Konrad and {Al Moubayed}, Samar and Edlund, Jens and Beskow, Jonas and Agostini, A and Albolino, S and Boselli, R and Michelis, G De and Paoli, F De and Dondi, R and Adams, Anne and Fitzgerald, Elizabeth and Priestnall, Gary and Mynatt, Elizabeth D and Abou-Zahra, Shadi and Aberg, Johan},
doi = {10.1145/2212776.2212459},
isbn = {9781450310154},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {"1-3 The Swarm at the Edge of the Cloud – A New P,- collaborative virtual environments,- social networking,- virtual reality,-component,-embedded multiagent system,-virtual reality,000,0004,06,1,1 introduction and motivation,1 virtual guide simulation,10,13,15,19,1997,2,2 constructivist epistemologies,20,2000,2003,2006,27,29,2lip,3,3 group and organization,32,360,3D Curve Generation,3d,3d applications,3d digital model,3d environments,3d from video,3d gis {\'{a}},3d graphics,3d hypermedia,3d interaction,3d interaction techniques,3d mobile games,3d modeling,3d object manipulation,3d rendering of,3d shape representation,3d sound,3d video,3d web,3d-gis,3d-interaction,3dui,5,5 computer applications,7,8,9,Asymmetry,Awareness,Biological,Collabor,Comma,Computer Aided Styling (CAS),Computer Simulation,Computer Systems,Cybernetics,Cybernetics: methods,DOM Integration,Data Display,Declarative 3D,Down syndrome,Environment,HTML5,Humans,Imaging,Man-Machine Systems,Media Space,Mobile-Communication,Mobile-Integration,Models,Online Systems,Polyfill,Reciprocity,Screen-camera links,Three-Dimensional,Three-Dimensional: methods,Touch,Touch: physiology,UIST,Ubicomp as a field,User-Computer Interface,Video Annotation,Visual Emergency Language,a,a block is in,a book if she,a coherent and historically,a descriptive,a part of working,a project has started,a projector is used,a strong impression on,a system,abilities,able to,abstract and,accelerometer,accessible design,accessible user interface,accessible video game,according to their view,accountability,acm 1-59593-298-4,acm classification keywords,action,actionable research,activity recognition,actuated interface,actuators,ad hoc,adaptation {\'{a}} wi-fi fingerprinting,adaptive context- aware learning,adaptive gesture recognition,adaptive interfaces,adaptive pixel resolution,advance reservation,advanced,aerial omnidirectional image,aesthetic design,affective computing,agents {\'{a}} clustering {\'{a}},agile development,aid effectiveness,aims to explore the,air hockey,air traffic control,alice,ality,all or part of,allocation,also,alternate reality,alternate reality games,alternative gaming,ambient content,ambient devices,ambient display,ambient media,amusement,analysed based on the,analytics,anastakis et al,ancient egypt,and,and 3d display,and activity,and computer science,and computers causes,and context-aware,and desktop boxes,and entertainment,and evaluation,and even public toilets,and fatigue avoidance,and finally,and task modeling in,and this,and using,and virtual environments,and was designed especially,anderson,animatics,animation,annotated photograph of the,anthropology,any activity are not,any heritage site,appliance coordination,applications is a,april 22,architecture,architectures,are,are some,art,art exploration,art installation,artist,artistic motivation,artistic shape creation,arts and humanities,as a fundamental unit,as early,assembly,assistive technologies,assistive technology,asur notation,asynchronous audio,attention,attention estimation,attentive user interfaces,audience interaction,audio,audio design,audio-augmented environments,auditory graph,aug-,augmented,augmented and mixed reality,augmented image synthesis,augmented paper,augmented paper map,augmented re-,augmented real-,augmented realities,augmented reality,augmented reality based virtual,augmented telepresence,augmented web browser,augmented web space,authoring tools,autism,automotive,avatar animation,awareness,background of mobile game,barcode,bare-hand interaction,bbs,be able to,be acquired by using,be focused on the,been proposed concentrate more,benchmark,benford {\'{a}} p,between players and supports,blender,blogging,blue-c,blue-sky research,bml,body as interaction device,body of research into,both resulting in,brainstorming,business process modelling,but distributed,but the,but they also become,button the user moves,by pointing the wand,ca,cad,calibration,calibration chart tracker,california,calm computing,camera,camera pose relocalization,can come in many,can enhance the experience,canada,capture,capture and access,car engine,cas,cast spells,cellular positioning,centralized,century,cependant la conception de,cgi,ch,chamberlain,change our appearance,changes from,chat,chemistry education,chi 2006,children,children collaboration,chinese calligraphy brush,churchill,cities buildings and furniture,classification,classroom,cloud,cloud computing,cloud computing {\'{a}} data,club,co-design,co-presence,co-work,cog-,cognitive constructivists typically see,cognitive modeling,cognitive rehabilitation,cohesion,collab-,collaboration,collaboration over a shared,collaborative,collaborative architecture,collaborative learning,collaborative post-processing,collaborative sys-,collaborative virtual environments,collaborative working,collada,collective decision making,collision,collocated collaboration,color blending,color calibration,color correction,color feature matching,color invariance,command post,common artefact,communication,communities,comp history,competition,component-based development,composite,composition,computational thinking,computer,computer aided styling,computer as material {\'{a}},computer based treatment,computer game,computer games,computer games course,computer graphics education,computer science education,computer simulation games,computer support tools,computer supported collaborative cooking,computer vision,computer vision with,computer-,computer-based cooperative work,computer-mediated communication,computer-supported cooperative care,computing,computing frameworks {\'{a}} interactive,computing {\'{a}} surveying,computing {\ae}mixed reality,conception participative,conceptual framework of activity,conducting,confab,configurator,connectedness,consequently,constructivism,consumer,context,context aware,context awareness,context awareness {\'{a}} community,context sensing,context {\'{a}},context-aware,context-aware applications,context-aware computing,context-aware information,context-awareness,context-awareness {\'{a}} identification,context-awareness {\'{a}} ubiquitous,cooperating,cooperation,cooperative,coordination,copernicus,copyright is held by,corporation,crc,create a game that,creative misuse,creativity,creativity support tools,crisis management,cross-community context management {\'{a}},cross-cultural,cross-cultural differences,cs,cs4hs,cscw,cue on another,cuisine,culinary technology,cultural heritage,culture,culture related public spaces,cumber-,curriculum,cyberphysical systems,cyberspace,daily activity levels,daisyphone,daphnia alias,data fusion,data integration,data mapping,data-based decision-,davis,daylight blocking,dead reckoning,decisional dna,declarative 3d approach in,defined networks,demands in the early,department of computer science,depth,des besoins r{\'{e}}els d,design,design exploration,design for,design methodology,design methods,design model,design of abstract representation,design patterns,design principles,design process,design prototyping,design research,design thinking,despite involving a large,detection,detection {\'{a}},determined using a wearable,development,devices in the environment,devices {\ae},different meth-,diffusion,digital,digital cities,digital city,digital heritage,digital image making,digital instructions manual,digital paper interfaces,digital pen,digital pen input,digitizing architecture,dimensional data visualization,direct manipulation,director,disassembly,disaster warning and relief,discovery {\'{a}} governance {\'{a}},display binned-profile,display ecosystem scale,display systems {\'{a}} social,displays,displays an image when,displays like the occulus,distance,distance education,distinct customs,distributed,distributed intelligence,distributed meetings,distributed physical user interfaces,distributed virtual environments,distributed virtual reality,distribution {\'{a}} thin {\'{a}},documentation,does not speak that,domain,drama,drastically but not permanently,drawing comparisons with,driven innovation,dsl,dynamic and autonomous services,dynamic guides,dynamic service,d{\'{e}}veloppement incr{\'{e}}mental,e,e-learning,e-participation,e2e,ease of use,echo,ecological interventions,ecologies,ecosystem,edutainment,effort has been made,electronic art,electronic commerce,electronic field guide,embedded inside them,embodied conversational agents,embodied interaction,embodied interaction {\'{a}} dynamic,emergent literacy,emerging practice {\ae}interdisciplinary,emerging technology,emotion recognition,empirical,empirical study,empty-picture plain of virtual,emulate real-life settings,enchantment {\'{a}} architecture {\'{a}},end-user,end-user development,energy,engaged living,engagement,engaging,engineering,engineering simulation,entertainment,entertainment technology,entertainment {\'{a}},entrepreneurship,environ-,environment,environment map,environment to environment communication,environment with additional information,environments,environments {\ae}tangible user,era images,ergonomic recommendations,espace de conception,et mobiles r{\'{e}}pondent {\`{a}},ethics,ethnography,ethologically inspired robotics,ethz,evaluation,evaluation methods,event,event detection,event-driven,events space {\'{a}},everywhere display,exertion interface,exhibits,expanding possibilities for interacting,expedia labs,experience,experience design,experiences in light of,experimental,experimental interfaces,experimental theaters,experimentation,exploration de l,exposure therapy,extended color gamut,extensibility,eye contact sensing,eye move-,eye tracking,eye-tracker,eye-tracking,f,face,facial expressions,facts on pervasive,fast polygonal region integration,feedback,feedforward,feel,ferencing,fiction,fiducial,field of view,field study,fields dealing with the,figure 0-1,figure 1,film,finding,finding solutions to finding,fine art,fine assembly process,fine manipulating tasks in,fingertips on a mouse,first person shooting,first responders,five,flan{\^{e}}ur,flexible display,flexible displays,flintham {\'{a}} s,fluid interaction,focus,focused visual hull,focussed on interactive augmentation,focussing,fog computing,for,for a better understanding,for example,for exploring interactive,for this,for which there are,form {\'{a}} interaction,forms,four-,framework,free-viewpoint video rendering,freewrl,from today,fuzzy,g,game,game consoles with virtual,game design,game development,game engine,game experience,game innovation,game-based learn-,games,games classifica-,games modeling,games projects,gaze,gaze manipulation,general literature,generated imagery,generated to populate the,genetic algorithm,geo,geo-web,geography,geospatial interaction {\'{a}} location-,ges-,gestalt {\'{a}} form-giving {\'{a}},gesture,gesture interaction,gesture recognition,gesture {\'{a}} interaction design,gestures,gies,global illumination,goes on to discuss,governance,gpgpu,gps,graphical abstraction,graphics hardware,grid computing,ground,group and organization interfaces,groupes,groupware,groupware architecture,groupware development toolkit,grows to 3-7mm,guided navigation,guides require robust,guis,h,h5,habitable bits,hand,hand tracking,hand-based interface,handheld,handheld computer,handheld computing application,handheld projectors,handheld tool,haptic input device,haptics,hardware,hardware-accelerated,hardware-aware computing,have a magnetic tracker,have when interacting with,hcbi,hci,head mounted displays,head movements,head-mounted display,heads-up,health,heritage digitization,high fidelity graphics,high performance com-,histogram equa-,historic sites interpretation,historical hypotheses,hmds are used for,hmds tend to,holography,home computing {\ae}personal,home networking,how the,however,hp-adaptive fem,hri,http,human centered comput-,human computer,human computer inter-,human computer interaction,human computer interaction inst,human computer interaction {\'{a}},human factors,human interaction,human motion,human robot interaction,human tracking and,human voice,human-centered computing,human-centered coputing,human-computer interaction,human-computer interface,human-computer interfaces,human-computer-biosphere interaction,human-computer-interaction,human-machine collaboration,human-machine interaction,human-robot interaction,human-robot interface,humanoid robot,hybrid media,hybrid practices,hydraulophone,hypertext,i,i emphasize a people-oriented,icp,icts,identification,identity formation,illuminating clay in use,im-,image annotation,image based modelling,image labeling,image plane,image processing,image recognition,image-based real-time rendering,image-based rendering,immersive,immersive performance,immersive systems and tangible,implicit interaction,importance driven visual-,important,improvised dis-,in addition to sharpening,in color on page,in disaster management,in health care,in order to achieve,in particular we have,in section 2,in the paper referred,in this setup players,in-game behavior,in-situ,in-vehicle agents,in-vehicle interfaces,in-volume inter-,inclusive desi,inclusive design,index terms,indirect light,indoor location,industrial aug-,industry,industry and academia in,inertial sensors,influence,information,information decoration,information interfaces and presentation,information systems design,information visualization,informative art,infrared,ing,ing with the goal,initial positions,innovation,innovation management,innovation policy,input,input device,inspiration,installations,instead of pressing a,instrumental,integral image,intelligent,inter-frame erasure coding,interact with the virtual,interactif,interaction,interaction design,interaction design and children,interaction device,interaction model,interaction paradigm,interaction techniques,interaction technology,interaction {\'{a}},interaction {\'{a}} design research,interactions in virtual environments,interactive,interactive art,interactive education,interactive environments,interactive interfaces,interactive media installation,interactive music,interactive paper,interactive projection,interactive surface,interactive surface {\'{a}} casual,interactive table,interactive tabletops,interactivity and interaction,interdisciplinary collaboration,interface,interface design,interface management system,interface techniques that have,interfaces,interfaces for cultural heritage,interfaces in even simple,interfaces {\'{a}} tabletop {\'{a}},interfaces {\ae},interfaces {\ae}memory sharing,interfaces {\ae}wearable devices,international development,international field research,internet of things {\'{a}},interoperability,interpretation {\'{a}} plug-in-based software,introduction,introduction and motivation,introductory and survey,investigated the use of,iot,ipheral communication,iphone,is granted without fee,is one example of,issues to be solved,it is,it is called the,it is crucial,it is crucial for,iterative design,ity in human-human or,j,japan,joseph j,k-12 outreach,kalman filter,kinect-based motion detection,knowledge,knowledge management,knowledge work,korea,lab bench,laboratories,laptop enables fast refresh,large-format touch display,laser projector,last lecture,latency,lation setup,laviola jr,layered impostors,lbs,learning,learning classifier system,learning style,les syst{\`{e}}mes mixtes collaboratifs,lesson,life-long learning,light is necessary because,line tracking,literature survey,live action role playing,living labs,living-room is an installation,localization,location,location awareness,location based games,location models,location sensing,location-aware,location-based game,location-based performance {\'{a}} cycling,location-based services,location-based systems,locative media,low efficiency and degrades,luminous inter-,m,machine vision,machinima,magic,magic lens,magic lenses,magical,magician,make a riddle,making,management,management as design,manipulation,manipulation by projection,manipulatives {\'{a}} mathematics learning,map acquisi-,maps,material,mathematical models,matrix pencil,maze,mazes,mean planning,meaningful virtual visualization of,media,media communication systems,media forms,mediascape,mediated communication,medical applications,medical informatics,medical visualization,mental health,ments,mersive display technology,mersively and intuitively since,mesh data,meta-design,meta-search {\ae}collaboration {\ae},metadata,metaphor {\'{a}} interactive learning,methods,mice,microblogging,middleware,minimal display,miscellaneous,mixed and augmented reality,mixed interactive systems,mixed objects,mixed reality,mixed systems,mmog,mobile,mobile 3d graphics,mobile and ubiquitous computing,mobile application,mobile applications,mobile augmented,mobile augmented reality {\'{a}},mobile camera phones,mobile cinema {\'{a}} context,mobile cloud,mobile computing,mobile device,mobile device interaction,mobile devices,mobile devices provide real-time,mobile event processing,mobile game,mobile games,mobile interaction,mobile museum guides,mobile phone,mobile phones,mobile processing on a,mobile projections {\'{a}} video-expression,mobile projector,mobile user,mobile video,mobile video production {\'{a}},mobile visualisation,mobile web,mobile work,mobility,model based tracking,modeling,modeling from multimedia,models,molecular,molecular biology,monitoring,montr{\'{e}}al,more perceptual and tangible,moser,most of our,motion and sensation,motion capture,motion tracking,motor rehabilitation,mots-cl{\'{e}}s,mouse,mouse input,movement sensing,moving wooden handles that,muller,multi-,multi-core,multi-layer interaction,multi-projectors,multi-robot systems,multi-stakeholder platform,multi-touch,multi-user interaction,multiagent system,multicast,multidisciplinary convergence,multimedia,multimedia information systems artificial,multimedia systems,multimodal,multimodal inter-,multimodal interaction,multimodal interaction {\ae}mobile,multimodal user interfaces,multiplayer game,multiple display environment,multiple kinects,multiple sclerosis,multiple senses are addressed,museum handheld devices,museum installations,museum visitors,museums,museums 24,museums {\'{a}} visitor attractions,music,music education,music improvisation,music interfaces,musical instrument,mutual occlusions,myth,mythology suggests that there,narrative,narratives,natural interfaces,nature interface,navigate in and to,navigation,nearly impossible for a,network programming,networked appliances,networking {\'{a}} pervasive computing,new interfaces,next generation,nighttime,nine are in their,nintendo wii remote,nitive overlap,nml,nomadic,nomadic work,non,not only control,not to mention before,notification systems,notification user interfaces,novel,novel artistic,novel interfaces,novel user interfaces and,now at university of,now working for kddi,numerical simulation,object,object supported,objectives and benefits of,observational studies,observations and reflections on,obstructive,octopocus,ods such as laser,of achieving real-time environment,of cam-,of direct-touch table-top,of epistemology,of the basic constraints,of the existing approaches,of the spatially varying,of the vision is,of tracking systems,ofdm,off-screen rendering,offers a view of,older adults,older people,olfactory,on a mobile,on control of the,on-body display,ontologies {\'{a}} smart home,open data,open development,open government,optical flow,optical see-,optics,or hard copies of,or infrared scanning,oration,order picking,organic user interface,organizations of experience,our goal in having,our students observational,our virtual space teleconferencing,our vision of the,our work is therefore,outdoor augmented,overview and detail,owner,pair programming,palmtop display for dextrous,pan and zoom,paper,paper computing,papier,parallel finite element software,parks,part or all of,participation,participatory desig,participatory design,participatory design {\ae}transitions,participatory simulations,particle filter,pattern recognition,pda,pddm,peephole displays,peer to peer,peer-to-peer networks {\ae}search,pen and paper user,pen input,penlight system,people tracking,people-centered approach {\'{a}} embodied,per-,peranakan,perceived usefulness,perceptible,perception,perceptual user interface,performance,peripheral awareness,peripheral displays,permission to make digital,persistent audio,person to perform a,personal fabrication,personal or classroom use,personal projection {\'{a}} semantic,personalized and adaptive systems,personas,persuasive displays,persuasive technology,pervasive computation,pervasive computing,pervasive gaming,pervasive network-,pervasive technologies,phatic,phidgets,photophone applications,physical,physical avatars,physical burden is not,physical interactive environments,physical interface,physical interfaces {\ae}mixed-media,physical user interfaces,picture 2,pin array display,pink cube,place and space,place-based storytelling,platform services,play,player,playground,policy modelling,position aware displays,posture and speech recognition,potential user,powerful way of creating,pre-visualization,prediction,presence,privacy,private,proactive computing,process,processes,processing of multi-,producing a high-quality fvvr,product design,production,profiling,programming models,project,project webpage,project-based,projected interfaces,projection systems,projective displays,projective texture mapping,prototypage par simulation,prototyping,prototyping {\'{a}} field studies,prove-,provided that copies,provided that copies are,proximity detection,proximity sensors,psychotherapy,public,public displays,public spaces,publishing,publishing framework,puting,puzzles and challenges that,qiong,qos,quaternion,qu{\'{e}}bec,rapid prototyping,reactive mas,real time,real time editing,real time systems,real time tracking,real world,real-time,real-time 3d,real-time applications,real-time graphics,real-time systems,real-world gaze manipu-,reality,realtime,recent literature in,recognition,reconnaissance,refer-,refocus on the quality,registration,rehabilitation,rehabilitation systems also they,relative positioning,reminiscence {\ae}story telling,remote communication,remote group creativity,remote rendering,remote support,rent web graphics technology,repeated marking,replay,represents a challenging,requirements,requirements of a 3d,requires collaboration,research,research agenda,research and development,research methods,research purpose,responsive environment,restriction,results with our virtual,retail,retrieval,rfid {\'{a}} infrared {\'{a}},rich media,rider spoke,rift will raise the,robot collaboration,robot coordination,robot teams,robot telecon-,robotics,ronment,routing {\'{a}},r{\'{e}}alit{\'{e}} augment{\'{e}}e,r{\'{e}}alit{\'{e}} mixte,s,s prototype,s restricted use of,saiba,same surgical target,scenarios,schemata {\'{a}},scherer et al,school of electrical engineering,science and technology,science exhibit,scientific visualisation,seamful,section,selecting a partly occluded,self-generative content,self-organization,self-organizing maps,selling,semantic,semi-immersive envi-,senior travelers,senses,sensing,sensing devices,sensing user movements {\'{a}},sensor fusion,sensor integration,sensor interactio,sensor interaction,sensor networks,sensors,serious games,service descriptions,service design,service oriented,services,session management,set of experience knowledge,several motivating applications,shamma {\'{a}} e,shape and color modeling,shape modeling,shared,shared memory,shared virtual reality,shared virtual worlds,shared-data object model,showing a real room,shown in figure 1,side-by-side,sift,sight impaired,simulation,simulation steering,simulators,simulta-,situated computing,situated interaction,situation awareness,skeleton,sketches,sketching support thinking by,sketching user interfaces,skills,skills assessment,slow technology,small groups,smart,smart ambient environments,smart camera networks,smart city,smart knowledge management system,smart objects,smart space {\'{a}} social,so-,social,social behavior,social impact,social interaction,social network services,social networks,social search,social web,socializing,software,software agents,software architecture,software art,software defects,software engineering,software metrics,software visualisation,som,sonification,soniture,space,space-,spaces,spatial augmented reality,spatial user interface {\'{a}},spatially immersive displays,spatially-aware display,spatio-temporal,specknet,speech,speed,spontaneous interaction {\'{a}},spontaneous interaction {\ae}personalization,stage performance,stages of a,staging,stance,standardization,standards,state of the art,stereoscopic 3d graphics to,stigma,storyboards,storytellers and,streaming,structure,students observe actual human,such as ease of,such difficulties,such systems provide,suit,super resolution,support tool,sur-,surface,sustainability,sustainable design,synthetic space,system,system administration,systems {\'{a}},syst{\`{e}}mes,table tennis,table-top displays,tabletop,tabletop gaming,tabletop groupware,tabletop interfaces,tactics,tactile,tagging,taiwan image,tangible,tangible bits,tangible interaction,tangible interface,tangible interfaces,tangible technologies {\'{a}} physical,tangible user in-,tangible user interfaces,tangibles,task model,tasks,tdoa,techniques,technology inspiration,technology probes,technology {\'{a}} virtual manipulatives,tele-embodiment,tele-immersion,tele-manipulation,telepresence,tels syst{\`{e}}mes est une,templates,temporal form {\'{a}} physical,tems,terface,terfaces,texturing,that people find desirable,the,the author,the bat handle naturally,the blocking of natural,the computer system,the concepts of action,the cur-,the current fixation is,the essence of activities,the extent to which,the four siftables in,the future work for,the inherent difficulty in,the literature,the living canvas initiative,the living-room idea,the near future,the other three form,the performing virtual hero,the position of the,the reader,the real world can,the results of,the scenario described above,the social mobile web,the surgical team to,the top tile,the use of such,the user and,the virtual maze,themselves rather,theory,there are numerous benefits,there can be a,there is a growing,they often consist of,they support remote,thick client {\'{a}} resource,thinking in engineering design,this figure is reproduced,this gap between paper,this material shows how,this unique community developed,this work for,this work or,thomas 2001b,thomas et al,through displays,thus making it less,times,tion,to,to as design action,to be effective and,to create a visual,to enrich the,to investigate our,to our surprise,to populate a virtual,to reduce simulator,to the various stakeholder,tolmie {\'{a}},tool,toolkit,toolkits,total internal reflection,touch,touch screen,touch technology,tour guide,toy,tracking,tracking {\'{a}} ambient illumination,training,transferring cultural,transfers to everyday,transparency,transparent widgets,tree fairy,tric meaning of mathematics,trust,truth data,tuio,tuis as opposed to,tummeling,ture recognition,turn {\'{a}} computational composites,tv {\'{a}} context,twitter,two hand,u,ualberta,ubi-,ubiquitous,ubiquitous computation,ubiquitous computing,ubiquitous tracking,ui techniques,uidl,uims,un-,under time-varying daylight,understanding how people use,unified theories of cognition,universal design,university of bristol,unknown foreign word in,unsynchronized communication,until the 19 th,urban,urban informatics,urban spaces,usability,usability engineering,user,user centered,user centred,user created content,user experience,user experiences,user generated content {\'{a}},user interface,user interface description language,user interface design,user interface software,user interfaces,user interfaces {\'{a}} embodied,user oriented,user research,user studies,user-centered design and development,user-generated content,users,users enjoyed it more,using drillsample selection technique,utilisateurs mobiles travaillant en,uwb,van,variety of explanations for,vehicle,vehicular ad hoc networks,very inviting for the,video,video analysis,video card game,video surrogates,video-guided surgery,videoconference,view video,view-dependent rendering,viewable 3d,viewable 3d display,viewpoint-corrected,virtual,virtual archaeology,virtual environment,virtual environment applications,virtual environments,virtual environments have proved,virtual human,virtual humans,virtual integration,virtual key frame,virtual keyboard,virtual realit,virtual reality,virtual space,virtual theater,virtual tours,virtual world,vision impaired,vision with video-see-through hmds,vision-based,vision-based interaction {\'{a}} exploration,visual informa-,visual interface design,visual querying,visual search,visual touchpad,visualization,vj,voice analysis,voice commands recognized by,voice-based augmentation,volume measurement,voxelization,vr,vr applications,vr art,vrml,wall interfaces,water flea,waterplay,we,we provide a review,we reflect on these,we set out to,we would like to,wearable,wearable computer,wearable computers,web,web accessibility,web accessibility policies,web accessibility testing,web augmentation,web development,web navigation,web search of an,web {\'{a}},webgl,weiser,well documented,when,when two surgeons perform,while the other,while the technology-oriented part,wide-spread necessity to,wiki,wireless,wireless art,wireless robotics,wireless sensor networks,with appropriate behaviors they,with digital technolo-,with interactive rooms is,wlan,workflow management,workshop,world more im-,writing,wsan,x3d,{\'{a}},{\'{a}} context,{\'{a}} d,{\'{a}} educational,{\'{a}} interaction design,{\'{a}} m,{\'{a}} seams {\'{a}},{\'{a}} tangible,{\ae},{\ae}cybernetics {\ae}haptic},
pmid = {20830212},
title = {{NotPhDSurveyPaper}},
year = {2012}
}
@article{Press2015,
abstract = {We analyze the environmental dependence of galaxy morphology and colour with two-point clustering statistics, using data from the Galaxy Zoo, the largest sample of visually classified morphologies yet compiled, extracted from the Sloan Digital Sky Survey. We present two-point correlation functions of spiral and early-type galaxies, and we quantify the correlation between morphology and environment with marked correlation functions. These yield clear and precise environmental trends across a wide range of scales, analogous to similar measurements with galaxy colours, indicating that the Galaxy Zoo classifications themselves are very precise. We measure morphology marked correlation functions at fixed colour and find that they are relatively weak, with the only residual correlation being that of red galaxies at small scales, indicating a morphology gradient within haloes for red galaxies. At fixed morphology, we find that the environmental dependence of colour remains strong, and these correlations remain for fixed morphology $\backslash$textit{\{}and{\}} luminosity. An implication of this is that much of the morphology--density relation is due to the relation between colour and density. Our results also have implications for galaxy evolution: the morphological transformation of galaxies is usually accompanied by a colour transformation, but not necessarily vice versa. A spiral galaxy may move onto the red sequence of the colour-magnitude diagram without quickly becoming an early-type. We analyze the significant population of red spiral galaxies, and present evidence that they tend to be located in moderately dense environments and are often satellite galaxies in the outskirts of haloes. Finally, we combine our results to argue that central and satellite galaxies tend to follow different evolutionary paths.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Press, California and Sciences, Biological and Zuckerberg, Benjamin and Fink, Daniel and Sorte, Frank A La and Hochachka, WM Wesley M. and Kelling, Steve and Zollo, Fabiana and Bessi, Alessandro and Vicario, Michela Del and Scala, Antonio and Caldarelli, Guido and Shekhtman, Louis and Havlin, Shlomo and Quattrociocchi, Walter and Zhao, Jun and Binns, Reuben and Kleek, Max Van and Shadbolt, Nigel R and You, Linlin and Motta, Gianmario and Liu, Kaixu and Ma, Tianyi and Yong, Ding L I and Liu, Yang and Low, Bing W E N and Espa{\~{n}}ola, Carmela P and Choi, Chang-yong and Yarnell, Richard W and Sciences, Environmental and Trent, Nottingham and Yang, Anran and Fan, Hongchao and Jing, Ning and Yang, Haofan Hui and Zhang, Jinglan and Roe, Paul and Xue, Yexiang and Davies, Ian and Fink, Daniel and Gomes, Carla P and Southern, In and Rica, Costa and Wright, Jessica and Worthington, Jenny P and Silvertown, Jonathan and Cook, Laurence and Cameron, Robert and Dodd, Mike and Greenwood, Richard M and McConway, Kevin and Skelton, Peter and Preprints, Peerj and Access, Open and Wolfe, John R and Paper, International and Wine, Stuart and Gagn{\'{e}}, Sara A and Meentemeyer, K and Williamson, Kirsty and Johanson, Graeme and Willett, Kyle W and Lintott, Chris J. and Bamford, Steven P. and Karen, L and Simmons, Brooke D and Casteels, Kevin R V and Edward, M and Fortson, Lucy F and Kaviraj, Sugata and Keel, William C. and Melvin, Thomas and Nichol, Robert C. and Raddick, M. Jordan and Simpson, Robert J and Skibba, Ramin A. and Smith, Arfon M. Arthur C M. and Thomas, Daniel and Id, Species and Wilderman, Candie C. and Introduction, Contents and Project, Methods and Conclusion, Discussion and Citizen, Introduction and Wiggins, Andrea and Wiggins, Andrea and Species, Swedish and Centre, Information and Anabela, Leipzig and Andrea, Ramos and Naturale, Storia and Lieberoth, Andreas and White, Elizabeth Erin L Easton R and Basford, Liam and Birch, Stephen and Black, Alison and Culham, Alastair and Mcgoff, J and Lundqvist, Karsten O and Oppenheimer, Philippa and Tanner, Jonathan and Mauchline, Alice L and Myers, Mark C and Flemming, Joanna Mills and Baum, Julia K and Lake, Big Spirit and Weyhenmeyer, Gesa A and Mackay, Murray and Stockwell, Jason D and Thiery, Wim and Westphal, Andrew J. and Anderson, David and Butterworth, Anna L. and Frank, David R. and Lettieri, Robert and Marchant, William and Korff, Joshua V O N and Zevin, Daniel and Ardizzone, Augusto and Campanile, Antonella and Capraro, Michael and Courtney, Kevin and Iii, Mitchell N Criswell and Crumpler, Dixon and Cwik, Robert and Gray, Fred Jacob and Hudson, Bruce and Imada, Guy and Karr, Joel and Lau, Lily and Wah, Wan and Mazzucato, Michele and Motta, Pier Giorgio and Rigamonti, Carlo and Spencer, Ronald C and Woodrough, Stephens B and Santoni, Irene Cimmino and Sperry, Gerry and Terry, Jean-noel and Wordsworth, Naomi and R, Tom Yahnke S and Allen, Carlton and Ansari, Asna and Bastien, Ronald K. and Bassim, Nabil and Bechtel, Hans A. and Borg, Janet and Brenker, Frank E. and Bridges, John and Brownlee, Donald E. and Burchell, Mark and Burghammer, Manfred and Heck, Philipp R. and Hillier, Jon K. and Flynn, George and Gainsforth, Zack and Un, Eberhard G R and Hoppe, Peter and Huth, Joachim and Hvide, Brit and Kearsley, Anton and King, Ashley J. and Lai, Barry and Leitner, Jan and Lemelle, Laurence and Leroux, Hugues and Leonard, Ariel and Nittler, Larry R. and Ogliore, Ryan and Ong, Wei Ja and Postberg, Frank and Price, Mark C. and Sandford, Scott A. and Tresseras, Juan-angel Sans Angel Sans and Schmitz, Sylvia and Schoonjans, Tom and Sutton, Steven and Trieloff, Mario and Tsou, Peter and Tsuchiyama, Akira and Westphal, Andrew J. and Stroud, Rhonda M. and Bechtel, Hans A. and Brenker, Frank E. and Hillier, Jon K. and Postberg, Frank and Simionovici, Alexandre S. and Sterken, Veerle J. and Weston, By Michael and Fendley, Michael and Jewell, Robyn and Satchell, Mary and Tzaros, Chris and Weston, Michael and Fendley, Michael and Jewell, Robyn and Weston, Sarah and Conrad, Cathy C. and Westenberg, David J and Weld, Daniel S and Webb, Ginny and Watson, David and Floridi, Luciano and No, Publication and Monitoring, Volunteer Stream and Manual, Training and States, United and Network, Snow and Hatte, Marie-fran{\c{c}}oise and Finn, Elizabeth and Massachusetts, T H E and Rain, Acid and Action, I N and Ward, Eric J and Marshall, Kristin N and Ross, Toby and Sedgley, Adam and Hass, Todd and Pearson, Scott F and Joyce, Gerald and Hamel, Nathalie J and Peter, J and Faucett, Robert C and Ward, Darren F and Ward, Robert C and Richard, Y and Diane, M and Wallenburg, Iris and Exel, Job Van and Stolk, Elly and Scheele, Fedde and Wachowiak, Mark P and Wachowiak-smol{\'{i}}kov{\'{a}}, Renata and Dobbs, Brandon T and Abbott, James Anthony and Walters, Daniel and James, Cindy and Manager, Grant Project and Volten, Hester and Devilee, Jeroen and Apituley, Arnoud and Carton, Linda and Grothe, Michel and States, United and Agency, Protection and Aumen, Nick and Gurtz, Marty and Chindler, D Avid W S and Chlesinger, W Illiam H S and Ilman, D Avid G T and Vitousek, Peter M and Mooney, Harold A and Lubchenco, Jane and Melillo, Jerry M and Vianna, Gabriel M S and Meekan, Mark G and Bornovski, Tova H and Meeuwig, Jessica J and Coleman, David J and Eng, P and Sabone, Botshelo and Candidate, M Sc E and Wal, Van Der and Fischer, Anke and Verma, Audrey Aprajita and Verbrugge, Laura N H and Ganzevoort, Wessel and Fliervoet, Jan M and Panten, Katharina and Born, Riyan J G Van Den and Vann-sander, Sarah and Clifton, Jon Julian and Harvey, Euan and Vliet, Kim Van and Moore, Cristopher Claybourne and Exel, M Van and Dias, E and Woerd, Hendrik J Van Der and Wernand, Marcel R and Valdivia, Abel and Cox, Courtney Ellen and Bruno, John Francis and Hill, Chapel and Hodgson, Gregor and Stepath, Carl M and Kelling, Steve and Gerbracht, Jeff and Needs, User and Report, Assessment and Crowdsourcing, Usaid and Study, Case and Government, Second Open and Action, National and For, Plan Project and United, T H E and Of, States and Urbanski, K and Voshell, Reese and Environmental, U K and Framework, Observation and Ravi, Sujith and Williams, Sarah Vukelich and Zak, Paul J and Kurzban, Robert and Matzner, William T and Zak, By Paul J and Urzban, R Obert K and Biology, Human and Youn, Hyejin and Sutton, Logan and Smith, Eric Alden and Moore, Cristopher Claybourne and Jon, F and Maddieson, Ian and Croft, William and Bhattacharya, Tanmoy and Central, Proquest and Silk, Joan B and Xue, Ming and Silk, Joan B and Wilson, Rick K. Rory P Robert J. Rick K and Eckel, Catherine C. and The, From and Mouse, Patchwork and Robert, A and National, Seoul and Bishops, Catholic and Weber, Cameron and Glass, Steven and Wang, Yonghong and Singh, Munindar P. and Waguih, Hoda and Bosch, Frans A J Van Den and Volberda, Henk W and Moita, Marta A P and House, Nancy A Van and Review, Personnel and House, Nancy A Van and Butler, Mark H and Schiff, Lisa R and Schatz, Bruce and Bishop, Ann Peterson and House, Nancy A Van and Buttenfield, Barbara P and Bunt, Gerhard G Van De and Wittek, Rafael P M and Klepper, Maurits C De and Uslaner, Eric M and Tyler, Tom R and Goff, Phillip Atiba and Maccoun, Robert J and Pierce, Jessica Jeffrey and Bekoff, Marc and Otherness, Significant and Haraway, Donna and Emery, Nathan J and Clayton, Nicola S and Elick, Catherine L and Elick, Catherine L and Diamond, Judy and Bond, Alan B and Diamond, Judy and Bond, Alan B and Brosnan, Sarah Frances and Bekoff, Marc and Justice, Wild and Austen, Jane and Ziegler, Cai-nicolas and Theodorakopoulos, George and Baras, John S and Donovan, John O and Smyth, Barry and Nurse, Jason R C and Agrafiotis, Ioannis and Goldsmith, Michael and Creese, Sadie and Lamberts, Koen and Smith, Arfon M. Arthur C M. and Editor, Action and Petta, Paolo and Marsella, Stacy C and Gratch, Jonathan and Le, Stephen and Killingback, Timothy and Doebeli, Michael and Kalia, Anup K and Zhang, Zhe Zhuolun Zhibin and Singh, Munindar P. and Dong, Trung and Nicholas, Huynh and Shadbolt, Nigel R and Foley, Simon N and Dolev, Shlomi and Gilboa, Niv and Kopeetsky, Marina and Ceolin, Davide and Nottamkandath, Archana and Fokkink, Wan and Chandrasekaran, Partheeban and Esfandiari, Babak and Buntain, Cody and Golbeck, Jennifer a and Brownstein, Dan and Dolev, Shlomi and Gilboa, Niv and Hermoni, Ofer and Bessi, Alessandro and Scala, Antonio and Rossi, Luca and Zhang, Qian and Quattrociocchi, Walter and Basu, Anirban and Monreale, Anna and Trasarti, Roberto and Corena, Juan C and Giannotti, Fosca and Pedreschi, Dino and Kiyomoto, Shinsaku and Miyake, Yutaka and Yanagihara, Tadashi and Aref, Abdullah M and Tran, Thomas T and Hartmanis, J and Leeuwen, J Van and Thom, David H and Hall, Mark A and Pawlson, L Gregory and Krukow, Karl and Harwood, William Thomas and Harwood, William Thomas and Tan, Yao-hua and Thoen, Walter and Sztompka, Piotr and Review, An Integrative and Swan, John E and Bowers, Michael R and Richardson, Lynne D and Sutter, Matthias and Kocher, Martin G and Subramanian, S V and Kim, Daniel J and Kawachi, Ichiro and Suarez, David and Suthaharan, Praveen and Rowell, Jonathan T and Rycht, Jan and Sturgis, Patrick and Read, Sanna and Hatemi, Peter K and Zhu, Gu and Trull, Tim and Wright, Margaret J Mark A. and Martin, Nicholas G and Behavior, Source Political and June, No and Allum, Nick and {\~{A}}, Patrick Sturgis and Smith, Patten and Society, International and Psychology, Political and Psychology, Political and Journal, An International and Stern, Marc J and Coleman, Kimberly J and Stern, Marc J and Coleman, Kimberly J and Multidimensionality, The and Stern, Marc J and Coleman, Kimberly J and Baird, Timothy D and Vol, Scientific Temper and Earle, Timothy C and Sherchan, Wanita and Paris, Cecile and Sharp, Emily A and Thwaites, Rik and Curtis, Allan and Millar, Joanne and Version, Published and Link, Citable and Shankar, Venkatesh and Urban, Glen L and Sultan, Fareena and Kim, Y B and Packer, M S and Zuris, J A and Liu, D R and Neuberger, M S and Mcgranahan, N and Starrett, G J and Harris, R S and Wu, Y L and Rada, C and Rada, C and Neuberger, M S and Schryen, Guido and Volkamer, Melanie and Darmstadt, T U and Ries, Sebastian and Darmstadt, T U and Schneeberger, Karin and Dietz, Melanie and Taborsky, Michael and Scharlemann, J{\"{o}}rn P W and Eckel, Catherine C. and Kacelnik, Alex and Wilson, Rick K. Rory P Robert J. Rick K and Saleem, Muniba and Prot, Sara and Saleem, Muniba and Prot, Sara and Cikara, Mina and Lam, Ben C P and Anderson, Craig A and Jelic, Margareta and Rusch, Hannes and Lo, Peter and Rowell, Jonathan T and Ellner, Stephen P and Reeve, H Kern and Rowell, Jonathan T and Ellner, Stephen P and Reeve, H Kern and Robbins, Blaine G and Social, Source and June, No and Rieh, Soo Young and Psychology, Neuroscience and Rendall, Drew and Owren, Michael J and Ryan, Michael J and Rempel, John Karl and Holmes, John G Jaimie and Rempel, John Karl and Holmes, John G Jaimie and Zanna, Mark P and Primiero, Giuseppe and Taddeo, Mariarosaria and Kosolosky, Laszlo and Sabater-mir, Isaac Pinyol Jordi and Rutte, Claudia and Pfeiffer, Thomas and Rutte, Claudia and Killingback, Timothy and Taborsky, Michael and Bonhoeffer, Sebastian and In, Building Security and Nguyen, A N H V and Saini, Jasmeet S and Rowell, Jonathan T and Newton, Kenneth E N and Zmerli, Sonja and Neisse, Ricardo and Nannestad, Peter and M{\"{o}}llering, Guido and Moe, Henry Allen and Henry, By and Moe, Allen and Medicine, Nature and Miller, Cory T and Bee, Mark A and Metcalf, Elizabeth Covelli and Mohr, Jakki J and Yung, Laurie and Metcalf, Peter and Craig, David and Menges, Jochen I and Kilduff, Martin and States, United and Scale, Trust and Ralston, Ekaterina S and Dietz, Graham and Hartog, Deanne N Den and Dietz, Graham and Adams, Samuel H and Wiswell, Albert K and Criterion, Bias and Paine, Katie Delahaye and Katie, By and Paine, Delahaye and Naef, Michael and Naef, Michael and Ermisch, John and Gambetta, Diego and Whitepaper, Executive Briefing and Castelfranchi, Cristiano and Falcone, Rino and Mcknight, D Harrison and Kacmar, Charles J and Cummings, Larry L and Chervany, Norman L and Nicolaou, Andreas I and McNamara, John M and Stephens, Philip A and Dall, Sasha R X and Houston, Alasdair I and McNamara, John M and Mcknight, D Harrison and Chervany, Norman L and Mcevily, Bill and Radzevick, Joseph R and Weber, Roberto A and Tortoriello, Marco and Mcallister, Daniel J and Mayer, Roger C and Davis, James H and Schoorman, F David and Mayer, Roger C and Davis, James H and Marien, Sofie and Hooghe, Marc and Manapat, Michael L and Nowak, Martin A. and Rand, David G and Maisonneuve, Nicolas and Stevens, Matthias and Ochab, Bartek and Lucas, Chloe and Leith, Peat and Davison, Aidan and Lindskold, Svenn and Li, Peter Ping and Lewis, J David and Weigert, Andrew and Dame, Universityof Notre and Lewicki, R O Y J and Mcallister, Daniel J and Bies, Robert J and Deutsch, Morton and Coleman, Peter T and Levi, Margaret and Wilson-rich, Noah and Systems, Signaling and Lehmann, L and Keller, Laurent and K{\"{u}}mmerli, Rolf and Colliard, Caroline and Fiechter, Nicolas and Petitpierre, Blaise and Russier, Flavien and Keller, Laurent and {K{\"{a}}{\"{a}}ri{\"{a}}inen, J.; Sir{\'{e}}n}, R and Kydd, Andrew H. and Krueger, Frank and Mccabe, Kevin and Moll, Jorge and Kriegeskorte, Nikolaus and Zahn, Roland and Strenziok, Maren and Heinecke, Armin and Grafman, Jordan and Madhok, Aneeta and Koenig, Melissa A. and Harris, Paul L. and Hypotheses, Putnam and Killingback, Timothy and Doebeli, Michael and Bieri, J and Flatt, T and Doebeli, Michael and Knowlton, Nancy and Monreale, Anna and Wang, Wendy Hui and Pratesi, Francesca and Rinzivillo, Salvatore and Pedreschi, Dino and Andrienko, Gennady and Andrienko, Natalia and Kalleberg, R. and J{\o}sang, Audun and Ismail, Roslan and Boyd, Colin and Presti, S. SL and Jonker, C M and Treur, J and Johnson, Noel D. and Mislin, Alexandra A. and Lopez, Javier and Roman, Rodrigo and Agudo, Isaac and Fernandez-Gago, Carmen and Irwin, Kyle and Ifti, Margarita and Killingback, Timothy and Doebeli, Michael and Hurley, Robert F and Huff, T. E. and Huang, Jingwei Joey and Nicol, David M and Models, P K I Trust and Harwood, William Thomas and Clarke, John and Jacob, Jeremy and Hartig, Olaf and Harris, Paul L. and Corriveau, Kathleen H and Harlos, Christian and Edgell, Tim C. and Hollander, Johan and Hardin, Russell. and Hamilton, Lawrence and Hall, Mark A and Camacho, Fabian and Lawlor, Janice S and Depuy, Venita and Sugarman, Jeremy and Weinfurt, Kevin and Guo, Yuanbo and Heflin, Jeff and Guilford, Tim and Dawkins, Marian Stamp and Guha, R and Kumar, Ravi and Raghavan, Prabhakar and Tomkins, Andrew and Gray, Kurt and Waytz, Adam and Young, Liane and Geller, Amanda and Garfinkel, Irwin and Cooper, Caren B. Carey E and Mincy, Ronald B and Mincy, Ronald B and Graneheim, Ulla Hallgren and Lundman, B. and Grandison, Tyrone and Sloman, Morris and Goodhue, P. and McNair, H. and Reitsma, Femke and Golbeck, Jennifer a and Glaeser, Edward L. and Laibson, David I. and Scheinkman, Jos{\'{e}} a. and Soutter, Christine L. and Gillespie, Nicole and Dietz, Graham and Gellner, Ernest and Gauchat, G. and Gambetta, Diego and Fischer, Ilan and Fischer, J. A V and Fetchenhauer, Detlef and Dunning, David and Fareri, Dominic S and Chang, Luke J and Delgado, Mauricio R and Fairbrother, Malcolm and Martin, Isaac William and Elsner, Wolfram and Schwardt, Henning and Eckel, Catherine C. and Wilson, Rick K. Rory P Robert J. Rick K and Earle, Timothy C and Dutcher, Patricia and Doebeli, Michael and Dirks, Kurt T and Ferrin, Donald L and Dinesen, Peter Thisted and Hooghe, Marc and The, Source and Resolution, Conflict and Dec, No and Deutsch, Morton and Delhey, Jan and Newton, Kenneth E N and Welzel, Christian and Robertson, Ann and Cresswell, Kathrin and Takian, Amirhossein and Petrakaki, Dimitra and Crowe, Sarah and Cornford, Tony and Lichtner, Valentina and Quinn, Casey and Ali, Maryam and Morrison, Zoe and Jani, Yogini and Waring, Justin and Marsden, Kate and Society, The Royal and Society, Royal and Sciences, Biological and Meichenbaum, Donald H. and {Deffenbacher, Jerry}, L. and Currall, Steven C. and Judge, Timothy A and Inkpen, Andrew C. and Cragin, M. H. and Palmer, Craig L. J. and Carlson, J. R. and Witt, M. and Costa, Ana Cristina and Bijlsma-Frankema, Katinka and Corburn, Jason and Colquitt, Jason a and Scott, Brent a and LePine, Jeffery a and Collins, Harry M. and Evans, Robert and Chopra, Amit K. and Paja, Elda and Giorgini, Paolo and Castaldo, Sandro and Premazzi, Katia and Zerbini, Fabrizio and Gambetta, Diego and {ISO Central Secretariat} and Cami, Bagher Rahimpour and Bruyninckx, Joeri and Brauchli, K. and Killingback, Timothy and Doebeli, Michael and Rxu, Iihfwv R I and Hyhorsphqw, Kdqqhov R Q and Bos, Nathan and Olson, Judy and Gergle, Darren and Olson, Gary and Wright, Zach and E, Tappan Suite and Arbor, Ann and Blomqvist, Kirsimarja and Birch, Jonathan and Okasha, Samir and Bhattacharya, Rajeev and Devinney, Timothy M. and Pillutla, Madan M. and Bao, Fenye and Chen, Ing-Ray and Chang, MoonJeong and Cho, Jin-Hee and Balliet, Daniel and {Van Lange}, Paul A. M. and Azzedin, Farag and Maheswaran, Muthucumaru and Ashraf, Nava and Bohnet, Iris and Piankov, Nikita and Artz, Donovan and Gil, Yolanda and Ak{\c{c}}ay, {\c{C}}aĝlar and Reed, Veronica A. and Campbell, S. J Elizabeth and Templeton, Christopher N. and Beecher, Michael D. and Acedo-Carmona, Cristina and Gomila, Antoni and Abdul-Rahman, Alfarez and Hailes, Stephen and Szathm{\'{a}}ry, E{\"{o}}rs and Leonhard, J{\"{o}}rn and Pe{\~{n}}a, Jorge and Wu, Bin and Traulsen, Arne and Luhmann, Niklas and Hang, Chung Wei and Zhang, Zhe Zhuolun Zhibin and Singh, Munindar P. and Dincer, Oguzhan and Uslaner, Eric M and Lavoro, Nota D I and Eni, Fondazione and Mattei, Enrico and Bijlsma-Frankema, Katinka and Costa, Ana Cristina and Tretheway, Raymond and Simon, Mark and Mcpherson, E Gregory and Mathis, Sabrina and Tian, Huidong and Stige, Leif C and Cazelles, Bernard and Kausrud, Kyrre Linne and Svarverud, Rune and Stenseth, Nils C and Zhang, Zhe Zhuolun Zhibin and Thornton, Teresa and Leahy, Jessica and Thomas, Chris D. and Gillingham, Phillipa K. and Thomas, Jeremy A. and Edwards, Mike and Simcox, David J. and Powney, Gary D. and August, Tom A. and Isaac, Nick J B and Thiel, Martin and Penna-Diaz, M. A. and Luna-Jorquera, Guillermo and Salas, S. and Sellanes, J. and Stotz, W. and Konisky, Raymond a and Burdick, David M and Dionne, Michele and Neckles, Hilary a and Wendt, Tracy R and Tenopir, Carol and Allard, Suzie and Douglass, Kimberly and Aydinoglu, Arsev Umur and Wu, Lei and Read, Eleanor and Manoff, Maribeth and Frame, Mike and Tennant, Jonathan P and Waldner, Fran{\c{c}}ois and Jacques, Damien C and Masuzzo, Paola and Collister, Lauren B and Hartgerink, Chris H J and Schonewille, Ben and Teacher, Amber G F and Griffiths, David J. and Hodgson, David J. and Inger, Richard and Tar, P D and Bugiolacchi, R and Thacker, N A and Gilmour, J D and Tar, P D and Bugiolacchi, R and Thacker, N A and Gilmour, J D and S{\o}rensen, Jens Jakob W. H. and Pedersen, Mads Kock and Munch, Michael and Haikka, Pinja and Jensen, Jesper Halkj{\ae}r and Planke, Tilo and Andreasen, Morten Ginnerup and Gajdacz, Miroslav and M{\o}lmer, Klaus and Lieberoth, Andreas and Sherson, Jacob F. and Szabo, Judit K. and Fuller, Richard A. and Possingham, Hugh P. and Sydenham, Peter H. and Swanson, Alexandra and Kosmala, Margaret and Lintott, Chris J. and Packer, Craig and Simpson, Robert J and Smith, Arfon M. Arthur C M. and Packer, Craig and Sutherland, William J. and Roy, David B. and Amano, Tatsuya and Suomela, Todd Ernest and Bonney, Rick E. and Cooper, Caren B. Carey E and Dickinson, Janis L. and Kelling, Steve and Phillips, Tinas B. and Rosenberg, Kenneth V. and Shirk, Jennifer L JL Jennifer L. and Phillips, Tinas B. and Wiggins, Andrea and Ballard, Heidi L. HL Heidi L. and Miller-Rushing, Abraham J. and Parrish, Julia K J.K. and Conrad, Cathy C. and Hilchey, Krista G. and Cooper, Caren B. Carey E and Dickinson, Janis L. and Phillips, Tinas B. and Bonney, Rick E. and Couvet, Denis and Jiguet, Fr{\'{e}}d{\'{e}}ric Frederic and Julliard, Romain and Levrel, Harold and Teyssedre, A. and Crain, Rhiannon L. and Cooper, Caren B. Carey E and Dickinson, Janis L. and Devictor, Vincent and Whittaker, Robert J. and Beltrame, Coralie and Dickinson, Janis L. and Shirk, Jennifer L JL Jennifer L. and Bonter, David N. and Bonney, Rick E. and Crain, Rhiannon L. and Martin, Jason and Phillips, Tinas B. and Purcell, Karen and Zuckerberg, Benjamin and Bonter, David N. and Evans, Celia and Abrams, Eleanor and Reitsma, Robert and Roux, Karin and Salmonsen, Laura and Marra, Peter P. and Galloway, Aaron W E and Tudor, Margaret T and Haegen, W Matthew Vander and Kawrykow, Alexander and Roumanis, Gary and Kam, Alfred and Kwak, Daniel and Leung, Clarence and Wu, Chu and Zarour, Eleyine and Sarmenta, Luis and Blanchette, Mathieu and Waldisp{\"{u}}hl, J{\'{e}}r{\^{o}}me and Maier, Raina M and Gandolfi, A Jay and Newman, Gregory J. and Wiggins, Andrea and a.W. Crall, Alycia W. and Graham, Eric and Newman, Sarah and Crowston, Kevin and Rotman, Dana and Preece, Jenny Jennifer and Hammock, Jennifer and Procita, Kezee and Hanse, Derek and Parr, Cynthia and Lewis, Darcy and Jacobs, David W. and Science, Citizen and For, Plan Project and Surveys, Biodiversity and Bnhs, A T and Reserve, Nature and Science, Citizen and Silvertown, Jonathan and Sutherland, William J. and Roy, David B. and Amano, Tatsuya and Theobald, E.J. and Ettinger, A.K. and Burgess, H.K. and DeBey, L.B. and Schmidt, N.R. and Froehlich, H.E. and Wagner, C. and HilleRisLambers, J. and Tewksbury, J. and Harsch, M.a. and Parrish, Julia K J.K. and Trumbull, Deborah J and Bonney, Rick E. and Bascom, Derek and Cabral, Anna and Wiggins, Andrea and Crowston, Kevin and Radhakrishna, Sindhu and Binoy, V.V. and Kurup, Anitha and Suomela, Todd Ernest and Johns, Erica and Miller-Rushing, Abraham J. and Primack, Richard and Bonney, Rick E. and Noble, Rachel T. and Dorsey, John H and Sui, Daniel Z and Saunders, D A and Smith, Rick D Stuart C. and Strong, Diane M and Lee, Yang W and Wang, Richard Y. and Storey, Richard G and Wright-Stow, Aslan and Kin, Elsemieke and Davies-Colley, Robert J and Stott, Rebecca and Storer, Jeremy and Chao, Joseph and Torelli, Andrew and Ostrowski, Alexis and Stockwell, Stephanie B and Stewart, Alexander J. Alan J A and Bantock, Tristan M. and Beckmann, Bj{\"{o}}rn C. Bj�rn Bjo??rn C. and Botham, Marc S. and Hubble, David S. and Roy, David B. and Stevenson, Jan and Steger, Cara Elizabeth and Stapanian, Martin A. and Lewis, Timothy E. and Palmer, Craig L. J. and Amos, Molly M. and Srinivasan, Veena and Spence, Angie M and Smith, Melissa L and Nairn, Robert W and Sparks, K. and Klippel, A. and Wallgr{\"{u}}n, J.O. and David, M. and Smith, Finlay G. and Jones, Benedict C. and Welling, Lisa L W and Little, Anthony C. and Vukovic, Jovana and Main, Julie C. and DeBruine, Lisa M. and Smeltzer, Eric and Walker, William W and Garrison, Virginia and Slosar, An{\v{z}}e Anze and Land, Kate and Bamford, Steven P. and Lintott, Chris J. and Andreescu, Dan and Murray, Phil and Nichol, Robert C. and Raddick, M. Jordan and Schawinski, Kevin and Szalay, Alexander S. and Thomas, Daniel and Vandenberg, Jan and Science, Citizen and Friebe, Holm and Passig, Kathrin and Science, Citizen and Skibba, Ramin A. and Bamford, Steven P. and Nichol, Robert C. and Lintott, Chris J. and Andreescu, Dan and Edmondson, Edd Edward M. and Murray, Phil and Raddick, M. Jordan and Schawinski, Kevin and Slosar, An{\v{z}}e Anze and Szalay, Alexander S. and Thomas, Daniel and Vandenberg, Jan and Simpson, Robert J and Page, Kevin R and {De Roure}, David and Silvertown, Jonathan and Harvey, Martin C. and Greenwood, Richard M and Dodd, Mike and Rosewell, Jon and Rebelo, Tony and Ansine, Janice and McConway, Kevin and Shirk, Jennifer L JL Jennifer L. and Ballard, Heidi L. HL Heidi L. and Wilderman, Candie C. and Phillips, Tinas B. and Wiggins, Andrea and Jordan, Rebecca and McCallie, Ellen and Minarchek, Mathew and Lewenstein, Bruce C. and Krasny, Marianne E. and Bonney, Rick E. and Sheppard, S. Andrew and Wiggins, Andrea and Terveen, Loren and Sheil, Douglas and Mugerwa, Badru and Fegraus, Eric H. and Sharma, Nirwan and Shah, Harsh R and Martinez, Luis R and Severinsen, Jeremy and Reitsma, Femke and Settele, Josef and Wiemers, Martin and Abstract, O G C and Geographic, Specification and Senaratne, Hansi and Mobasheri, Amin and Ali, Ahmed and Capineri, Cristina and Haklay, Mordechai Muki and Schon, Jennifer and Schnetzer, Julia and Kopf, Anna and Bietz, Matthew J and Buttigieg, Pier Luigi and Fernandez-guerra, Antonio and Ristov, Aleksandar Pop and Gl{\"{o}}ckner, Frank Oliver and Kottmann, Renzo and Schawinski, Kevin and Urry, C. Megan and Virani, Shanil and Coppi, Paolo and Bamford, Steven P. and Treister, Ezequiel and Lintott, Chris J. and Sarzi, Marc and Keel, William C. and Kaviraj, Sugata and Cardamone, Carolin N. and Masters, Karen L. and Ross, Nicholas P. and Andreescu, Dan and Murray, Phil and Nichol, Robert C. and Raddick, M. Jordan and Slosar, An{\v{z}}e Anze and Szalay, Alexander S. and Thomas, Daniel and Vandenberg, Jan and Sawyer, Kim R and Sankey, Howard and Lombardo, Ric and Savan, Beth and Morgan, Alexis J. and Gore, Christopher and Sauermann, Henry and Franzoni, Chiara and Satterfield, D. A. and Maerz, J. C. and Altizer, S. and Sardi??as, Hillary S. and Kremen, Claire and Ruiz-mall{\'{e}}n, Isabel and Riboli-sasco, Livio and Ribrault, Claire and Heras, Maria and Peri{\'{e}}, Le{\"{i}}la and Roy, Helen E. H.E. and Preston, Christopher D. and Roy, David B. and Baxter, Elizabeth and Saunders, Aoine and Pocock, Michael J. O. M.J.O. and Roy, E Helen and Rorke, L Steph and Beckmann, Bj{\"{o}}rn C. Bj�rn Bjo??rn C. and Booy, Olaf and Botham, S Marc and Brown, M J Peter and Harrower, Colin A. and Noble, David G. and Sewell, Jack and Walker, Kevin J. Kathy O. Kevin and Heath, John and Perring, Franklyn and Rotman, Dana and Preece, Jenny Jennifer and Hammock, Jennifer and Procita, Kezee and Hansen, Derek L. and Parr, Cynthia and Lewis, Darcy and Jacobs, David W. and Rossiter, David G and Liu, Jing and Carlisle, Steve and Zhu, A.-Xing and Rossi, Giovanni Battista and Ronke, M. Eliese and Krementz, David G. and Berkeley, U C and Richard, David and Myers, David Richard and Yung, Laurie and Slovic, Paul and Sjoberg, L and Fischhoff, Baruch and Nature and Riesch, Hauke and Potter, Clive and Riddiford, Nick J. and Veraart, Jeroen a. and F{\'{e}}rriz, Inmaculada and Owens, Nick W. and Royo, Laura and Honey, Martin R. and Stem, C and Margoluis, R and Salafsky, N and Brown, M J Peter and Figliola, RS and Beasley, DE and Redford, Kent H. and Aune, Keith and Plumb, Glenn and Ratnieks, Francis L. W. and Schrell, Felix and Sheppard, Rebecca C. and Brown, Emmeline and Bristow, Oliver E. and Garbuzov, Mihail and McPherson, Jana and Raddick, M. Jordan and Bracey, Georgia and Carney, Karen and Gyuk, Geza and Qin, H. and Aburizaiza, A. O. and Rice, R. M. and Paez, F. and Rice, M. T. and States, United and Pyle, Richard L. and Purse, Bethan V. and Golding, Nick and Forest, Nicolet National and Howe, Robert W and Roberts, Lance J and Yamagishi, T. and Wang, Zhijian and Zhou, Yanran and Lien, Jaimie W. and Zheng, Jie and Xu, Bin and Stewart, Alexander J. Alan J A and Plotkin, Joshua B. and Singer, Emily and Sally, D. and Rong, Zhihai and Wu, Zhi Xi and Hao, Dong and Chen, Michael Z Q and Zhou, Tao and Rapoport, Amnon and Seale, Darryl A. and Colman, Andrew M. and Press, William H and Dyson, Freeman J and McAvoy, Alex and Hauert, Christoph and Knight, Vincent and Campbell, Owen and Harper, Marc and Langner, Karol M and Campbell, James and Campbell, Thomas and Carney, Alex and Chorley, Martin and Davidson-pilon, Cameron and Glynatsi, Nikoleta and Ehrlich, Tom{\'{a}}{\v{s}} and Jones, Martin Matthew O. B. and Koutsovoulos, Georgios and Muller, Jochen and Palmer, Geraint Georgina and Petunov, Piotr and Slavin, Paul and Standen, Timothy and Visintini, Luis and Molden, Karl and Hilbe, Christian and Traulsen, Arne and Sigmund, Karl and Wu, Bin and Traulsen, Arne and Nowak, Martin A. and Fischer, Ilan and Frid, Alex and Goerg, Sebastian J and Levin, Simon a and Rubenstein, Daniel I and Selten, Reinhard and Engel, Christoph and Zhurakhovska, Lilia and Baek, Seung Ki and Jeong, Hyeong-Chai and Hilbe, Christian and Nowak, Martin A. and Sigmund, Karl and Price, Steven J. and Dorcas, Michael E. and Preston, Christopher D. and Pearman, David A and Prainsack, Barbara and Powney, Gary D. and Isaac, Nick J B and Pocock, Michael J. O. M.J.O. and Newson, Stuart E. and Henderson, Ian G. and Peyton, Jodey and Sutherland, William J. and Noble, David G. and Ball, Stuart G. and Beckmann, Bj{\"{o}}rn C. Bj�rn Bjo??rn C. and Biggs, Jeremy and Brereton, Tom and Bullock, David J. and Buckland, Stephen T. and Edwards, Mike and Eaton, Mark A. and Harvey, Martin C. and Hill, Mark O. and Horlock, Martin and Hubble, David S. and Julian, Angela M. and Mackey, Edward C. and Mann, Darren J. and Marshall, Matthew J. and Medlock, Jolyon M. and O'Mahony, Elaine M. and Pacheco, Marina and Porter, Keith and Prentice, Steve and Procter, Deborah A. and Roy, Helen E. H.E. and Southway, Sue E. and Shortall, Chris R. and Stewart, Alexander J. Alan J A and Wembridge, David E. and Wright, Margaret J Mark A. and Roy, David B. and Ponsonby, Anne Louise and Mattingly, Karl and Polfus, Jean L. and Heinemeyer, Kimberly and Hebblewhite, Mark and Pocock, Michael J. O. M.J.O. and Newson, Stuart E. and Henderson, Ian G. and Peyton, Jodey and Sutherland, William J. and Noble, David G. and Ball, Stuart G. and Beckmann, Bj{\"{o}}rn C. Bj�rn Bjo??rn C. and Biggs, Jeremy and Brereton, Tom and Bullock, David J. and Buckland, Stephen T. and Edwards, Mike and Eaton, Mark A. and Harvey, Martin C. and Hill, Mark O. and Horlock, Martin and Hubble, David S. and Julian, Angela M. and Mackey, Edward C. and Mann, Darren J. and Marshall, Matthew J. and Medlock, Jolyon M. and O'Mahony, Elaine M. and Pacheco, Marina and Porter, Keith and Prentice, Steve and Procter, Deborah A. and Roy, Helen E. H.E. and Southway, Sue E. and Shortall, Chris R. and Stewart, Alexander J. Alan J A and Wembridge, David E. and Wright, Margaret J Mark A. and Roy, David B. and Preston, Christopher D. and Roy, David B. and Tweddle, John C. and Savage, Joanna and Robinson, Lucy D. and Roy, Helen E. H.E. and Crowston, Kevin and Mc, Erasmus and Wiley, John and {Katherine D. Heineman1, 5, Pedro Caballero2, Arturo Morris2, Carmen Velasquez2, Kiria Serrano2, Nelly Ramos2, Jonathan Gonzalez2, 1 Luis Mayorga2, Marife D. Corre3}, and James W. Dalling and Pocock, Michael J. O. M.J.O. and Chapman, D.S. Daniel S. and Sheppard, Lucy J L.J. and Roy, Helen E. H.E. and Brooks, J. S. and Waylen, K. A. and {Borgerhoff Mulder}, M. and Pittman, Shannon E and Dorcas, Michael E. and Maesham, Thomas G and Barnett, Guy B and Philip, Lorna J and Townsend, Leanne and Roberts, Elisabeth and Beel, David and Philip, Lorna J and Townsend, Leanne and Roberts, Elisabeth and Beel, David and Pfeffer, Max and Wagenet, Linda and Peters, Monica A. MA and Hamilton, David and Eames, Chris and Innes, John and Mason, Norman W H and Hamilton, David and Pescott, Oliver L. and Simkin, Janet M. and August, Tom A. and Randle, Zoe and Dore, Anthony J. and Botham, Marc S. and Kennerley, Mike and Neely, Andy and {Mel Hudson, Andi Smart}, Mike Bourne and Pennington, Catherine and Freeborough, Katy and Dashwood, Claire and Dijkstra, Tom and Lawrie, Kenneth and Patterson, D. J. and Cooper, J. and Kirk, P. M. and Pyle, Richard L. and Remsen, D. P. and Goldman, Jeffrey and Shilton, Katie and Burke, Jeff and Estrin, Deborah and Hansen, Mark and Ramanathan, Nithya and Reddy, Sasank and Samanta, Vids and Lukyanenko, Roman and Parsons, Jeffrey and Parr, T. W. and Sier, A. R J and Battarbee, R. W. and Mackay, A. and Burgess, J. and Ferretti, M and Simpson, I C and Forsius, M and Kov{\'{a}}cs-L{\'{a}}ng, E and Parfitt, Ian and Pantidi, Nadia and Moran, Stuart and Bachour, Khaled and Rodden, Tom and Zilli, Davide and Merrett, Geoff and Rogers, Alex and Tschannen-Moran, Megan and Hoy, Wayne K. and Baltag, Alexandru and Moss, Ls Lawrence S and Solecki, Slawomir and Ohara, Kieron and O'Fallon, Liam R. and Dearry, Allen and Overdevest, Christine and Orr, Cailin Huyck and Stepenuck, Kristine F and Mayer, Brian and Ottinger, G. and Otegui, Javier and Guralnick, Robert P. and Gartenschlaeger, Uwe and Osawa, Takeshi and Ortiz-Troncoso, Alvaro and {National Science Foundation} and O'Donnell, Ryan P. and Durso, Andrew M. and MacKenzie, Darryl I. and Nichols, James D. and Hines, James E. and Knutson, Melinda G. and Franklin, Alan B. and Royle, J. Andrew and Nichols, James D. and Lachman, Gideon B. and Droege, Sam and Royle, Andrew A. and Langtimm, Catherine A. and Bailey, Larissa L. and Simons, Theodore R. and Pollock, Kenneth H. and Obrecht, D. V. and Milanick, M. and Perkins, B. D. and Ready, D. and Jones, J. R. and Prysby, Michelle D and Oberhauser, Karen S. and Ries, Leslie and Oberhauser, Karen S. and Network, T H E Natureserve and Novoa, St??fani and Wrnand, Marcl and van der Woerd, Hendrik Jan and Northcutt, Katharine V and Pierce, Simon James and Dove, Alistair and Berumen, Michael L and Norman, Bradley M and Holmberg, Jason A and Arzoumanian, Zaven and Reynolds, Samantha and Wilson, Rick K. Rory P Robert J. Rick K and Gleiss, Adrian C and Rob, Dani and Pierce, Simon James and Parra, Rafael De and Galvan, Beatriz and Ramirez-macias, Deni and Robinson, David and Fox, Steve and Noble, Rachel T. and Weisberg, Stephen B. and Leecaster, Molly K. and McGee, Charles D. and Ritter, Kerry and Walker, Kevin J. Kathy O. Kevin and Vainik, Patricia M. and et al {Nisbet M} and Nisbet, Matthew C. and Scheufele, Dietram A. and Newman, Chris and Buesching, Christina D. and Macdonald, David W. and Gouveia, Cristina and Fonseca, Alexandra and {Someshwar Das, U. C. Mohanty, Ajit Tyagi, D. R. Sikka, P. V. Joseph, L. S. Rathore, Arjumand Habib, Saraju K. Baidya, Kinzang Sonam}, and Abhijit Sarkar and New, Leslie F. and Hall, Ailsa J. and Harcourt, Robert and Kaufman, Greg and Parsons, E. C M and Pearson, Heidi C. and Cosentino, A. Mel and Schick, Robert S. and Wilson, Kell and Goldstein, Allen and Falge, Eva and Aubinet, Marc and Baldocchi, Dennis D. and Berbigier, Paul and Bernhofer, Christian and Ceulemans, Reinhart and Dolman, Han and Field, Chris and Grelle, Achim and Ibrom, Andreas and Law, B. E. and Kowalski, Andy and Meyers, Tilden and Moncrieff, John and Monson, Russ and Oechel, Walter and Tenhunen, John and Valentini, Riccardo and Verma, Shashi and Post, H. and {Hendricks Franssen}, H. J. and Graf, A. and Schmidt, M. and Vereecken, H. and Fitzjarrald, David and Baldocchi, Dennis D. and Foken, Thomas and Nerbonne, Julia and Nelson, Kristen and Nerbonne, Brian A. and Vondracek, Bruce and Nebeker, Camille and Lopez-Arenas, Araceli and Pilz, David and Jones, Eric T and Ballard, Heidi L. HL Heidi L. and Green, Eugene and Benjamin-sirmons, Denise and Advisory, National and Policy, Environmental and Nacept, Technology and Munscher, Eric and Butterfield, Brian P and Moturu, Sai T. and Liu, Huan and Johnson, William G. and Morelli, Federico and Jiguet, Fr{\'{e}}d{\'{e}}ric Frederic and Reif, Jiri and Plexida, Sofia and Valli, Andrea Suzzi and Indykiewicz, Piotr and ????mov??, Petra and Tichit, Muriel and Moretti, Marco and Tryjanowski, Piotr and Morais, Alessandra M. M. and {Guarino de Vasconcelos}, Leandro and Santos, Rafael D. C. and Raddick, M. Jordan and Scholz, Jenna G and Booth, Derek B and Moffett, Er and Neale, Mw and Zhang, Zhe Zhuolun Zhibin and Wang, Sufen and Wand, Yair and Wang, Richard Y. and Miller, Harvey J and Michelucci, Pietro and Castrup, HT and Eicke, WG and Taib, Ibrahim Adham and McIntosh, Andrew Stuart and Caponecchia, Carlo and Baysari, Melissa T. and Eq, For and Monet, David G. and Levine, Stephen E. and Canzian, Blaise and Ables, Harold D. and Bird, Alan R. and Dahn, Conard C. and Guetter, Harry H. and Harris, Hugh C. and Henden, Arne A. and Leggett, Sandy K. and Levison, Harold F. and Luginbuhl, Christian B. and Martini, Joan and Monet, Alice K. B. and Munn, Jeffrey A. and Pier, Jeffrey R. and Rhodes, Albert R. and Riepe, Betty and Sell, Stephen and Stone, Ronald C. and Vrba, Frederick J. and Walker, Richard L. and Westerhout, Gart and Brucato, Robert J. and Reid, I. Neill and Schoening, William and Hartley, M. and Read, M. A. and Tritton, S. B. and {EURACHEM/CITAC Guide} and Helm, Irja and Jalukse, Lauri and Leito, Ivo and Guide, Citac and Edition, First and Citac and Eurachem and Guide, Citac and Edition, First and Cai, Qutang and Zhang, Changshui and Peng, Chunyi and Pauli, Jonathan N. and Whiteman, John P. and Riley, Meghan D. and Middleton, Arthur D. and Note, Production and Merkle, Bethann Garramon and McShea, William J. and Forrester, Tavis D. and Costello, Robert and He, Zhihai and Kays, Roland and McNamara, John M and Stephens, Philip A and Dall, Sasha R X and Houston, Alasdair I and MacNeil, Heather Marie and Mcknight, D Harrison and Chervany, Norman L and McKnight, NLC D Harrison and Dunn, Robert R. and Urban, Julie M. and Stalls, Jennifer and Millis, Courtney and Flythe, Taylar and McKenney, Erin and Stevens, Julia L. and McKinley, Duncan C. and Miller-Rushing, Abraham J. and Ballard, Heidi L. HL Heidi L. and Bonney, Rick E. and Brown, Hutch and Evans, David M Daniel M and French, Rebecca A and Parrish, Julia K J.K. and Phillips, Tinas B. and Ryan, Sean F and Shanley, Lea A and Shirk, Jennifer L JL Jennifer L. and Stepenuck, Kristine F and Weltzin, Jake F. and Wiggins, Andrea and Boyle, Owen D and Briggs, Russell D. and Iii, Stuart F Chapin and Hewitt, David A and Preuss, Peter W and Soukup, Michael A and McKinley, Duncan C. and Miller-Rushing, Abraham J. and Ballard, Heidi L. HL Heidi L. and Bonney, Rick E. and Brown, Hutch and Evans, David M Daniel M and French, Rebecca A and Parrish, Julia K J.K. and Phillips, Tinas B. and Ryan, Sean F and Shanley, Lea A and Shirk, Jennifer L JL Jennifer L. and Stepenuck, Kristine F and Weltzin, Jake F. and Wiggins, Andrea and Boyle, Owen D and Briggs, Russell D. and Iii, Stuart F Chapin and Hewitt, David A and Preuss, Peter W and Soukup, Michael A and Bartuska, Ann M. and McKenzie, L J and Campbell, S. J Elizabeth and Mazzetti, Paolo and Roncella, Roberto and Mihon, Danut and Bacu, Victor and Lacroix, Pierre and Guigoz, Yaniss and Ray, Nicolas and Giuliani, Gregory and Gorgan, Dorian and Nativi, Stefano and Mattson, Mark D. and Walk, Marie-Francoise and Kerr, Peter A. and Slepski, Anne M. and Zajicek, O. Thomas and Godfrey, Paul J. and Mason, Suzanna C. and Palmer, Geraint Georgina and Fox, Richard and Gillings, Simon and Hill, Jane K. Jocelyn and Thomas, Chris D. and Oliver, Tom H. and Marshall, Philip J. and Verma, Audrey Aprajita and More, Anupreeta and Davis, Christopher P. Clayton A and More, Surhud and Kapadia, Amit and Parrish, Michael and Snyder, Chris and Wilcox, Julianne and Baeten, Elisabeth and Macmillan, Christine and Cornen, Claude and Baumer, Michael and Simpson, Edwin and Lintott, Chris J. and Miller, David and Paget, Edward and Simpson, Robert J and Smith, Arfon M. Arthur C M. and K{\"{u}}ng, Rafael and Saha, Prasenjit and Collett, Thomas E. and Tecza, Matthias and Fletcher, Leigh N. and Marsh, David M. and Trenham, Peter C. and Venter, J Craig and Friedman, Robert M and Markusic, Janis S and Bonney, Rick E. and Shirk, Jennifer L JL Jennifer L. and Phillips, Tinas B. and Wiggins, Andrea and Ballard, Heidi L. HL Heidi L. and Miller-Rushing, Abraham J. and Parrish, Julia K J.K. and Mari, Luca and Janssen, Marco A. and Moves, Quantum and Moves, Quantum and Mahard, Tyler J. and Litvaitis, John A. and Tate, Patrick and Reed, Gregory C. and Broman, Derek J A and Maes, Dirk and Isaac, Nick J B and Harrower, Colin A. and Collen, Ben and van Strien, Arco J. and Roy, David B. and Lenz, Bernard N and Miller, Michael a and MacNeil, Heather Marie and Mak, Bonnie and Mackechnie, Colin and Maskell, Lindsay and Norton, Lisa and Roy, David B. and Maas, Richard P and Kucken, Darlene J and Gregutt, Peter F and Lynch, Louise I and Lukyanenko, Roman and Wiersma, Yolanda Y.F and Parsons, Jeffrey and Lukyanenko, Roman and Parsons, Jeffrey and Geisler, Sandra and Weber, Sven and Quix, Christoph and Lowry, Christopher S. and Fienen, Michael N. and Lowe, Winsor H. and Likens, Gene E. and Lofgren, Eric T and Collins, Kristy M and Smith, Tara C and Cartwright, Reed A and Littmann, Mark and Suomela, Todd Ernest and Lintott, Chris J. and Schawinski, Kevin and Bamford, Steven P. and Slosar, An{\v{z}}e Anze and Land, Kate and Thomas, Daniel and Edmondson, Edd Edward M. and Masters, Karen L. and Nichol, Robert C. and Raddick, M. Jordan and Szalay, Alexander S. and Andreescu, Dan and Murray, Phil and Vandenberg, Jan and Keel, William C. and {Van Arkel}, Hanny and Bennert, Nicola and Edmondson, Edd Edward M. and Thomas, Daniel and Smith, Daniel J B David R. Davey and Herbert, Peter D. and Jarvis, Matt J. and Virani, Shanil and Andreescu, Dan and Bamford, Steven P. and Land, Kate and Murray, Phil and Nichol, Robert C. and Raddick, M. Jordan and Slosar, An{\v{z}}e Anze and Szalay, Alexander S. and Vandenberg, Jan and Tobergte, David R. and Curtis, Shirley and Lintott, Chris J. and Schawinski, Kevin and Slosar, An{\v{z}}e Anze and Land, Kate and Bamford, Steven P. and Thomas, Daniel and Raddick, M. Jordan and Nichol, Robert C. and Szalay, Alexander S. and Andreescu, Dan and Murray, Phil and Vandenberg, Jan and Lindenmayer, David B. and Likens, Gene E. and Lindeman, Kenyon C. and Dame, Lauren E. and Avenarius, Christine B. and Horton, Benjamin P. and Donnelly, Jeffrey P. and Corbett, D. Reide and Kemp, Andrew C. and Lane, Phil and Mann, Michael E. and Peltier, W. Richard and Lewandowski, Eva and Specht, Hannah and Levrel, Harold and Fontaine, Beno{\^{i}}t and Henry, Pierre-yves and Jiguet, Fr{\'{e}}d{\'{e}}ric Frederic and Julliard, Romain and Lees, Alexander C. and Pimm, Stuart L. and Smith, Jennifer E. and Gavrilets, Sergey and Mulder, Monique Borgerhoff and Hooper, Paul L. and Mouden, Claire El and Nettle, Daniel and Hauert, Christoph and Hill, Kim and Perry, Susan and Pusey, Anne E. and van Vugt, Mark and Smith, Eric Alden and Lawrence, Anna and Law, Edith and Wiggins, Andrea and Gray, Mary L. and Williams, Alex and Lausen, Georg and Ziegler, Cai-nicolas and Larsen, Peter E and Land, Kate and Slosar, An{\v{z}}e Anze and Lintott, Chris J. and Andreescu, Dan and Bamford, Steven P. and Murray, Phil and Nichol, Robert C. and Raddick, M. Jordan and Schawinski, Kevin and Szalay, Alexander S. and Thomas, Daniel and Vandenberg, Jan and Africa, South and Kumar, Pramod and Com-, Mass and Mahaveer, Vardhman and Kullberg, Cecilia and Fransson, Thord and Hedlund, Johanna and Jonz{\'{e}}n, Niclas and Langvall, Ola and Nilsson, Johan and Bolmgren, Kjell and Kridelbaugh, Donna M. and Krell, Frank Thorsten and Krasny, Marianne E. and Lee, Sun-kyung and Kotovirta, Ville and Toivanen, Timo and J{\"{a}}rvinen, Marko and Lindholm, Matti and Kallio, Kari and Koss, Rs and Miller, K. and Brunce, A and Gilmour, P and McBurnie, J and Kosmala, Margaret and Wiggins, Andrea and Swanson, Alexandra and Simmons, Brooke D and Id, Cross R E F and Bmu, Ejournals and Library, Main and Koerten, Henk and Kletzl, Sebastian and Kitchin, Rob and Kinchy, Abby J and Jalbert, Kirk and Lyons, Jessica and Kim, Sunyoung and Mankoff, Jennifer and Paulos, Eric and Robson, Christine and Zimmerman, Thomas and Pierce, Jessica Jeffrey and Haber, Eben M. and Monreale, Anna and Wang, Wendy Hui and Pratesi, Francesca and Rinzivillo, Salvatore and Pedreschi, Dino and Andrienko, Gennady and Andrienko, Natalia and Munaf{\`{o}}, Marcus R and Tilling, Kate and Taylor, Amy E and Evans, David M Daniel M and Smith, Daniel J B David R. Davey and Kelling, Steve and Johnston, Alison and Hochachka, WM Wesley M. and Iliff, M and Fink, Daniel and {La Sorte}, Frank A. and Johnston, Alison and Bruns, Nicholas E. and Hochachka, WM Wesley M. and Bouchard, R W and Huggins, Donald and Kriz, James and Borne, Kirk and Kaufman, Allison B and Kasten, Kyle and Stenoien, Carl and Caldwell, Wendy and Oberhauser, Karen S. and Kasperowski, Dick and Letstudio, The and Kaspar, F. and Tinz, B. and M{\"{a}}chel, H. and Gates, L. and Kamp, Johannes and Oppel, Steffen and Heldbjerg, Henning and Nyegaard, Timme and Donald, Paul F. and Schr??der, Boris and Kamar, Ece and Kapoo, Ashish and Horvitz, Eric and Arking, Robert and Lee, Yang W and Pipino, Leo L and Funk, James D and Wang, Richard Y. and Jones, G. R. and George, J. M. and Johnston, Gary and Jiguet, Fr{\'{e}}d{\'{e}}ric Frederic and Julliard, Romain and Couvet, Denis and Petiau, Aurelie and Jarvis, Rebecca M. and Breen, Barbara Bollard and Kr{\"{a}}geloh, Christian U. and Billington, D. Rex and Jarnevich, Catherine S. and Graham, James Jim Jason R. and Newman, Gregory J. and a.W. Crall, Alycia W. and Stohlgren, Thomas J. and James, Trevor J and Jagers, Sverker C. and Martinsson, Johan and Matti, Simon and Jacobsen, John W. and Agency, Environmental Protection and Ii, Level and Quality, Environmental and Agency, Protection and Maximum, Total and Load, Daily and Isaac, Nick J B and Pocock, Michael J. O. M.J.O. and Driscoll, Charles T. and Lambert, Kathy Fallon and Weathers, Kathleen C. and Pilz, David and Ballard, Heidi L. HL Heidi L. and Jones, Eric T and Paper, White and Hyder, Kieran and Townhill, Bryony and Anderson, Lucy G. and Delany, Jane and Pinnegar, John K. and Hutchinson, Rebecca A and He, Liqiang and Emerson, Sarah C and Adhianto, L. and Banerjee, S. and Fagan, M. and Krentel, M. and Marin, G. and Mellor-Crummey, J. and Tallent, N. R. and Humber, F. and Godley, B. J. and Ramahery, V. and Broderick, A. C. and Hulbert, Joseph M. and Howard, Elizabeth and Davis, Andrew M. AK Andrew K and Hourston, Mathew and Mcdonald, Justin I and Hewitt, Matthew J and Holt, Ben G. and Rioja-Nieto, Rodolfo and {Aaron Macneil}, M. and Lupton, Jan and Rahbek, Carsten and Anghelache, Romeo and Hoey, Jessica and Campbell, Marnie and Hewitt, Chad and Gould, Brendan and Bird, Rosemary and Hodkinson, Ian D. and Jackson, John K. and Hochachka, WM Wesley M. and Caruana, Rich and Fink, Daniel and Munson, Art and Riedewald, Mirek and Sorokina, Daria and Kelling, Steve and Hill, Andrew and Guralnick, Robert P. and Smith, Arfon M. Arthur C M. and Sallans, Andrew and Gillespie, Rosemary and Denslow, Michael and Gross, Joyce and Murrell, Zack and Conyers, Tim and Oboyski, Peter and Ball, Joan and Thomer, Andrea and Prys-Jones, Robert and de la Torre, Javier and Kociolek, Patrick and Fortson, Lucy F and Hill, Mark O. and Preston, Christopher D. and Vi, Factsheet and Herron, Elizabeth and Green, Linda and Stepenuck, Kristine F and Addy, Kelly and {European Enviornment Agency} and Hecht, Julie and {Spicer Rice}, Eleanor and He, Kate S. and Bradley, Bethany A. and Cord, Anna F. and Rocchini, Duccio and Tuanmu, Mao-Ning and Schmidtlein, Sebastian and Turner, Woody and Wegmann, Martin and Pettorelli, Nathalie and Hartup, Barry K. and Dhondt, Andr{\'{e}} Andre a and Sydenstricker, Keila V. and Hochachka, WM Wesley M. and Kollias, George V. and Harris, Paul L. and Koenig, Melissa A. and Yang, Haofan Hui and Hanson, Eric and Burakowski, Elizabeth and Hansen, Derek L. and Jacobs, David W. and Lewis, Darcy and Biswas, Arijit and Preece, Jenny Jennifer and Rotman, Dana and Stevens, Eric and {Lawson Handley}, Lori and Hand, Eric and Hames, Ralph S and Rosenberg, Kenneth V. and Lowe, James D and Barker, Sara E and Dhondt, Andr{\'{e}} Andre a and Halley, Matthew R and Haklay, Mordechai Muki and Sui, Daniel Z and Elwood, Sarah and Goodchild, Michael F. and Haines, Julia Katherine and Gurney, Mark and Gunnthorsdottir, Anna and Mccabe, Kevin and Smith, Vernon and Schnoor, Jerald L and Grossman, G D and Groffman, Peter M. and Stylinski, Cathlyn and Nisbet, Matthew C. and Duarte, Carlos M. and Jordan, Rebecca and Burgin, Amy and {Andrea Previtali}, M. and Cary, James Coloso and Emery, Nathan J and Gray, Stefan Steven A. and Jordan, Rebecca and a.W. Crall, Alycia W. and Newman, Gregory J. and Hmelo-Silver, Cindy and Huang, Jingwei Joey and Novak, Whitney and Mellor, David and Frensley, Troy and Prysby, Michelle D and Singer, Alison and Gray, Stefan Steven A. and de Kok, Jean Luc and Helfgott, Ariella E R and O'Dwyer, Barry and Jordan, Rebecca and Nyaki, Angela and Granell, Carlos and Ostermann, Frank O. and Graham, James Jim Jason R. and Willcox, Everett and Ellis, James D. and Vellinga, Willem-pier and Planqu, Robert and Joly, Alexis and Vellinga, Willem-pier and Planqu, Robert and Rauber, Andreas and Bird, Lifeclef and Task, Identification and Gouveia, Cristina and Fonseca, Alexandra and C{\^{a}}mara, Ant{\'{o}}nio and Ferreira, Francisco and Gordienko, N. and Lodygensky, O. and Fedak, G. and Gordienko, Yu and Goodchild, Michael F. and Goffredo, Stefano and Pensa, Francesco and Neri, Patrizia and Gittleman, Mara and Jordan, Kelli and Brelsford, Eric and Gillingham, Phillipa K. and Bradbury, Richard B. and Roy, David B. and Anderson, Barbara J. and Baxter, John M. and Bourn, Nigel A D and Crick, Humphrey Q P and Findon, Richard A. and Fox, Richard and Franco, Aldina and Hill, Jane K. Jocelyn and Hodgson, Jenny A. and Holt, Alison R. and Morecroft, Mike D. and O'Hanlon, Nina J. and Oliver, Tom H. and Pearce-Higgins, James W. and Procter, Deborah A. and Thomas, Jeremy A. and Walker, Kevin J. Kathy O. Kevin and Walmsley, Clive A. and Wilson, Rick K. Rory P Robert J. Rick K and Thomas, Chris D. and Greve, By Adrienne I and Survey, U S Geological and Geoghegan, H. and Dyke, A. and Pateman, R. and West, S. and Everett, Glyn and Gee, Wilfred T and Guyon, Olivier and Walawander, Josh and Jovanovic, Nemanja and Pattengill-Semmens, Christy V. and Gardiner, Mary M. and Allee, Leslie L. and Brown, Peter M J and Losey, John E. and Roy, Helen E. H.E. and Smyth, Rebecca Rice and Gardiner, Lauren M. and Bachman, Steven P. and Garda, Chris and Castleden, Heather and Conrad, Cathy C. and Garbarino, Jeanne and Mason, Christopher E and Galloway, Aaron W E and Tudor, Margaret T and Haegen, W Matthew Vander and Gallino, Luciano and Danielsen, Finn and Burgess, Neil D. and Balmford, Andrew and Freitag, Amy and Meyer, Ryan and Whiteman, Liz and Frank, David R. and Westphal, Andrew J. and Zolensky, Michael E. and Gainsforth, Zack and Butterworth, Anna L. and Bastien, Ronald K. and Allen, Carlton and Anderson, David and Ansari, Asna and Bajt, Sasa and Bassim, Nabil and Bechtel, Hans A. and Borg, Janet and Brenker, Frank E. and Bridges, John and Brownlee, Donald E. and Burchell, Mark and Burghammer, Manfred and Changela, Hitesh and Cloetens, Peter and Davis, Andrew M. AK Andrew K and Doll, Ryan and Floss, Christine and Flynn, George and Gr??n, Eberhard and Heck, Philipp R. and Hillier, Jon K. and Hoppe, Peter and Hudson, Bruce and Huth, Joachim and Hvide, Brit and Kearsley, Anton and King, Ashley J. and Lai, Barry and Leitner, Jan and Lemelle, Laurence and Leroux, Hugues and Leonard, Ariel and Lettieri, Robert and Marchant, William and Nittler, Larry R. and Ogliore, Ryan and Ong, Wei Ja and Postberg, Frank and Price, Mark C. and Sandford, Scott A. and Tresseras, Juan-angel Sans Angel Sans and Schmitz, Sylvia and Schoonjans, Tom and Silversmit, Geert and Simionovici, Alexandre S. and Sol??, Vicente A. and Srama, Ralf and Stephan, Thomas and Sterken, Veerle J. and Stodolna, Julien and Stroud, Rhonda M. and Sutton, Steven and Trieloff, Mario and Tsou, Peter and Tsuchiyama, Akira and Tyliszczak, Tolek and Vekemans, Bart and Vincze, Laszlo and {Von Korff}, Joshua and Wordsworth, Naomi and Zevin, Daniel and Frampton, Sally and Moulds, Alison and Fowler, Amy and Whyatt, J. Duncan and Davies, Gemma and Ellis, Rebecca and Foster, Garth N and Fortson, Lucy F and Masters, Karen L. and Nichol, Robert C. and Borne, Kirk and Edmondson, Edd Edward M. and Lintott, Chris J. and Raddick, Jordan and Schawinski, Kevin and Wallin, John and Forrester, Tavis D. and Baker, Megan Scott and Costello, Robert and Kays, Roland and Parsons, Arielle W. and McShea, William J. and Impey, Chris D. and Wenger, Matthew C. and Austin, Carmen L. and Fore, Leska S. and Paulsen, Kit and O'Laughlin, Kate and Ford, R. Glenn and Foody, G. M. and Fontichiaro, Kristin and Oehrli, Jo Angela and Follett, Ria and Strezov, Vladimir and Flower, Emily and Jones, Darryl and Bernede, Lilia and Floridi, Luciano and Flesch, Elizabeth and Belt, Jami J. and Pattengill-Semmens, Christy V. and Semmens, Brice X. and Floridi, Luciano and Finkelstein, Ludwik and Feyisetan, Oluwaseyi and Simperl, Elena and Kleek, Max Van and Shadbolt, Nigel R and Fernandez-gimenez, Maria E and Ballard, Heidi L. HL Heidi L. and Sturtevant, Victoria E and Bhatt, Chintan and Dey, Nilanjan and Ashour, Amira S. and Farmer, Robert G and Leonard, Marty L and Horn, Andrew G and Faniel, Ixchel M. and Zimmerman, Ann and Falsay, James and Bonner, Kate and Crawford, Keeley and Phillips, Rosemary and Falchi, Fabio and Cinzano, Pierantonio and Duriscoe, Dan and Kyba, Christopher C M and Elvidge, Christopher D and Baugh, Kimberly and Portnov, Boris A and Rybnikova, Nataliya A and Furgoni, Riccardo and Taylor, Roger and Henderson, Sandra and Borum, Jens and Duarte, Carlos M. and Krause-Jensen, Dorte and Greve, Tina Maria and Information, Geographic and Fernandez-gimenez, Maria E and Ballard, Heidi L. HL Heidi L. and Sturtevant, Victoria E and Muhlfeld, Clint C. and Taper, Mark L. and Staples, David F. and Shepard, Bradley B. and Unit, Science Communication and White, L and Donnell, Michael O and Bell, Phillip and Lewenstein, Bruce C. and Shouse, Andrew W. and Feder, Michael a. and Brossard, Dominique and Lewenstein, Bruce C. and Bonney, Rick E. and Engelmann, Jan M and Herrmann, Esther and Tomasello, Michael and Engelmann, Jan M and Engel, Sarah Rose and Voshell, J. Reese and Fell, Richard D and Stone, Nicholas D and Voshell, J. Reese and Embling, C. B. and Walters, A. E M and Dolman, S. J. and Elwood, Sarah and Elmendorf, Sarah C. and Jones, Katherine D. Kara S. and Cook, Benjamin I. and Diez, Jeffrey M. and Enquist, Carolyn A F and Hufft, Rebecca A. and Jones, Martin Matthew O. B. and Mazer, Susan J. and Miller-Rushing, Abraham J. and Moore, David J P and Schwartz, Mark D. and Weltzin, Jake F. and Ellis, Rebecca and Waterton, Claire and Eiffert, Samantha and Noibi, Yomi and Vesper, Stephen and Downs, Jonathan and Fulk, Florence and Wallace, Juanita and Pearson, Melanie and Winquist, Andrea and Edwards, Richard and Goodchild, Michael F. and Edelson, Mindy Carol and Eckmanl, Karlyn and Gregersenl, Hans M and Lundgrenl, Allen L and Wilderman, Candie C. and Dunbar, Robin I M and Droege, Sam and Yves, L and Done, Terence and Roelfsema, Chris and Harvey, Andrew and Schuller, Laura and Hill, Jane K. Jocelyn and Schl{\"{a}}ppy, Marie-Lise and Lea, Alexandra and Bauer-Civiello, Anne and Loder, Jennifer and Dodge, Martin and Kitchin, Rob and Dillon, Justin and Stevenson, Robert B. D and Wals, Arjen E J and Devillers, Rodolphe and Stein, Alfred and B{\'{e}}dard, Yvan and Chrisman, Nicholas and Fisher, Peter and Shi, Wenzhong and Devictor, Vincent and Whittaker, Robert J. and Beltrame, Coralie and Gommerman, Luke and Monroe, Martha C and Deguines, Nicolas and Julliard, Romain and de Flores, Mathieu and Fontaine, Colin and Debelius, Justine W and V{\'{a}}zquez-Baeza, Yoshiki and McDonald, Daniel and Xu, Zhenjiang and Wolfe, Elaine and Knight, Rob and Davis, Christopher P. Clayton A and Heiman, Julia and Janssen, Erick and Garcia, Justin and Davis, Andrew M. AK Andrew K and Paper, White and Stepenuck, Kristine F and Herron, Elizabeth and Green, Linda and Melchior, P. and Sheldon, E. and Drlica-Wagner, A. and Rykoff, E. S. and Abbott, T. M C and Abdalla, Filipe B. and Allam, S. and Benoit-L{\'{e}}vy, A. and Brooks, D. and Buckley-Geer, E. and {Carnero Rosell}, A. and {Carrasco Kind}, M. and Carretero, J. and Crocce, M. and D'Andrea, C. B. and da Costa, L. N. and Desai, S. and Doel, P. and Evrard, A. E. and Finley, D. A. and Flaugher, B. and Frieman, J. and Gaztanaga, E. and Gerdes, D. W. and Gruen, D. and Gruendl, R. A. and Honscheid, K. and James, D. J. and Jarvis, Matt J. and Kuehn, K. and Li, T. S. and Maia, M. A G and March, M. and Marshall, J. L. and Nord, B. and Ogando, R. and Plazas, A. A. and Romer, A. K. and Sanchez, E. and Scarpine, V. and Sevilla-Noarbe, I. and Smith, Rick D Stuart C. and Soares-Santos, M. and Suchyta, E. and Swanson, M. E C and Tarle, G. and Vikram, V. and Walker, A. R. and Wester, W. and Zhang, Y. and Lintott, Chris J. and Schawinski, Kevin and Slosar, An{\v{z}}e Anze and Land, Kate and Bamford, Steven P. and Thomas, Daniel and Raddick, M. Jordan and Nichol, Robert C. and Szalay, Alexander S. and Andreescu, Dan and Murray, Phil and Vandenberg, Jan and George, Daniel and Huerta, E A and Thelen, Brett Amy and Thiet, Rachel K and Darg, D. W. and Kaviraj, Sugata and Lintott, Chris J. and Schawinski, Kevin and Sarzi, Marc and Bamford, Steven P. and Silk, Joan B and Proctor, R. and Andreescu, Dan and Murray, Phil and Nichol, Robert C. and Raddick, M. Jordan and Slosar, An{\v{z}}e Anze and Szalay, Alexander S. and Thomas, Daniel and Vandenberg, Jan and Dallimer, Martin and Davies, Zoe G. and Diaz-Porras, Daniel F. and Irvine, Katherine N. and Maltby, Lorraine and Warren, Philip H. and Armsworth, Paul R. and Gaston, Kevin J. and Dale, Bc and Norton, Michael and Downes, Constance and Collins, Brian and Curtis, Vickie and Purdam, Kingsley and Culp, J.M. and Cash, K.J. and Halliwell, D.B. and Plan, F.R.A. and Delaney, David G. and Sperling, Corinne D. and Adams, Christiaan S. and Leung, Brian and Cross, Tyler J and Croft, M V and Chow-Fraser, P and Criscuolo, Laura and Pepe, Monica and Seppi, Roberto and Bordogna, Gloria and Carrara, Paola and Zucca, Francesco and a.W. Crall, Alycia W. and Newman, Gregory J. and Jarnevich, Catherine S. and Stohlgren, Thomas J. and Waller, Donald M. and Graham, James Jim Jason R. and Crain, Rhiannon L. and Cooper, Caren B. Carey E and Dickinson, Janis L. and Crabbe, M. James C and Couvillon, Margaret J. and Ratnieks, Francis L. W. and Couvet, Denis and Prevot, Anne Caroline and Jiguet, Fr{\'{e}}d{\'{e}}ric Frederic and Julliard, Romain and Levrel, Harold and Teyssedre, A. and Cottman-Fields, Mark and Brereton, Margot and Wimmer, Jason and Roe, Paul and Cosentino, Bradley J. and Marsh, David M. and Jones, Katherine D. Kara S. and Apodaca, Joseph J. and Bates, Christopher and Beach, Jessica and Beard, Karen H. and Becklin, Kelsie and Bell, Jane Margaret and Crockett, Christopher and Fawson, George and Fjelsted, Jennifer and Forys, Elizabeth A. and Genet, Kristen S. and Grover, Melanie and Holmes, John G Jaimie and Indeck, Katherine and Karraker, Nancy E. and Kilpatrick, Eran S. and Langen, Tom A. and Mugel, Stephen G. and Molina, Alessandro and Vonesh, James R. and Weaver, Ryan J. and Willey, Anisha and Corser, Jeffrey D and White, Elizabeth Erin L Easton R and Schlesinger, Matthew D and Corcoran, Patricia L. and Norris, Todd and Ceccanese, Trevor and Walzak, Mary Jane and Helm, Paul A. and Marvin, Chris H. and Cooter, Robert and Cooper, Caren B. Carey E and Stevenson, Robert B. D and Cooper, Kelsey and Vanderhoff, Natasha and Cooney, Christopher R and Bright, Jen A and Capp, Elliot J R and Chira, Angela M and Hughes, Emma C and Moody, Christopher J A and Nouri, Lara O and Varley, Zo{\"{e}} K and Thomas, Gavin H and Peer, Limor and Green, Ann and Stephenson, Elizabeth and Comber, A. and Mooney, P. and Purves, R. S. and Rocchini, Duccio and Walz, A. and Colston, Nicole M. and Vadjunec, Jacqueline M. and Wakeford, Tom and Collins, Harry M. and Coleman, David J and Georgiadou, Yola and Labonte, Jeff and Observation, Earth and Canada, Natural Resources and Id, Cross R E F and Bmu, Main Library and Library, Main and Colautti, Robert and Franks, Steven J. and Hufbauer, Ruth a. and Kotanen, Peter M. and Torchin, Mark and Byers, James E. and Py{\v{s}}ek, Petr and Bossdorf, Oliver and Source, Jeffrey P Cohn and Coble, Michael D and Kline, Margaret C and Butler, John M and CLEMENTS, E. D. and NEAL, E. G. and YALDEN, D. W. and Crowston, Kevin and Hall, Hinds and Wiggins, Andrea and Goodchild, Michael F. and Trias-Blasi, A and Vorontsova, M and Cigliano, John A. and Meyer, Ryan and Ballard, Heidi L. HL Heidi L. and Freitag, Amy and Phillips, Tinas B. and Wasser, Ann and Churchwell, Roy and Geupel, Geoffrey R and Iii, William J Hamilton and Schlafmann, Debra and Cho, Jin-Hee and Chan, Kevin and Adali, Sibel and Cheeser, Megan E. and Ceci, Chiara and Chapman, D.S. Daniel S. and Bell, Simon Sandra and Helfer, Stephan and Roy, David B. and Chapman, Arthur D. and Speers, Larry and Chandler, Mark and See, Linda and Copas, Kyle and Bonde, Astrid M.Z. and L{\'{o}}pez, Bernat Claramunt and Danielsen, Finn and Legind, Jan Kristoffer and Masinde, Siro and Miller-Rushing, Abraham J. and Newman, Gregory J. and Rosemartin, Alyssa and Turak, Eren and Chambers, Lynda E. and Barnard, Phoebe and Poloczanska, Elvira S. and Hobday, Alistair J. and Keatley, Marie R. and Allsopp, Nicky and Underhill, Les G. and Cassinelli, Alvaro and Saakes, Daniel and Cash, D W and Casey, L. M. and Rebelo, H. and Rotheray, E. and Goulson, D. and Caruana, Rich and Elhawary, Mohamed and Munson, Art and Riedewald, Mirek and Sorokina, Daria and Fink, Daniel and Hochachka, WM Wesley M. and Kelling, Steve and Cardamone, Carolin N. and Schawinski, Kevin and Sarzi, Marc and Bamford, Steven P. and Bennert, Nicola and Urry, C. Megan and Lintott, Chris J. and Keel, William C. and Parejko, John and Nichol, Robert C. and Thomas, Daniel and Andreescu, Dan and Murray, Phil and Raddick, M. Jordan and Slosar, An{\v{z}}e Anze and Szalay, Alexander S. and Vandenberg, Jan and Camp, Miranda and Shein, Karsten and Abbott, James Anthony and Foster, Kristi and Apps, Why Build and Callaghan, Corey T. and Gawlik, Dale E. and Calladine, John and Wernham, Chris and Cagua, E Fernando and Cochran, Jesse E M and Rohner, Christoph A and Prebble, Clare E M and Sinclair-Taylor, Tane H and Pierce, Simon James and Berumen, Michael L and Burns, Maureen and Medvecky, Fabien and Burnett, Stephen and Furlong, Michelle and Melvin, Paul Guy and Singiser, Richard and Bhullar, Bhart-Anjan S. and Vi, Factsheet and Herron, Elizabeth and Green, Linda and Stepenuck, Kristine F and Addy, Kelly and Buettel, Jessie C. and Brook, Barry W. and Budig, Benedikt and Dijk, Thomas C Van and Arteaga, Mauricio Giraldo and Bruhn, LC and Soranno, PA and Brown, William T and Valley, Keene and Schoch, Nina and Breed, Greg a. and Stichter, Sharon and Crone, Elizabeth E. and Bravo, Macarena and {de los {\'{A}}ngeles Gallardo}, M. and Luna-Jorquera, Guillermo and N{\'{u}}{\~{n}}ez, Paloma and V{\'{a}}squez, Nelson and Thiel, Martin and {Brandon, Al., Spyreas, G., Molano-Flores, B., Carroll, C., Ellis}, J. and Boyle, W. Alice and Sigel, Bryan J. and Bowser, Anne and Shanley, Lea A and Borer, Elizabeth T. and Seabloom, Eric W. and Jones, Martin Matthew O. B. and Schildhauer, Mark and Zhang, Sicong and Yang, Haofan Hui and Singh, Lisa and Onter, D Avid N B and Ochachka, W Esley M H and Road, Sapsucker Woods and York, New and Bois, Sarah T. and Silander, John a. and Mehrhoff, Leslie J. and Boakes, Elizabeth H. and McGowan, Philip J K and Fuller, Richard A. and Chang-Qing, Ding and Clark, Natalie E. and O{\&}apos;Connor, Kim and Mace, Georgina M. and Pocock, WRc and Black, Jeffrey M and Bishr, Mohamed and Mantelas, Lefteris and Bird, Tomas J. and Bates, Adam J. Amanda E. and Lefcheck, Jonathan S. and Hill, Nicole A. and Thomson, Russell J. and Edgar, Graham J. and Stuart-Smith, Rick D. and Wotherspoon, Simon and Krkosek, Martin and Stuart-Smith, Jemina F. and Pecl, Gretta T. and Barrett, Neville and Frusher, Stewart and Bird, European and Council, Census and Biggart, Nicole Woolsey and Cook, Robert Book and Olson and Bestelmeyer, Stephanie V. and Elser, Monica M. and Spellman, Katie V. and Sparrow, Elena B. and Haan-Amato, Stephanie S. and Keener, Anna and Carlsson, Lars and Berkes, Fikret and Benony, Marguerite and Cardon, Marianne and Ferr{\'{e}}, Arnaud and Coquet, Jean and Foulquier, Nathan and Thonier, Florian and {Le Lann}, Lucas and {De Belly}, Henry and Evans, Alexandre and Jain, Aakriti and {Garc{\'{i}}a Arcos}, Juan Manuel and Bland, Jason and Marcus, Ian and Lindner, Ariel B and Wintermute, Edwin H and Bell, Simon Sandra and Cornford, Dan and Bastin, Lucy and Prichard, Bob and Juliette, C and Herzele, Van and Helen, E and Bekoff, Marc and Beausoleil, Richard A. and Clark, Joseph D. and Maletzke, Benjamin T. and Bear, Michael and Bauer, Paul Cornelius and Freitag, Prof. Dr. Markus and Gambetta, Prof. Dr. Diego and Bates, Adam J. Amanda E. and Sadler, Jon P. and Everett, Glyn and Grundy, Dave and Lowe, Norman and Davis, George and Baker, David and Bridge, Malcolm and Clifton, Jon Julian and Freestone, Roger and Gardner, David and Gibson, Chris and Hemming, Robin and Howarth, Stephen and Orridge, Steve and Shaw, Mark and Tams, Tom and Young, Heather and Batalden, Rebecca V and Oberhauser, Karen S. and Peterson, A Townsend and Batalden, Rebecca V and Oberhauser, Karen S. and Basset, Yves and Novotny, Vojtech and Miller, Scott E. and Pyle, Richard L. and Barsalou, Lawrence W and {Kyle Simmons}, W and Barbey, Aron K and Wilson, Christine D and Barrows, Cameron W. and Hoines, Josh and Vamstad, Michael S. and Murphy-Mariscal, Michelle L. and Lalumiere, Kristen and Heintz, James and Hernandez, Rebecca R. and Barnard, L. and Scott, C. and Owens, M. J. and Lockwood, M. and Tucker-Hood, K. and Thomas, S. and Crothers, S. and Davies, J. A. and Harrison, R. and Lintott, Chris J. and Simpson, Robert J and O'Donnell, J. and Smith, Arfon M. Arthur C M. and N.Waterson and Bamford, Steven P. and Romeo, F. and Kukula, M. and B.Owens and N.Savani and J.Wilkinson and Baeten, Elisabeth and Poeffel, L. and Harder, B. and Barclay, Pat and Barber{\'{a}}n, Albert and Hammer, Tobin J and Madden, Anne A and Fierer, Noah and Banerji, Manda and Lahav, Ofer and Lintott, Chris J. and Abdalla, Filipe B. and Schawinski, Kevin and Bamford, Steven P. and Andreescu, Dan and Murray, Phil and Raddick, M. Jordan and Slosar, An{\v{z}}e Anze and Szalay, Alexander S. and Thomas, Daniel and Vandenberg, Jan and Skibba, Ramin A. and Bamford, Steven P. and Nichol, Robert C. and Lintott, Chris J. and Andreescu, Dan and Edmondson, Edd Edward M. and Murray, Phil and Raddick, M. Jordan and Schawinski, Kevin and Slosar, An{\v{z}}e Anze and Szalay, Alexander S. and Thomas, Daniel and Vandenberg, Jan and Baldock, Katherine C R and Goddard, Mark A and Hicks, Damien M and Kunin, E and Mitschunas, Nadine and Osgathorpe, Lynne M and Potts, Simon G and Robertson, Kirsty M and Scott, Anna V and Stone, Graham N and Vaughan, Ian P and Memmott, Jane and Bahlai, Christie A. and Landis, Douglas A. and Azzurro, Ernesto and Aguzzi, J and Maynou, F and Chiesa, JJ and Savini, D and Axler, Richard and Hagley, Cynthia and Host, George and Schomberg, Jesse and Stepath, Carl M and August, Tom A. and Harvey, Martin C. and Lightfoot, Paula and Kilbey, David and Papadopoulos, Timos and Jepson, Paul and Baker, Megan Scott and Oeschger, Ian and Mccormack, Meaghan and Ashcroft, Michael B. and Gollan, John R. and Batley, Michael and Godfrey, Paul J. and Mattson, Mark D. and Kerr, Peter A. and Zajicek, O. Thomas and Arcanjo, Jeferson S. and Luz, Eduardo F P and Fazenda, {\'{A}}lvaro L. and Ramos, Fernando M. and Conference, Ieee International and October, Cybernetics and Elith, J and Leathwick, J R and Dickinson, Janis L. and Zuckerberg, Benjamin and Bonter, David N. and Andaloro, Franco and Castriota, Luca and Falautano, Manuela and Azzurro, Ernesto and Deidun, Alan and Allen, Zachary and Allan, J. David and Mceachern, Preston and Biosci, G and Tg, A B and Alabri, Abdulmonem and Hunter, Jane and Agate, Larra and Beam, Deborah and Bucci, Collen and Dukashin, Yegor and Beh, Raneem Jo and Brien, Kelsey O and Jude, Brooke A and Adriaens, Tim and {San Martin y Gomez}, Gilles and Bogaert, Johan and Crevecoeur, Luc and Beuckx, Jean Pierre and Maes, Dirk and Aceves-Bueno, Er??ndira and Adeleye, Adeyemi S. and Bradley, Darcy and {Tyler Brandt}, W. and Callery, Patrick and Feraud, Marina and Garner, Kendra L. and Gentry, Rebecca and Huang, Yuxiong and McCullough, Ian and Pearlman, Isaac and Sutherland, Sara A. and Wilkinson, Whitney and Yang, Yi and Zink, Trevor and Anderson, Sarah E. and Tague, Christina and Wiersma, Yolanda Y.F and Resource, Natural and Collins, Fort and Pattengill-Semmens, Christy V. and Semmens, Brice X. and Elwood, Sarah and Goodchild, Michael F. and Sui, Daniel Z and Sarnelle, Orlando and Morrison, Jamie and Kaul, Rajreni and Horst, Geoffrey and Wandell, Howard and Bednarz, Ralph and Ely, Eleanor and {Volunteer Monitor} and Jeffery, Keith G. and Asserson, Anne and Alabri, Abdulmonem and Hunter, Jane and Hanahan, Ruth Anne and Cottrill, Caitlin and Kelling, Steve and Hochachka, WM Wesley M. and Fink, Daniel and Riedewald, Mirek and Caruana, Rich and Ballard, Grant and Hooker, Giles and Stafford, Richard and Williams, Rachel L. and Herbert, Roger J H and Smith, Daniel J B David R. Davey and Robinson, Timothy J. and Scheifele, Lisa Z and Burkett, Thomas and Scassa, Teresa and Chung, Haewon and Sawyer, Kim R and Sankey, Howard and Lombardo, Ric and Philippoff, Joanna and Baumgartner, Erin and Mendez-Jimenez, Adriana and Heyman, William D and DiMarco, Steven F and Goodchild, Michael F. and Glennon, J. Alan and Gartner, Richard and Fails, J.A. and Herbert, K. and Hill, E. and {De Stefano}, A. and Hesse, B. and Cushman, P. and Gant, T. and Shah, S. and Abreu-Cruz, A. and Panchariya, N. and Nimbagal, V. and Dunn, Robert R. and Urban, Julie M. and Cavelier, Darlene and Cooper, Caren B. Carey E and Daume, Stefan and Galaz, Victor and Dahlhausen, Katherine and Krebs, Bethany L. and Watters, Jason V. and Ganz, Holly H. and Brossard, Dominique and Lewenstein, Bruce C. and Bonney, Rick E. and Chopra, Kari and Wallace, William A and {Kramer jww Rajeev Gor{\'{e}}}, Simon and Okamoto, Eiji and Tsukuba, U and Huang, Jingwei Joey and Fox, Mark S and Fatemi, Hassan and {Van Sinderen}, Marten and Wieringa, Roel and Sherchan, Wanita and Nepal, Surya and Hunklinger, Jonathon and Bouguettaya, Athman and Harwood, William Thomas and Huang, Jingwei Joey and Nicol, David M and Briguez, Cristian E and Capobianco, Marcela and Maguitman, Ana G and Siebenlist, Frank and Welch, Von and Tuecke, Steven and Foster, Ian and Nagaratnam, Nataraj and Janson, Philippe and Dayka, John and Nadalin, Anthony},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {0024-4066},
issn = {10958312},
journal = {Biological Journal of the Linnean Society},
keywords = {"wilderness,'Prosumers',- citizen-science,- corresponding author,- trust,0,0404:Climate Change,0459:Communication,0477:Environmental Studies,1 - the albufera,1 pp,10.1002/2015SW001167 and citizen science,1048,1053,2 - alterra,2005,2008,2010,2010 target,21,31,383-396,432,44,64,6700 aa wageningen,748,766,8,: citizen science,A13,ACQUISITION of data,ARUC,Above-ground carbon,Accumulation rates,Accuracy,Acidification,Actinic bulb,Adaptation,Adaptive management,Additive models,Administrative Data Pro-cessing—Business,Adolescent,Adult,Advancement,Age Factors,Agricultural water policy,Ammonia,Amperometric sensors,Amphibian,Anchoring,Animal Communication,Animals,Anonymisation,Anthropocene,Anthropogenic impacts,Anthropogenic marine debris,Anurans,Apis mellifera,Apoidea,App,Aquatic insects,Argumentation,Arkansas,Astronomy,Astronomy education,Atlas,Attractiveness,Attribute certification,Australia,Authorship,Avian,Awareness,BF Psychology,BMPs,Background deposition,Bacterial indicators,Bagging,Bargaining games,Batesian mimicry,Bayesian belief network,Bayesian confirmation theory,Bayesian model,Beach litter,Beach survey,Beach water quality,Beached bird surveys,Behavior,Behavioral economics,Benthos,Best practices,Bias,Big data,Binary analysis,Biodiversity,Biodiversity governance,Biodiversity indicators,Biodiversity monitoring,Bioindication,Bioindicators,Biological,Biological Evolution,Biological Records Centre,Biological assessment,Biological invasions,Biological recording,Biotic,Biotic interactions,Bird species richness,Bird surveys,Birds,Body mass index,Bootstrap,Botany,Bottlenose dolphin,Brain,Brain Mapping,Brain Mapping: methods,Brain: physiology,Branta canadensis maxima,Breeding bird atlas,Britain,British Isles,Brokered architectures,Bufo americanus,Bumblebees,Bushmeat,Business process broker,Butterflies,C72,C73,C90,CITIZEN science,CORNELL University. Macaulay Library,CSCW,Calibration,Call path profiling,Camera trap,Camera traps,Cape Cod National Seashore (CACO),Capture-recapture,Carcass persistence,Carcinus maenas,Caretta caretta,Cartography,Case study,Catalogues,Catchment,Cell-phone Spectrometer,Chelonia mydas,Chemistry,Chemistry: education,Chemistry: manpower,Chemistry: trends,Child,Child Development,Children's health,China,Citizen Science,Citizen networks,Citizen programs,Citizen science,Citizen science guidelines,Citizen scientist,Citizen-science,Citizens,Civic participation,Classification margin,Classifier ensemble,Climate,Climate Change,Climate adaptation,Climate change,Cloud audit,Cloud computing,Co-management,Co-production,Coastal conservation,Coastal wetland,Coevolution,Cognition,Collaboration,Collaborative systems,Colonisation rate,Colonization,Comanagement,Common property,Commons Lab,Communication,Communication and the arts,Communication network,Community,Community outreach,Community science,Community-based conservation,Community-based monitoring,Community-based natural resource management,Community-based participatory research,Competition,Competitive altruism,Computational modeling,Computer Simulation,Computer vision,Computers,Conceptual modeling,Condition-dependent preferences,Conservation,Conservation attitudes,Conservation costs,Conservation impact,Consumer Participation,Consumer Participation: statistics {\&} numerical dat,Contextual metadata,Continuity,Continuous,Convergence,Cooking,Cooperation,Cooperative Behavior,Coral monitoring,Coral reef,Credibility,Critical GIS,Cross-cultural experiments,Cross-scale linkages,Crowdsourcing,Cryptogam,Cuculus canorus,Cultural Evolution,Cultural heritage,Cumulative effects,Customer Relationship,Customer Trust,Cyanobacteria,Cyberinfrastructure,DEL,DNA,DNA barcoding,Dark matter,Data Quality,Data aggregation,Data analysis and validation,Data and Information,Data archives.,Data capture,Data collection,Data management,Data mining,Data quality,Data recovery,Data sharing,Data synergy,Data validation,Database,Database design,Databases,Datalogger,Datasets,Decision,Decision-making,Democracy,Dependability,Detection probability,Digitization,Dis-tributed Artificial Intelligence,Disaster management,Discovery metadata,Discrete Fourier transform,Discrimination (Psychology),Dispersal,Distance education,Distribution,Distribution change,Distributions,Disturbance,Dog,Domains,Dreissena bugensis,Dreissena polymorpha,E-commerce,E-infrastructures,EAV,EXCHANGE of publications,Early research opportunities,Early warning systems,Earth science,Earth sciences,Earthwatch,Eavesdropping,Ecological niche modeling,Ecological niche models,Ecology,Ecology: trends,Economic games,Ecosystem restoration,Ecosystem services,Ecosystems,Eddy covariance technique,Education,Endorphins,Endorphins: physiology,Energy balance,Environmental DNA,Environmental Monitoring,Environmental Monitoring: methods,Environmental Policy,Environmental Pollutants,Environmental Pollutants: analysis,Environmental Pollution,Environmental Pollution: statistics {\&} numerical da,Environmental Studies,Environmental change,Environmental democratization,Environmental education,Environmental governance,Environmental health,Environmental health sciences,Environmental justice,Environmental monitoring,Environmental policy,Epistemic instruments,Epistemic logic,Eretmochelys imbricata,Error detection,Error taxonomy,Essential biodiversity variables (EBV),Estimating trustworthiness,Europe,Evidence-based trust,Evolution,Evolutionary game theory,Execution monitoring,Exotic species,Expansion,Experience,Experimental,Experimental design,Exposure assessment,Extinction,Extinction rate,Extirpation,Extortion,FLUXNET,Faces,Felis aurata,Female,Field calibration,Fish,Fisheries,Flanders (north Belgium),Food plant,Forest management,Forest monitoring,Formal accreditation,From Conservation to Crowdsourcing: A Typology of,Fuzzy cognitive mapping,GBIF,GEOSS,GIS and Society,Galaxies: Seyfert,Galaxies: active,Galaxies: clusters: general,Galaxies: dwarf,Galaxies: elliptical and lenticular,Galaxies: evolution,Galaxies: formation,Galaxies: general,Galaxies: haloes,Galaxies: high-redshift,Galaxies: individual: IC 2497,Galaxies: interactions,Galaxies: peculiar,Galaxies: spiral,Galaxies: starburst,Galaxies: structure,Game,Game Theory,Game theory,Games,Games with a purpose,Geo-Wiki,Geo-enabled technologies,Geo-social media,Geographic information,Geographic range size,Georeference,Geoweb,Global Biodiversity Information Facility (GBIF),Global positioning systems,Governance,Government agencies,Grass buffers,Grasshoppers,Grasshoppers: physiology,Grassland,Great Barrier Reef,Great Britain,Great Lakes,Great Lakes coastal wetlands,Great Smoky Mountains National Park,Green space,Grey literature,Grid computing,Grooming,Grooming: physiology,Group on Earth Observations Biodiversity Observati,Growing degree days,H Social Sciences (General),H4 [Information Systems],HPC,Habitat,Habitat corridor,Habitat degradation,Habitat fragmentation,Habitat loss,Habitat modelling,Habitat quality,Hamilton's rule,Health and environmental sciences,Health disparities,Heath trap,Heathland,Hedgerows,Hemigrapsus sanguineus,Herpetofauna,Hierarchical model,Hirundo rustica,Historical ecology,Horseshoe crabs,Hotspots,Human Issues,Human behaviour,Human factors,Human progress,Human-centered computing: Collaborative filtering,Humans,I211 [Artificial Intelligence],ICT,ISBN-13: 9780262025447,IUCN Red List,Identification,Image processing,Imitative Behavior,Incident reporting system,Inclusive fitness,Indicator species,Indicators,Indirect reciprocity,Informal education,Informal learning,Information Quality,Information and communication technologies,Information and communication technology (ICT),Information content,Information management,Information systems: Crowdsourcing,Ingress,Insect conservation,Integrated monitoring,Intercalibration,Internet/Web technologies,Interns,Internships,Interpersonal Relations,Invariance,Invasions,Invasive species,Investment game,J50,Kin selection,Knowledge,Koala management,Kohonen Self-organizing Maps,Kriging,LTER,Laboratory experiments,Lake,Lake-bottom sediment,Land cover,Land management,Land use,Land-use change,Landscape components,Landscape ecology,Landscape fragmentation,Landscape structure,Landslide,Large-scale structure of Universe,Latent content,Lattice,Laughter,Law {\&} Behavioral Economics,Leading-edge,Leaf identification,Learning,Learning design,Lepidochelys olivacea,Lepidoptera,Lesson Plan,LiDAR and RADAR metrics,Limulus polyphemus,Linear models,Local constituencies,Local extinction,Local knowledge,Locally-based monitoring,Long-term monitoring,Lynx rufus,MANAGEMENT,MERIS,MODIS,MONITORING,MOOC,MSR,Machiavellianism,Macroinvertebrates,Macrosystem,Male,Management,Manifest content,Marine conservation,Marine evidence,Marine fish,Marine invasive species,Marine mammals,Marine management,Marine policy,Marine resource management,Marine spatial planning,Market interaction,Masculinity,Mass-flowering crop,Mathematical model,Maximum likelihood,Measurability,Measurement,Measurement philosophy,Measurement theory,Measurement uncertainty,Medical error,Medical informatics,Mercury vapour bulb,Meta-analysis,Meta-data,Metabarcoding,Metadata,Metapopulation,Methods: data analysis,Methods: statistical,Microbiology,Microcystin,Microcystis,Microplastics,Middle Aged,Milkweed,Miscellaneous General Terms Theory Keywords,Mobile,Model,Models,Mojave and Sonoran Deserts,Molecular food webs,Molt-migration,Monarch butterfly,Monitoring,Monitoring networks,Monitoring programs,Morphospecies,Morphotype,Moth sampling,Motivation,Mount Lofty Ranges,Movements,Multi-agent systems,Multilevel selection,Museum collections,NEON,NRDA,National,National Park,Natural history museums,Natural resource management,Negotiating,Negotiating: psychology,Network,Network design,Neurons,Neurons: physiology,Neurophysiology,News Recommender Systems,Niche concept,Nitrogen oxides,Non-native species,Nongovernmental organizations,North carolina,Null models,Nursing,OLCI,Objective knowledge,Occupancy models,Oil spill response,Oklahoma Academy of Science Proceedings,One-shot Prisoner's Dilemma,Online education,Online learning,Ontario,Ontology,Open Air Laboratories,Open population,Open source,Open-source data,Opportunistic sampling,Optimal bound,Organizational Culture,Outreach,Parataxonomic units,Parataxonomy,Parks,Participation,Participatory design,Participatory modeling,Participatory modelling,Participatory monitoring,Participatory research,Participatory science,Partnerships,Patch occupancy,Patient safety,Pellets,People involvement,Performance tools,Periodicity,Personality,Pharmacists,Phenology model,Philosophy of information,Phragmites australis,Physical habitat,Physiological,Physiological tolerance,Phytophagous insects,Plant phenology,Plastics,Plethodontid salamanders,Policies,Policy,Policy-based trust,Pollution,Popper,Popular science,Population decline,Population estimates,Population monitoring,Power analysis,Precision,Predictive model,Predictive soil mapping,Predictivity,Preschool,Primates,Primates: physiology,Primates: psychology,Prisoner's Dilemma,Prisoners,Privacy,Problem Solving,Profelis aurata,Program MONITOR,Proportion of area occupied,Pseudacris crucifer,Psychological,Psychomotor Performance,Psychomotor Performance: physiology,Public Opinion,Public Participation in Scientific Research,Public awareness,Public engagement,Public goods,Public goods game,Public health,Public participation,Public participation geographic information system,Public participation in scientific research (PPSR),Puma concolor,QA/QC,QoS,Qualitative content analysis,Quality assurance,Quantile regression,Quasars: general,RTU,Radio frequency,Range expansion,Range extension,Range shift,Range shifts,Rangifer tarandus,Reciprocity,Recorder effort,Recording,Recording behaviour,Recreational exposure,Recruitment,Reef Environmental Education Foundation,Reef fish,Reef life survey,Renewable energy,Repeated games,Representation,Reptiles,Reputation,Research,Research Design,Research Personnel,Research: education,Research: manpower,Reserves,Residential choice,Residential landscapes,Resilience,Restoration,Retention,Review,Richness,Riparian areas,Risk,Risk aversion,Risk maps,Road traffic,Roadside habitat,Robinson trap,Robust design,Rocky shore,Roving diver transect,Rural sociology,SCUBA,SLA,SOCIAL capital (Sociology),SOEP,SOUND recording {\&} reproducing,SSSI,Sample design,Sampling designs,Scale,Schools,Science,Science and society,Science communication,Science education,Science literacy,Science policy,Scientific literacy,Scientific rigor,SeaWiFS,Seabird damage assessment,Seagrasses,Search efficiency,Search intervals,Security,Semantic services,Semantics,Sensitive data,Sensory data,Sensory information,Sequential data,Shannon diversity,Shared databases,Shore-based survey,Shorebirds,Site occupancy,Site of special scientific interest,Skinner trap,Small-scale fisheries,Smartphone,Snapshot Serengeti,Social Behavior,Social Environment,Social Values,Social capital,Social epistemology,Social evolution,Social media,Social network,Social network services,Social networking,Social networks,Social preferences,Social trust,Social-ecological systems,Sociology,Soft systems,Software,Sorting,South carolina,Sparse data,Spatial,Spatial autocorrelation,Spatial pattern,Special Area of Conservation,Special Track on Computational Sustainability,Special feature: NEON design,Specialization,Species,Species at Risk Act,Species density,Species detectability,Species distribution,Species distribution modelling,Species distribution models,Species occurrence,Species richness,Species trends,Stakeholder attitudes,Stakeholder engagement,Standard metadata,Station,Statistical analysis,Stewardship,Stream monitoring,Stream theory,Streams,Study design,Sulphur dioxide,Sunflower,Surveillance,Survey,Surveys,Sustainability,Sustainable tourism,Syntax,Systematic mapping,Systems thinking,Taxonomy,Technical Papers,Technology,Temporality,Territoriality,The National Advisory Council for Environmental Po,The state,Theoretical,Thing knowledge,Threatened species,Thresholds,Time Factors,Time scale,Toolkit,Total phosphorus,Tracing,Traditional ecological knowledge,Trailing-edge,Training,Transcription,Transferability,Transitivity,Translational research,Transparency-based trust,Trend analysis,Trend calculations,Trends,Trophic interactions,Trust,Trust Management,Trust game,Trust management,Trust mechanisms,Trust modeling,Trust ontology,Trust service,Trust testbed,Trust: psychology,Trustworthiness,Twitter,UK,Uncertainty sources,Underwater visual census,Unsystematic observations,Urban ecology,Urban ecosystems,Urban greenspace,Urban growth,Urbanization,User generated content,User-Computer Interface,Utility,VGI,Validation,Values-led conservation,Variability,Vegetation structure,Victoria,Virtual globe,Visualization,Voluntary biological monitoring,Volunteer,Volunteer data,Volunteer environmental monitoring,Volunteer monitoring,Volunteer participation,Volunteer-collected data,Volunteered geographic information,Volunteers,Volunteers: education,Volunteers: organization {\&} administration,Waist-hip ratio,Water Co,Watershed,Web 2.0,Web of trust,Web services,Web-based platforms,Wild bees,Wildlife,Wireless sensor networks,Wooded buffers,Woodland,Young Adult,Z10,Zero-determinant strategies,Zero-determinant strategy,Zoology,Zooniverse,{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_},a,a complex social-ecological system,a escala continental,abbott a,abstract the effectiveness of,academic science,accepted 11 november 2016,accepted 17 october 2014,accessibility,acoustic communication,acoustic survey,acoustics,activation models,active,activity rhythms,adaptation,adaptive dynamics,adaptive governance,adaptive management,afin de d{\'{e}}terminer les,age,aggression,agriculture,agriculture and the global,air quality monitoring,algal bloom monitoring,alien,alpine,alteration,although escalation of conflict,although this multidisciplinary perspective,altruism,amateur astronomy,amateurs,analyzed institutional norms of,and,and by diminishing the,and economics,and ending,and hostility between groups,and n,and phrases,and research,and social ecology,and the nitrogen cycle,and using local knowledge,animal communication,annotations,anonymisation,anthropogenic disturbance,anthropogenic global change,apis mellifera,app,appraisal theory,aquatic,aquatic ecosystems,aquatic plants,araschnia levana,are per-,argumentative influence,aristotelianism,artificial,artificial nest,as the core of,astrometry,at the undergraduate,attachment,attention patterns,attribute certificates,attribute importance,audiences,auditory object formation,auditory scene analysis,augmented reality,authentication,authorisation,authoritative,autocatalysis,awareness,b,bagging,baird,balance theory,base filters,bayesian confirmation theory,because,bee,bee foraging,behavior,behavior is one of,behavioral economics,behaviour,behavioural syndromes,benthic macroinvertebrates,biais sur les esp{\`{e}}ces,bias,big data,bilevel optimization,bio jet fuel,biodiversity,biodiversity conservation,biodiversity monitoring,biofuels,biological diversity,biomathematics,biomonitoring,biopsy,bipartite networks,bird migration {\'{a}} phenology,birds,bmkBeleidsgerichteMeetnetten,bonaire,boundary organization,boundary-spanning programs,bowen ratio,breeding bird survey,business relationships,but the real-world is,butions,c,cD,call count surveys,caloric restriction,camp m,canada,candidate in education at,capacity building,carbon dioxide flux,carduelis tristis,carnivores,carpodacus mexicanus,carpodacus purpureus,catalogs,catalogues,cbd indicators,cell-phone spectrometer,cellular automata,centre,change,changement climatique,changes {\'{a}} spring arrival,chimney bee,china,cicada,citizen,citizen science,citizen science data,citizen science typologies,citizen sensors,citizen-science,citizenship,civicness,civilizational analysis,clark fork river,classification,classification accuracy,classroom pipeline,climate,climate change,climate change invasions niche predict presence-on,climatewatch,clustering,co-creation sustainability interdisciplinarity tra,co-management,coastal wetlands,cognition,cognitive models,cognitive trust and reputation,coleman,collaboration,collaborative assessment,collaborative filtering,collaborative governance,collaborative monitoring,collective action,collectively autocatalytic sets,color figures,colourimetry,com,commitment,commitments,common bird monitoring,communication,communities of practice,community based participatory research,community engagement,community environmental groups,community informatics,community involvement,community temperature index,community-based,community-based monitoring,community-based water monitoring,complementary computing,composite trust,comprehensive overview of the,compromiso del,computational,computational biology,computational trust and reputation,computer games,concept design,concepts,conceptual analysis,concludes that,conditional anonymity,confidence,conflict,conjunctivitis,consensus tasks,conservation,conservation {\'{a}} public {\'{a}},considerable amounts of,consideration is,content analysis,continuous games,continuous prisoner,contributions,contributory expertise,control,control by n,convention on biological diversity,cooperation,cooperative pest control,coordinating work across space,coordinator at university of,coping,copyright 2000,coral reefs,correspondence author,corresponding editor,cougar,coupled human and natural,coupled systems,coverboard,creating a research to,credibility,crime reporting,critical habitat,cross-cultural,cross-cultural analysis,cross-platform,cross-scale dynamics,crowd-source,crowd-sourcing,crowding out,crowdsourcing,cultural authority of science,cultural heritage,cultural theory,curation,customer relationships,cyanobacteria,cyberinfrastructure,d,danaus plexippus l,dance decoding,data,data analysis,data analysis methods,data analytics,data cleaning,data cleansing,data collection,data exchange,data governance,data integration,data mining,data models,data policy,data quality,data stream management,data streams,data validity,data-,data-driven science,data-intensive science,databases,datos a lo largo,davide zilli,de grandes escalas espaciales,de largo t{\'{e}}rmino y,de observaci{\'{o}}n de comederos,debra p,deception,decision making,decision trees,decision-making,decisions,del norte county,deland,delegation,delimiting survey,democratizing science,demographics,demonstrative reason-,density,department of ecology and,deposition and nitrogen loss,descriptive and normative,detection,determinants,deterrence,development,dialectic,dietary restriction,digital arbitration,digital humanities,digitization,discipline and the agency,disciplines,disciplines con-,disengaged,distributed access,distributed ai,distribution,distribution model,distributions,distrust,divide big data apps,divided into disciplines,dlpaws,dslr cameras,e reprint,e-business,e-commerce,e-mail,eBird,eDNA,eMammal,early detection,eastern tropical pacific,eavesdropping,ebird,ec,ecological indicator,ecological niche modeling,ecology,ecology letters,economic values,economics,ecosystem functioning,ecosystem services,ecosystem-based management,ecosystems,editorial,editors,education,education and outreach,education through science,effets des observateurs et,el monitoreo efectivo de,el proyecto,elliptical and lenticular,emancipatory social science,emotion,emotions,end of theory,energy balance closure,energy storage,enjeux,environment,environment education,environmental assessment,environmental challenges,environmental communication {\'{a}} visual,environmental education,environmental justice,environmental management,environmental monitoring,environmental policy,environmental restoration,environmental science,environmental science and management,epidemiology,epistemic,epistemic logic,epistemology,erinaceidae,estuarine ecology,estuary,et exc{\`{e}}s de confiance,ethics,ethnic group,ethos of science,european wasp,eutrophication of estuaries,evo-,evolution,evolution of cooperation,evolutionary biology,evolutionary dynamics,evolutionary game theory,exoplanet survey,exotic,exotic species,expectations,experiential education,experiment,experimental design,expertise,exploratory data analysis,exploring,facteurs,fair isle,field sign surveys,field test,field-based research,final version received 20,fisheries,fl usa,foldit,for bridging local and,forest,forest birds,forest managers and scientists,formal control,formal coordination,formation,foundations of measurement,fourier analysis,fragmentation,freshwater,friendship,functional redundancy,fundamental concepts,galaxies,galaxy morphology,game theory,games,gamification,gender,general,general social survey,general social trust,general trust,generalized linear mixed models,generalized trust,genetics,geocrowdsourcing,geoff merrett,geographic data,geographic distri-,geographical,geomatica vol,global,global n-cycle,governance,graphosoma lineatum,grassland,grassland birds,gravitational lensing,grid,group selection,group-structured populations,ha,habitat associations,habitat loss,habitat suitability index models,habitat suitability modeling,haps the best known,has,hawk-dove game,health communication,health span,her son,herpetology,historical maps,holds an m,horizontal trust,host range,hound handler,hue angle,human cooperation,human evolutio,human evolution,human issues in e-commerce,humboldt county,hybrid trust model,hymenoptera {\'{a}} new,hypercycle,hyperspectral and multispectral data,hypothesis generation,iNaturalist,icle,ict,idaho,identification,ilarity,image classification,implications for practice,important topic of inquiry,in,in a recent and,in a variety of,in education from ashford,in-group,incentives engineering,including management,indicator,industrialists and academics alike,inequality,inequity aversion,info,informatics,information,information and communication technology,information quality,information systems,information transmission,information trustworthiness,ing,initiative for biodiversity,insect identification,insights into the ethos,institutional,institutional resilience,integrated management,integrating science and policy,integrity,inter-organizational conflict,interaction experts,interaction of scientists,interactive expertise,interdisciplinary research,interface design,intergroup,internet consumers,interorganizational relationship,intractable the-,introduction,invasion,invasive,invasive species,inventory,invertebrates,investment game,ipane,iphone,is a,is an integral,islam,issue of growing importance,issue-attention cycle,it is our contention,iterated games,j,jags,jennifer a,jeroen,jizz and the joy,jtrowell,july 2015,key performance indicators,kin selection,knoxville,laboratory experiment,ladder of participation,lais,lake monitoring,lakes,land cover,land stewardship,land use,latent heat flux,laughter,lawyer to help her,le d{\'{e}}nombrement des chants,leading scholars from various,learner effect,legitimacy,les inventaires fauniques sont,level,levels and dynamics,life span,likert survey,linkage mapping,linking contributor motivations to,listserv,literature,litters,local ecological knowledge,locating witnesses,location-based services,log analysis,logic and information,long-term demographic study of,long-term studies,longitudinal effects,longspur,lontra canadensis,lutionary game theory,machine learning,machine-readable table,macroecology,macroinvertebrates,made by people with,management effectiveness,many and varied,many organisations are investing,marine,marine biosecurity,marine protected areas,marine reserve,marine science,marine trophic index,mccown,mci,mdps,measurement,measurement and evaluation,measurement of human-perceived quantities,measurement philosophy,measurement science,measurement theory,mechanistic target of rapamycin,med care 2006,media studies,medical research,mediterranean sea,ment,merton,meta-analysis,methods,mice,microsatellite,microtasks,middle east,migratory strategy {\'{a}} climate,miner bee,mirror self recognition,mis- and distrust,misinformation,mission,mobile,mobile applications,mobile development,mobile phone,mobile phones,mobility,model,model inspection,models,modern science,monarch butterfly,monarch butter{\ss}ies,monitoring,monitoring improvement studies,monitoring plan,monitoring protocol,monte carlo planning,moran process,morbidity climate services digital,moreover,motivation,mountain goats,mpa,mpas,ms,multi-agent systems,multiagent systems,multiagent-systems,multidisciplinaire,multilevel analysis,multilevel modeling,multimetric index,multiparty monitoring,museum management,mycoplasma gallisepticum,n cycle,naming,narrow expertise have,national mapping organizations,national parks,natural history,natural resource management,natural resources,natural sounds,nature,nature of science,nature of science education,nearctic river,nearctic river otter,neogeography,net radiation,network analysis,networking naturalists,neuroeconomics,neuroendocrinology,new,new zealand,nick,nitrogen,nitrogen and land,nitrogen cycle,nitrogen-containing trace gases,nivel continental requiere el,nkhwanana,nl,no,non-d{\'{e}}tection et de faux,non-indigenous,non-interference,nondetection,norms of science,northern california,not neatly,november 2014,null data,observation,observations,observer,observer effects,occupancy,occupancy model,occupancy modelling,ocean colour remote sensing,of cooperative or altruistic,of insight in,of migratory insects,of pattern recognition,of science,of streams,ommunity engagement through the,omy,on social and political,online citizen science,online-only material,ontologies,open source,openstreetmap,optical transients,oreamnos americanus,organizational change,organizational relations,out-group trust,outcomes,outreach,oxyalert,oxytocin,p,paradigms,parrot,partial dependence function,participation,participatory gis,participatory inventory,participatory processes,participatory sensing,particular social trust,partnerships,pate in research,pavlov,perception,performance measurement,performance measurement is an,performative contradiction,personal data,personal value,personality differences,pest,pest management,pet,peter nannestad,peters,phenology,philosophy of information,photo elicitation,phytoestrogens,planetary science,planning,please address correspondence,please do not quote,po box 47,poblaciones de aves a,policing,policy,political ideology,political institutions,political trust,pollution measurement,polygon consensus,population ecology,population index,population management,population monitoring,population trend,populations,positifs,post-academic science,pre-publication 2011,predictor and response variables,prejudice,present address,primary biodiversity data,primates,prisoner,prisoners dilemma,privacy,private and public,procedural justice,produsers,profile sim-,program effectiveness,programming,project feederwatch,propagation,property crimes,protected areas,provenance,psychology,psychometric paradigm,public,public engagement,public engagement in science,public engagement {\'{a}},public goods game,public participation in research,public participation in scientific research,public scientific literacy,public understanding of science,public value,publication process,publics,published 22 december 2014,pups,pvc,python,p{\'{u}}blico para poder adquirir,quality,quality assurance,qualit{\'{e}} des inventaires par,rapid response,rapid survey,rares,rational distrust,rationality,rea marina protegida,reasoner,received 2 february 2015,received 31 august 2016,received 4 july 2014,reciprocal,reciprocal altruism,reciprocity,recommender systems,recovery planning,reef fish,reflect all dimensions of,regional assess-,relationship management,relevance,reliance,renewable energy,replacement cost,replicator dynamics,repositories,representativity,reputation,reputation management,research,resilience,resource implementing measures that,resource management,resource selection functions,responsible citizenship,responsible environmental behavior,restless legs syndrome,restora-,restoration,resumen,review,reviewing,revised 10 november 2016,revised 16 october 2014,revised manuscript accepted 29,riddiford,risk,risk assessment,risk communication,risk management,risk perception,rls,rls-5,robert merton,robert merton described and,rocky-reef fishes,rr,rumor spreading,r{\'{e}}sum{\'{e}},s,s confidence,s contributions to the,s dilemma,s mccall outdoor science,s pipit,s sparrow,sabone,salt marsh,sampling bias,sampling error,sanctions,sanctuaries,sanctuary,sarah rose engel and,scalar similarity,scale,scale development,schon is the program,school,schoolton,science,science and,science communication,science education,science in several,science policy,science-society-policy interactions,scientific,scientific citizenship,scientific consensus on,scientific ethos,scientific exchange,scientific literacy,scientists,scotland,scuba divers,sea,sea-level rise,seagrass,seagrass watch,search citizen science programme,seasonal distributions,seasonal migration,seastar,secure system design,security,security policy,security properties,seeing,semantic similarity,semantic web,semantics,semantics of trust,sensible heat flux,sensitivity,sequential data,seyfert,sharing,sharks,she hit upon mr,she is a ph,she was so im-,sheep farmers,shetland ze2 9ju,sider trust in others,signaling,simulation,sirtuins,small,smart crowdsourcing,smartphone,social affiliation,social and,social bonding,social capital,social complexity,social dilemma,social dilemmas,social epistemology,social group,social learning,social machines,social media,social network,social networking,social networks,social sciences computing virtual reality distrust,social-ecological systems,social-media content,socioecological systems,sociology,sociology of science were,soci{\'{e}}t{\'{e}},soil heat flux,sons ltd,source water protection,spatial,spatial and temporal resolution,spatial data,spatial data infrastructure,spatial data quality,spatial ecology,spatial games,spatially,spatially explicit,species distribution database,species distribution models,species identification,species occurrence,specimens,spectral bands,speech act,spiral,spiritual values",sprague,spreading,spreadsheets,sta,standards,starting in the mid-1930s,state of the environment,statistical,statistics,stellar variability,stetson university,stewardship,stochastic simulations,strategy dynamics,streams,strong,strongly defined,structural svm,structured,subjective logic,subsistence,subtropical reefs,sujets aux erreurs de,superfund project,surveillance,survey,sustainabiltiy,sustainable development,sustainable use,sweden,systems,tacit presupposition,tagging,taib,tangible,target of rapamycin,technical decision-making,technology {\'{a}},temporal trends,tennessee 37996,text-mining,texte {\`{a}} consulter,that similar practices operate,thcoming in social studies,the baltic sea,the effectiveness of using,the establishment of a,the most enduring and,the netherlands,the origin and maintenance,the question of trust,the religious,their performance,theory of measurement,they extend from unique,this paper reports on,tidal restoration,time,time-domain astron-,tion practitioners can,tit-for-tat,to,to build trust in,tolerance,topological theory of measurement,topology,tourism,tracking,traditional ecological knowledge,traditional knowledge,tragedy of the commons,tragedy of the commune,transformative learning,transit method,transitory obstacles,trends,trolling the internet,trust,trust conceptualization,trust contexts,trust evaluation,trust game,trust has become an,trust in medical researchers,trust in the police,trust m etrics,trust management,trust metrics,trust modeling,trust propagation,trust radius,trust specification,trust testbed,trust transfer,trust visuals,trusted communication,trustee,trustfulness,trusting behavior,trustor,trustworthiness,turbulent flux,two-stage game,uk naturalist arts of,un programa de monitoreo,uncertainties in trust,ungulate,uni-,university,university and a b,university of idaho and,until in press,urban sensing,urbanization,user,user evaluation,user interaction,utility,variable investment,vegetation replacement,veraart,version,versity of tennessee,vertical trust,vespula germanica,vgi,violence,virginia polytechnic and state,virtuosity,visual census,visual inspection,visualization,voluminous and rapidly expanding,voluntary,volunteer,volunteer monitoring,volunteered geographic,volunteered geographic information,volunteered geographic information (VGI),volunteered geographical,volunteered geographical information (VGI),volunteering geographic information to,volunteers,volunteers for biological monitoring,volunteers in research,wageningen university and research,waggle dance,was looking for a,water beetles,water contaminant analysis,water interactions,water quality,water quality monitoring,watershed management,watershed organizations,waveform analysis,weakly defined,web 2,web and,web observatories,web of trust,western pacific,wide reaching impacts,widely defined,wildland fire,wildlife conservation,willingness to cooperate,willingness to participate in,wilson,win custody of,with his doctoral dissertation,wur,www,xenoestrogens,y temporales,zealand {\'{a}} observation,zero-inflated,zooniverse,{\textordfeminine} 2016 john wiley,{\'{a}}},
pmid = {25246403},
title = {{Design for Citizen Science}},
year = {2015}
}
@article{Yasui2015,
author = {Yasui, Masahiko and Cassinelli, Alvaro and Okumura, Kohei and Oku, Hiromasa and Ishikawa, Masatoshi},
doi = {10.18974/tvrsj.20.1_55},
issn = {1344-011X},
journal = {Transactions of the Virtual Reality Society of Japan},
keywords = {display on moving object,laser,single scan,visual afterimage},
title = {{Proposal and fundamental study of a display method relying on afterimage and a flying tracked object as support for projection}},
year = {2015}
}
@article{Takahashi2013,
author = {Takahashi, Yoichi and Izadi, Shahram and Holtzman, Henry and Raskar, Ramesh and Bonnard, Quentin and Legge, Amanda and Geiduschek, Anna and Kaplan, Fr{\'{e}}d{\'{e}}ric and Dillenbourg, Pierre and Matoba, Yasushi and Koike, Hideki and Cassinelli, Alvaro and Angesleva, Jussi and Watanabe, Yoshihiro and Frasca, Gonzalo and Ishikawa, Masatoshi and Hirsch, Matthew},
doi = {10.1145/2451856.2451859},
issn = {10725520},
journal = {interactions},
title = {{Demo hour}},
year = {2013}
}
@inproceedings{Chavel2000,
author = {Chavel, Pierre H. and Cassinelli, Alvaro and Glaser, I.},
booktitle = {Optics in Computing 2000},
doi = {10.1117/12.386805},
issn = {0277786X},
title = {{{\textless}title{\textgreater}Optoelectronic cellular automata for video real-time vision{\textless}/title{\textgreater}}},
year = {2000}
}

@InProceedings{Fujimoto2015,
  author    = {Fujimoto, Yuichiro and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title     = {{[POSTER] Pseudo printed fabrics through projection mapping}},
  booktitle = {Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2015},
  year      = {2015},
  abstract  = {Projection-based Augmented Reality commonly projects on rigid objects, while only few systems project on deformable objects. In this paper, we present Pseudo Printed Fabrics (PPF), which enables the projection on a deforming piece of cloth. This can be applied to previewing a cloth design while manipulating its shape. We support challenging manipulations, including heavy occlusions and stretching the cloth. In previous work, we developed a similar system, based on a novel marker pattern; PPF extends it in two important aspects. First, we improved performance by two orders of magnitudes to achieve interactive performance. Second, we developed a new interpolation algorithm to keep registration during challenging manipulations. We believe that PPF can be applied to domains including virtual-try on and fashion design.},
  doi       = {10.1109/ISMAR.2015.51},
  isbn      = {9781467376600},
  keywords  = {H.5.1 [Information Interfaces and Presentation],I.4.8 [Image Processing and Computer Vision],Multimedia Information Systems-Artificial, augmented, and virtual realities,Scene Analysis-Tracking;},
}

@Article{Ericson2015,
  author   = {Ericson, Marc and Santos, C. and Polvi, Jarkko and Taketomi, Takafumi and Yamamoto, Goshiro and Sandor, Christian and Kato, Hirokazu},
  title    = {{*TE 3. EXCELLENT Toward Standard Usability Questionnaires for Handheld AR SCRVD}},
  journal  = {IEEE Computer Graphics and Applications},
  year     = {2015},
  abstract = {Usability evaluations are important to improving handheld augmented reality (HAR) systems. However, no standard questionnaire considers perceptual and ergonomic issues found in HAR. The authors performed a systematic literature review to enumerate these issues. Based on these issues, they created a HAR usability scale that consists of comprehensibility and manipulability scales. These scales measure general system usability, ease of understanding the information presented, and ease of handling the device. The questionnaires' validity and reliability were evaluated in four experiments, and the results show that the questionnaires consistently correlate with other subjective and objective measures of usability. The questionnaires also have good reliability based on the Cronbach's alpha. Researchers and professionals can directly use these questionnaires to evaluate their own HAR applications or modify them with the insights presented in this article.},
  doi      = {10.1109/MCG.2015.94},
  keywords = {augmented reality,computer graphics,handheld augmented reality,information interfaces,usability engineering,virtual reality},
  pmid     = {26416363},
}

@InProceedings{Hilliges2004,
  author    = {Hilliges, Otmar and Sandor, Christian and Klinker, Gudrun},
  title     = {{A Lightweight Approach for Experimenting with Tangible Interaction Metaphors}},
  booktitle = {Proceedings of the International Workshop on Multi-user and Ubiquitous User Interfaces (MU3I)},
  year      = {2004},
  abstract  = {Interaction techniques for Augmented Reality user interfaces (UIs) differ considerably from well explored 2D UIs, be-cause these include new in-/output devices and new inter-action metaphors such as tangible interaction. For experimenting with new devices and metaphors we pro-pose a flexible and lightweight UI framework that supports rapid prototyping of multimodal and collaborative UIs. We use the DWARF 1 framework as foundation. It allows us to build highly dynamic systems enabling the exchange of components at runtime.},
}

@Article{Sandor2005,
  author   = {Sandor, Christian and Klinker, Gudrun},
  title    = {{A rapid prototyping software infrastructure for user interfaces in ubiquitous augmented reality}},
  journal  = {Personal and Ubiquitous Computing},
  year     = {2005},
  issn     = {16174909},
  abstract = {Recent user interface concepts, such as multimedia, multimodal, wearable, ubiquitous, tangible, or augmented-reality-based (AR) interfaces, each cover different approaches that are all needed to support complex human-computer interaction. Increasingly, an overarching approach towards building what we call ubiquitous augmented reality (UAR) user interfaces that include all of the just mentioned concepts will be required. To this end, we present a user interface architecture that can form a sound basis for combining several of these concepts into complex systems. We explain in this paper the fundamentals of DWARF's user interface framework (DWARF standing for distributed wearable augmented reality framework) and an implementation of this architecture. Finally, we present several examples that show how the framework can form the basis of prototypical applications. {\textcopyright} Springer-Verlag London Limited 2005.},
  doi      = {10.1007/s00779-004-0328-1},
  keywords = {Augmented reality,Frameworks,Mobile systems,Multimodality,Software architectures,Tangible user interfaces,Ubiquitous computing},
}

@InProceedings{Yamamoto2015,
  author    = {Yamamoto, Goshiro and Hyry, Jaakko and Krichenbauer, Max and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu and Pulli, Petri},
  title     = {{A user interface design for the elderly using a projection tabletop system}},
  booktitle = {2015 3rd IEEE VR International Workshop on Virtual and Augmented Assistive Technology, VAAT 2015},
  year      = {2015},
  abstract  = {We present a design of a user interface on a projection tabletop system for the elderly. Nowadays, the population imbalance between the elderly who require daily care and caregivers is being one of global social issues since the population aging advances. Additionally, if the amount of the elderly with memory problem increases, the imbalanced situation makes the elderly worse because of few caregivers, then a negative chain reaction would be occurred. In order to avoid the worst situation, we believe that it is important to support the daily lives of the elderly persons at the early stage for reducing aging effects. In this research, we challenge to develop the internet-connected assistive system to support their daily lives individually in order to help them manage their time without the need for caregivers support on location. We focus on projection technology to display visual information onto physical surfaces such as tables or walls especially for indoor use. The projection technology does not require embedding other displays such as LCD into tables or walls that the users have already owned, and to hold or wear the special devices in hands or on heads. Also we apply interactive functions for the design that lets the users think and decide by themselves, since the quality of lives of the elderly needs to be considered to keep them from just following the given instruction without thinking. Finally, we have decided three designs of graphical user interface on the projection tabletop system through a brief user test. In this paper we show the thee different interaction methods based on the graphical user interface designs and one of augmented reality applications as an example of a practical use of the system.},
  doi       = {10.1109/VAAT.2015.7155407},
  isbn      = {9781467365185},
  keywords  = {Assistive technology,Conferences},
}

@Article{Geiger2018,
  author  = {Geiger, Andreas and Valkov, Dimitar and Sandor, Christian and Johnsen, Kyle and Serafin, Stefania},
  title   = {{ACM SUI 2018 welcome message}},
  journal = {SUI 2018 - Proceedings of the Symposium on Spatial User Interaction},
  year    = {2018},
  isbn    = {9781450357081},
}

@InProceedings{Novak2004,
  author    = {Novak, Vinko and Sandor, Christian and Klinker, Gudrun},
  title     = {{An AR workbench for experimenting with Attentive User Interfaces}},
  booktitle = {ISMAR 2004: Proceedings of the Third IEEE and ACM International Symposium on Mixed and Augmented Reality},
  year      = {2004},
  abstract  = {We present a workbench to build, evaluate and iteratively develop user interfaces using augmented reality, eye tracking, and visual programming. To test our system, we have developed an Attentive User Interface (AUI)for automotive environments, which coordinates its activities based on the context and visual attention of the user. Development of AUI requires interdisciplinary teams like psychologists, human-factor engineers, designers and computer scientists to work together. The main problem of interdisciplinary communication is a lack of common language and different notion of the system. We have developed a workbench, which facilitates the communication between the team members and enhances the comprehension of the system by visualizing users' attention and system reactions. {\textcopyright} 2004 IEEE.},
  doi       = {10.1109/ISMAR.2004.12},
  isbn      = {0769521916},
}

@InProceedings{Heinrich2008,
  author    = {Heinrich, Marko and Thomas, Bruce H. and Mueller, Stefan and Sandor, Christian},
  title     = {{An augmented reality weather system}},
  booktitle = {Proceedings of the 2008 International Conference on Advances in Computer Entertainment Technology, ACE 2008},
  year      = {2008},
  abstract  = {This paper presents ARWeather, a simulation application, which can simulate three types of precipitation: rain, snow, and hail. We examined a range of weather phenomenon and how they may be simulated in a mobile augmented reality system. ARWeather was developed and deployed on the Tinmith wearable computer system to enable autonomous and free movement for the user. The user can move freely inside the simulated weather without limitation. The result of this work, the ARWeather application, has been evaluated with a user study to determine the user's acceptance and draw conclusions to the applicability of augmented reality simulated weather. {\textcopyright} 2008 ACM.},
  doi       = {10.1145/1501750.1501790},
  isbn      = {9781605583938},
  keywords  = {Augmented reality,Design,Human factors,Multimodal weather simulation},
}

@InProceedings{Sandor2010,
  author    = {Sandor, Christian and Cunningham, Andrew and Dey, Arindam and Mattila, Ville Veikko},
  title     = {{An augmented reality X-ray system based on visual saliency}},
  booktitle = {9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings},
  year      = {2010},
  abstract  = {In the past, several systems have been presented that enable users to view occluded points of interest using Augmented Reality X-ray visualizations. It is challenging to design a visualization that provides correct occlusions between occluder and occluded objects while maximizing legibility. We have previously published an Augmented Reality X-ray visualization that renders edges of the occluder region over the occluded region to facilitate correct occlusions while providing foreground context. While this approach is simple and works in a wide range of situations, it provides only minimal context of the occluder object. In this paper, we present the background, design, and implementation of our novel visualization technique that aims at providing users with richer context of the occluder object. While our previous visualization only employed one salient feature (edges) to determine which parts of the occluder to display, our novel visualization technique is an initial attempt to explore the design space of employing multiple salient features for this task. The prototype presented in this paper employs three additional salient features: hue, luminosity, and motion. We have conducted two evaluations with human participants to investigate the benefits and limitations of our prototype compared to our previous system. The first evaluation showed that although our novel visualization provides a richer context of the occluder object, it does not impede users to select objects in the occluded area; but, it also indicated problems in our prototype. In the second evaluation, we have investigated these problems through an online survey with systematically varied occluder and occluded scenes, focussing on the qualitative aspects of our visualizations. The results were encouraging, but pointed out that our novel visualization needs a higher level of adaptiveness. {\textcopyright}2010 IEEE.},
  doi       = {10.1109/ISMAR.2010.5643547},
  isbn      = {9781424493449},
  keywords  = {Augmented reality,Augmented reality X-ray,Evaluation,Saliency,Visualization},
}

@InProceedings{Maier2012,
  author   = {Maier, Patrick and Dey, Arindam and Waechter, Christian A. L. and Sandor, Christian and Tonnis, Marcus and Klinker, Gudrun},
  title    = {{An empiric evaluation of confirmation methods for optical see-through head-mounted display calibration}},
  year     = {2012},
  abstract = {The calibration of optical see-through head-mounted displays is an important fundament for correct object alignment in augmented reality. Any calibration process for OSTHMDs requires users to align 2D points in screen space with 3D points in the real world and to confirm each alignment. In this poster, we present the results of our empiric evaluation where we compared four confirmation methods: Keyboard, Hand-held, Voice, and Waiting. The Waiting method, designed to reduce head motion during confirmation, showed a significantly higher accuracy than all other methods. Averaging over a time frame for sampling user input before the time of confirmation improved the accuracy of all methods in addition. We conducted a further expert study proving that the results achieved with a video see-through head-mounted display showed valid for optical see-through head-mounted display calibration, too.},
  doi      = {10.1109/ismar.2011.6162910},
}

@InProceedings{Akiyama2016,
  author    = {Akiyama, Ryo and Yamamoto, Goshiro and Amano, Toshiyuki and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title     = {{Appearance control in dynamic light environments with a projector-camera system}},
  booktitle = {2016 IEEE VR 2016 Workshop on Perceptual and Cognitive Issues in AR, PERCAR 2016},
  year      = {2016},
  abstract  = {Appearance of objects change from their original colors according to a lighting condition. Nowadays, a projector-camera system can act a tool to control colors or textures of objects in each small area divided in pixel level by light projection onto their physical surfaces. Conventional system can change the appearance in realtime under constant environmental lights. However, the change of environmental lights also influence the appearance. Therefore, we propose an advanced appearance control method considering the change of the environmental lights. In particular, we use a projector-camera system consisting of two cameras and one 3D projector. In addition, we apply liquid crystal shutter filters in order to estimate reflectances and variations of environmental lights separately simultaneously in real-time. In this paper, we show our theory for the estimation and results under different conditions of environmental lights.},
  doi       = {10.1109/PERCAR.2016.7562418},
  isbn      = {9781509008384},
  keywords  = {H.5.1 [Information Interfaces and Presentation]: Multimedia and Informations Systems-Artificial,I.4 [Image Processing and Computer Vision]:Scene Analysis-Color,and virtual realities,augmented},
}

@InProceedings{Plopski2018,
  author    = {Plopski, Alexander and Mori, Ryosuke and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title     = {{AR-PETS: Development of an augmented reality supported pressing evaluation training system}},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year      = {2018},
  abstract  = {With age, changes to the human nervous system lead to a decrease in control accuracy of extremities. Especially, reduced control over the fingers gravely affects a person's quality of life and self-reliance. It is possible to recover the ability to accurately control the amount of power applied with one's fingers through training with the Pressing Evaluation Training System (PETS). When training with the PETS users have to focus on guidance provided on a monitor and lose sight of their fingers. This could lead to increased mental workload and reduced training efficiency. In this paper we explore if presenting the guidelines closer to the user's fingers provides better guidance to the user, thus improving the performance results. In particular, we use a video-see-through head-mounted display to present guidance next to the user's fingers through augmented reality (AR), and a haptic device to replicate the tasks during PETS training. We test our implementation with 18 university students. Although the results of our study indicate that presenting information closer to the interaction area does not improve the performance, several participants preferred guidance presented in AR.},
  doi       = {10.1007/978-3-319-92037-5_10},
  isbn      = {9783319920368},
  issn      = {16113349},
  keywords  = {Augmented reality,Elderly training,Force sensing,Haptic device,Pressing evaluation training system,Video see-through head-mounted display},
}

@Article{Santos2016,
  author   = {Santos, Marc Ericson C. and L{\"{u}}bke, Arno in Wolde and Taketomi, Takafumi and Yamamoto, Goshiro and Rodrigo, Ma Mercedes T. and Sandor, Christian and Kato, Hirokazu},
  title    = {{Augmented reality as multimedia: the case for situated vocabulary learning}},
  journal  = {Research and Practice in Technology Enhanced Learning},
  year     = {2016},
  issn     = {17937078},
  abstract = {Augmented reality (AR) has the potential to create compelling learning experiences. However, there are few research works exploring the design and evaluation of AR for educational settings. In our research, we treat AR as a type of multimedia that is situated in authentic environments and apply multimedia learning theory as a framework for developing our educational applications. We share our experiences in developing a handheld AR system and one specific use case, namely, situated vocabulary learning. Results of our evaluations show that we are able to create AR applications with good system usability. More importantly, our preliminary evaluations show that AR may lead to better retention of words and improve student attention and satisfaction.},
  doi      = {10.1186/s41039-016-0028-2},
  keywords = {Augmented reality,Multimedia learning,Ubiquitous learning,Vocabulary learning},
}

@Article{Krichenbauer2018,
  author   = {Krichenbauer, Max and Yamamoto, Goshiro and Taketom, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title    = {{Augmented Reality versus Virtual Reality for 3D Object Manipulation}},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2018},
  issn     = {10772626},
  abstract = {Virtual Reality (VR) Head-Mounted Displays (HMDs) are on the verge of becoming commodity hardware available to the average user and feasible to use as a tool for 3D work. Some HMDs include front-facing cameras, enabling Augmented Reality (AR) functionality. Apart from avoiding collisions with the environment, interaction with virtual objects may also be affected by seeing the real environment. However, whether these effects are positive or negative has not yet been studied extensively. For most tasks it is unknown whether AR has any advantage over VR. In this work we present the results of a user study in which we compared user performance measured in task completion time on a 9 degrees of freedom object selection and transformation task performed either in AR or VR, both with a 3D input device and a mouse. Our results show faster task completion time in AR over VR. When using a 3D input device, a purely VR environment increased task completion time by 22.5 percent on average compared to AR (p {\textless}0.024). Surprisingly, a similar effect occurred when using a mouse: users were about 17.3 percent slower in VR than in AR ( p{\textless}0.04 ). Mouse and 3D input device produced similar task completion times in each condition (AR or VR) respectively. We further found no differences in reported comfort.},
  doi      = {10.1109/TVCG.2017.2658570},
  keywords = {Artificial,and virtual realities-multimedia information systems-information interfaces and representation,augmented,interaction techniques-methodology and techniques-computer graphics},
}

@InProceedings{Santos2014b,
  author    = {Santos, Marc Ericson C. and Ty, Jayzon F. and Luebke, Arno In Wolde and Rodrigo, Ma Mercedes T. and Taketomi, Takafumi and Yamamoto, Goshiro and Sandor, Christian and Kato, Hirokazu},
  title     = {{Authoring augmented reality as situated multimedia}},
  booktitle = {Proceedings of the 22nd International Conference on Computers in Education, ICCE 2014},
  year      = {2014},
  abstract  = {Augmented reality (AR) is an enabling technology for presenting information in relation to real objects or real environments. AR is situated multimedia or information that is positioned in authentic physical contexts. In this paper, we discuss how we address issues in creating AR content for educational settings. From the learning theory perspective, we explain that AR is a logical extension of multimedia learning theory. From the development perspective, we demonstrate how AR content can be created through our in situ authoring tool and our platform for handheld AR.},
  isbn      = {9784990801410},
  keywords  = {Augmented reality,Authoring tool,Multimedia learning,Situated learning},
}

@InProceedings{Mori2017,
  author    = {Mori, Shohei and Ikeda, Sei and Sandor, Christian and Plopski, Alexander},
  title     = {{BrightView: Increasing Perceived Brightness in Optical See-Through Head-Mounted Displays}},
  booktitle = {Adjunct Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2017},
  year      = {2017},
  abstract  = {Virtual content on optical see-through head-mounted displays (OST-HMDs) appears dim in bright environments. In this paper, we demonstrate how a liquid crystal (LC) filter can be used to dynamically increase the perceived brightness of the virtual content. Continuously adjusting the LC filter opacity attenuates the real scene and increases the perceived brightness without being noticed by the user. The results of our psychophysical experiment with 16 participants validate our prototype OST-HMD. Our design could be combined with existing and future OST-HMDs to improve the visibility of the virtual content in augmented reality.},
  doi       = {10.1109/ISMAR-Adjunct.2017.66},
  isbn      = {9780769563275},
  keywords  = {Brightness adaptation,Illumination shedding,Liquid crystal visor,OST-HMD,Optical see-through displays},
}

@InProceedings{Mori2018,
  author    = {Mori, Shohei and Ikeda, Sei and Plopski, Alexander and Sandor, Christian},
  title     = {{BrightView: Increasing Perceived Brightness of Optical See-Through Head-Mounted Displays Through Unnoticeable Incident Light Reduction}},
  booktitle = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
  year      = {2018},
  abstract  = {Optical See-Through Head-Mounted Displays (OST-HMDs) lose the visibility of virtual contents under bright environment illumination due to their see-Through nature. We demonstrate how a liquid crystal (LC) filter attached to an OST-HMD can be used to dynamically increase the perceived brightness of virtual content without impacting the perceived brightness of the real scene. We present a prototype OST-HMD that continuously adjusts the opacity of the LC filter to attenuate the environment light without users becoming aware of the change. Consequently, virtual content appears to be brighter. The proposed approach is evaluated in psychophysical experiments in three scenes, with 16, 31, and 31 participants, respectively. The participants were asked to compare the magnitude of brightness changes of both real and virtual objects, before and after dimming the LC filter over a period of 5, 10, and 20 seconds. The results showed that the participants felt increases in the brightness of virtual objects while they were less conscious of reductions of the real scene luminance. These results provide evidence for the effectiveness of our display design. Our design can be applied to a wide range of OST-HMDs to improve the brightness and hence realism of virtual content in augmented reality applications.},
  doi       = {10.1109/VR.2018.8446441},
  isbn      = {9781538633656},
  keywords  = {Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality Interaction devices Displays and imagers},
}

@InProceedings{Weir2012,
  author    = {Weir, Peter and Sandor, Christian and Swoboda, Matt and Nguyen, Thanh and Eck, Ulrich and Reitmayr, Gerhard and Dey, Arindam},
  title     = {{BurnAR: Feel the heat}},
  booktitle = {ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers},
  year      = {2012},
  abstract  = {Augmented Reality systems that run interactively and in real time, using high quality graphical displays and sensational cues, can create the illusion of virtual objects appearing to be real. This paper presents the design and implementation of BurnAR, a demonstration which enables users to experience the illusion of seeing their own hands burning, which we achieve by overlaying virtual flames and smoke on their hands. Surprisingly, some users reported an involuntary warming sensation of their hands. {\textcopyright} 2012 IEEE.},
  doi       = {10.1109/ISMAR.2012.6402599},
  isbn      = {9781467346603},
  keywords  = {H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems - [Artificial,augmented and virtual realities] H.1.2. [Information Systems]: Models and Principles - [Human factors]},
}

@InProceedings{Weir2013,
  author    = {Weir, Peter and Sandor, Christian and Swoboda, Matt and Nguyen, Thanh and Eck, Ulrich and Reitmayr, Gerhard and Day, Arindam},
  title     = {{Burnar: Involuntary heat sensations in augmented reality}},
  booktitle = {Proceedings - IEEE Virtual Reality},
  year      = {2013},
  abstract  = {Augmented Reality systems that run interactively and in real time, using high quality graphical displays and sensational cues, can create the illusion of virtual objects appearing to be real. This paper presents the design, implementation, and evaluation of BurnAR, a novel demonstration which enables users to experience the illusion of seeing their own hands burning, which we achieve by overlaying virtual flames and smoke on their hands. Surprisingly, some users reported an involuntary warming sensation of their hands. Based on these comments, we hypothesized that stimulation of multiple sensory modalities presented in this AR environment can induce an involuntary experience in an additional sensory pathway: observation of virtual flames resulting in a heat sensation. This cross-modal transfer, known as virtual synesthe-sia, is a temporary experience which affects some people who are not synesthetes and only lasts for a short time during the illusory experience. To verify our hypothesis, we conducted an exploratory study where participants experienced the BurnAR demonstration under controlled conditions. {\textcopyright} 2013 IEEE.},
  doi       = {10.1109/VR.2013.6549357},
  isbn      = {9781467347952},
  keywords  = {H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems-[Artificial, augmented and virtual realities],H.l.2. [Information Systems]: Models and Principles-[Human factors]},
}

@InProceedings{Lugtenberg2016,
  author    = {Lugtenberg, Geert and Sandor, Christian and H{\"{u}}rst, Wolfgang and Plopski, Alexander and Taketomi, Takafumi and Kato, Hirokazu},
  title     = {{Changing perception of physical properties using multimodal augmented reality: Position paper}},
  booktitle = {Proceedings of the 2016 Workshop on Multimodal Virtual and Augmented Reality, MVAR 2016 - In conjunction with ACM ICMI 2016},
  year      = {2016},
  abstract  = {By means of augmented reality (AR) systems it has become increasingly easy to manipulate our perception of real objects. In this position paper we review existing work that changes physical property perception, and propose methods for changing perceived object density during haptic interaction. Our goal is to modulate the sound emitted by an object when touched. We hypothesize that augmented sound can make people think that an object is hollow or solid regardless of its actual object density. We describe an experiment to validate this hypothesis. Participants tap a hollow or solid cube and are asked to determine if it is hollow or solid, based on the multimodal feedback. This research is a first step towards an AR system that can alter multimodal perception of object physical properties, and open doors for related research.},
  doi       = {10.1145/3001959.3001968},
  isbn      = {9781450345590},
  keywords  = {Audio feedback,Augmented reality,Haptic interaction,Multimodality,Object density,Physical property perception},
}

@InProceedings{Csongei2012,
  author    = {Csongei, Michael and Hoang, Liem and Eck, Ulrich and Sandor, Christian},
  title     = {{ClonAR: Rapid redesign of real-world objects}},
  booktitle = {ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers},
  year      = {2012},
  abstract  = {ClonAR enables users to rapidly clone and edit real-world objects. First, real-world objects can be scanned using KinectFusion. Second, users can edit the scanned objects in our visuo-haptic Augmented Reality environment. Our whole pipeline does not use mesh representation of objects, but rather Signed Distance Fields, which are the output of KinectFusion. We directly render Signed Distance Fields haptically and visually. We do direct haptic rendering of Signed Distance Fields, which is faster and more flexible than rendering meshes. Visual rendering is performed by our custom-built raymarcher, which facilitates realistic illumination effects like ambient occlusions and soft shadows. Our prototype demonstrates the whole pipeline. We further present several results of redesigned real-world objects. {\textcopyright} 2012 IEEE.},
  doi       = {10.1109/ISMAR.2012.6402572},
  isbn      = {9781467346603},
  keywords  = {H.1.2. [Information Systems]: Models and Principles - [Human factors],H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems - [Artificial, augmented and virtual realities],H.5.2. [Information Interfaces and Presentation]: User Interfaces - [Haptic I/O ]},
}

@InProceedings{Eck2014,
  author    = {Eck, Ulrich and Pankratz, Frieder and Sandor, Christian and Klinker, Gudrun and Laga, Hamid},
  title     = {{Comprehensive workspace calibration for visuo-haptic augmented reality}},
  booktitle = {ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings},
  year      = {2014},
  abstract  = {Visuo-haptic augmented reality systems enable users to see and touch digital information that is embedded in the real world. Precise co-location of computer graphics and the haptic stylus is necessary to provide a realistic user experience. PHANToM haptic devices are often used in such systems to provide haptic feedback. They consist of two interlinked joints, whose angles define the position of the haptic stylus and three sensors at the gimbal to sense its orientation. Previous work has focused on calibration procedures that align the haptic workspace within a global reference coordinate system and developing algorithms that compensate the non-linear position error, caused by inaccuracies in the joint angle sensors. In this paper, we present an improved workspace calibration that additionally compensates for errors in the gimbal sensors. This enables us to also align the orientation of the haptic stylus with high precision. To reduce the required time for calibration and to increase the sampling coverage, we utilize time-delay estimation to temporally align external sensor readings. This enables users to continuously move the haptic stylus during the calibration process, as opposed to commonly used point and hold processes. We conducted an evaluation of the calibration procedure for visuo-haptic augmented reality setups with two different PHANToMs and two different optical trackers. Our results show a significant improvement of orientation alignment for both setups over the previous state of the art calibration procedure. Improved position and orientation accuracy results in higher fidelity visual and haptic augmentations, which is crucial for fine-motor tasks in areas including medical training simulators, assembly planning tools, or rapid prototyping applications. A user friendly calibration procedure is essential for real-world applications of VHAR.},
  doi       = {10.1109/ISMAR.2014.6948417},
  isbn      = {9781479961849},
  keywords  = {H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems - [Artificial,H.5.2. [Information Interfaces and Presentation],User Interfaces - [Haptic I/O],augmented and virtual realities]},
}

@InProceedings{Kuparinen2013,
  author    = {Kuparinen, Liisa and Swan, J. Edward and Rapson, Scott and Sandor, Christian},
  title     = {{Depth perception in tablet-based augmented reality at medium- and far-field distances}},
  booktitle = {Proceedings - SAP 2013: ACM Symposium on Applied Perception},
  year      = {2013},
  abstract  = {Current augmented reality (AR) systems often fail to indicate the distance between the user and points of interest in the environment. Empirical evaluations of human depth perception in AR settings compared to real world settings are needed. Our goal in this study was to understand tablet-based AR depth perception by comparing it with real-world depth perception. {\textcopyright} 2013 Authors.},
  doi       = {10.1145/2492494.2514654},
  isbn      = {9781450322621},
}

@Article{Hyry2017,
  author   = {Hyry, Jaakko and Krichenbauer, Max and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu and Pulli, Petri},
  title    = {{Design of assistive tabletop projector-camera system for the elderly with cognitive and motor skill impairments}},
  journal  = {ITE Transactions on Media Technology and Applications},
  year     = {2017},
  issn     = {21867364},
  abstract = {Projector-camera (ProCam) systems have a potential to become popular and affordable as they can create interactive surfaces for example on tabletops, walls, household items or on a palm of a hand. The possibility that these systems will be used at homes in the future is increasing. The elderly living alone at home often need assistance in their daily tasks as the likelihood of cognitive and motor skill related impairments increases with age. ProCam systems could be used for guidance due to easy to manipulate large interaction surfaces, but research on its suitability for elderly users is scarce. Our research focus is on elderly users and examining their characteristics as potential users of ProCam systems and the implications for interaction design. We conducted a user study with a mixed impairments group of elderly aged 82-94 to investigate how a personalized and skill-suited user interface should be designed. In our qualitative approach, we discovered that the combinations of both cognitive and motor skill deficiencies of the elderly prohibit one-for-all designs so the user interface design should be adapted to each individual's interaction skills. Lastly, we make suggestions for designing ProCam interaction for elderly.},
  doi      = {10.3169/mta.5.57},
  keywords = {Cognitive impairments,Elderly,Interaction,Motor skill impairment,Projector-camera systems},
}

@InProceedings{Rompapas2014,
  author    = {Rompapas, Damien Constantine and Sorokin, Nicholas and L{\"{u}}bke, Arno In Wolde and Taketomi, Takafumi and Yamamoto, Goshiro and Sandor, Christian and Kato, Hirokazu},
  title     = {{Dynamic augmented reality X-ray on google glass}},
  booktitle = {SIGGRAPH Asia 2014 Mobile Graphics and Interactive Applications, SA 2014},
  year      = {2014},
  abstract  = {(b) (c) (d) Figure 1: In our demonstration users A and B are separated by a wall, both wearing a Google Glass. User A views a point of interest, currently only visible to user B ((a) and (b)). User A's view is augmented with video streamed from user B when the target is partially (c) or fully (d) occluded. (Note: (c) and (d) contain simulated augmentations)},
  doi       = {10.1145/2669062.2669087},
  isbn      = {9781450318914},
  keywords  = {Augmented reality,Google glass,X-ray visualization},
}

@InProceedings{Sherstyuk2012a,
  author    = {Sherstyuk, Andrei and Dey, Arindam and Sandor, Christian and State, Andrei},
  title     = {{Dynamic eye convergence for head-mounted displays improves user performance in virtual environments}},
  booktitle = {Proceedings - I3D 2012: ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games},
  year      = {2012},
  abstract  = {In Virtual Environments (VE), users are often facing tasks that involve direct manipulation of virtual objects at close distances, such as touching, grabbing, placement. In immersive systems that employ head-mounted displays these tasks could be quite challenging, due to lack of convergence of virtual cameras. We present a mechanism that dynamically converges left and right cameras on target objects in VE. This mechanism simulates the natural process that takes place in real life automatically. As a result, the rendering system maintains optimal conditions for stereoscopic viewing of target objects at varying depths, in real time. Building on our previous work, which introduced the eye convergence algorithm [Sherstyuk and State 2010], we developed a Virtual Reality (VR) system and conducted an experimental study on effects of eye convergence in immersive VE. This paper gives the full description of the system, the study design and a detailed analysis of the results obtained. {\textcopyright} 2012 ACM.},
  doi       = {10.1145/2159616.2159620},
  isbn      = {9781450311946},
  keywords  = {hand-eye coordination,stereoscopic vision},
}

@InProceedings{Plopski2018a,
  author    = {Plopski, Alexander and Fuvattanasilp, Varunyu and Poldi, Jarkko and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title     = {{Efficient In-Situ Creation of Augmented Reality Tutorials}},
  booktitle = {2018 Workshop on Metrology for Industry 4.0 and IoT, MetroInd 4.0 and IoT 2018 - Proceedings},
  year      = {2018},
  abstract  = {With increasing complexity of system maintenance there is an increased need for efficient tutorials that support easy understanding of the individual steps and efficient visualization at the operation site. This can be achieved through augmented reality, where users observe computer generated 3D content that is spatially consistent with their surroundings. However, generating such tutorials is a tedious process, as they have to be prepared from scratch in a time consuming process. An intuitive interface that allows users to easily place annotations and models could help reduce the complexity of this task. In this paper, we discuss the design of an interface for efficient creation of 3D aligned annotations on a handheld device. We also show how our method could improve the collaboration between a local user and a remote expert in a remote support scenario.},
  doi       = {10.1109/METROI4.2018.8428320},
  isbn      = {9781538624975},
  keywords  = {Annotation,Augmented Reality,Handheld Augmented Reality,Interaction,Remote Assistance,Training},
}

@InProceedings{Sandor2010a,
  author    = {Sandor, Christian and Cunningham, Andrew and Eck, Ulrich and Urquhart, Donald and Jarvis, Graeme and Dey, Arindam and Barbier, Sebastien and Marner, Michael R. and Rhee, Sang},
  title     = {{Egocentric space-distorting visualizations for rapid environment exploration in mobile mixed reality}},
  booktitle = {Proceedings - IEEE Virtual Reality},
  year      = {2010},
  abstract  = {Most of today's mobile internet devices contain facilities to display maps of the user's surroundings with points of interest embedded into the map. Other researchers have already explored complementary, egocentric visualizations of these points of interest using mobile mixed reality. Being able to perceive the point of interest in detail within the user's current context is desirable, however, it is challenging to display off-screen or occluded points of interest. We have designed and implemented space-distorting visualizations to address these situations. While this class of visualizations has been extensively studied in information visualization, we are not aware of any attempts to apply them to augmented or mixed reality. Based on the informal user feedback that we have gathered, we have performed several iterations on our visualizations. We hope that our initial results can inspire other researchers to also investigate space-distorting visualizations for mixed and augmented reality. {\textcopyright}2010 IEEE.},
  doi       = {10.1109/VR.2010.5444815},
  isbn      = {9781424462582},
  keywords  = {H.5.1. [information interfaces and presentation]: multimedia information systems - [artificial, augmented and virtual realities] I.3.6 [computer graphics]: methodology and techniques-[interaction techniques]},
}

@InProceedings{Bork2017,
  author    = {Bork, Felix and Barmaki, Roghayeh and Eck, Ulrich and Yu, Kevin and Sandor, Christian and Navab, Nassir},
  title     = {{Empirical study of non-reversing magic mirrors for augmented reality anatomy learning}},
  booktitle = {Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2017},
  year      = {2017},
  abstract  = {Left-right confusion occurs across the entire population and refers to an impeded ability to distinguish between left and right. In medicine this phenomenon is particularly relevant as left and right are always defined with respect to the patient's point of view, i.e. the doctor's right is the patient's left. Traditional anatomy learning resources such as illustrations in textbooks naturally consider this by consistently depicting the anatomy of a patient as seen by an observer standing in front. Augmented Reality Magic Mirrors (MM) are one example of novel anatomy teaching resources and show a user's digital mirror image augmented with virtual anatomy on a large display. As left and right appear to be reversed in such MM setups, similar to real-world physical mirrors, intriguing perceptual questions arise: is a non-reversing MM (NRMM) the more natural choice for the task of anatomy learning and do users even learn anatomy the wrong way with a traditional, reversing MM (RMM)' In this paper, we explore the perceptual differences between an NRMM and RMM design and present the first empirical study comparing these two concepts for the purpose of anatomy learning. Experimental results demonstrate that medical students perform significantly better at identifying anatomically correct placement of virtual organs in an NRMM. However, interaction was significantly more difficult compared to an RMM. We explore the underlying psychological effects and discuss the implications of using an NRMM on user perception, knowledge transfer, and interaction. This study is relevant for the design of future MM systems in the medical domain and lessons-learned can be transferred to other application domains.},
  doi       = {10.1109/ISMAR.2017.33},
  isbn      = {9781538629437},
}

@InProceedings{Polvi2014,
  author    = {Polvi, Jarkko and Taketomi, Takafumi and Yamamoto, Goshiro and Billinghurst, Mark and Sandor, Christian and Kato, Hirokazu},
  title     = {{Evaluating a SLAM-based handheld augmented reality guidance system}},
  booktitle = {SUI 2014 - Proceedings of the 2nd ACM Symposium on Spatial User Interaction},
  year      = {2014},
  abstract  = {In this poster we present the design and evaluation of a Handheld Augmented Reality (HAR) prototype system for guidance.},
  doi       = {10.1145/2659766.2661212},
  isbn      = {9781450328203},
  keywords  = {Guidance,Handheld augmented reality,User study},
}

@InProceedings{Santos2014a,
  author    = {Santos, Marc Ericson C. and Luebke, Arno In Wolde and Taketomi, Takafumi and Yamamoto, Goshiro and Rodrigo, Ma Mercedes T. and Sandor, Christian and Kato, Hirokazu},
  title     = {{Evaluating augmented reality for situated vocabulary learning}},
  booktitle = {Proceedings of the 22nd International Conference on Computers in Education, ICCE 2014},
  year      = {2014},
  abstract  = {Augmented reality (AR) is an emerging technology for communicating learning contents. Several AR systems are designed for learning. However, studies that have investigated instructional strategies for applying AR are few. This investigation requires the implementation of prototypes that use state-of-the-art technology and sound learning theory. In this work, we implemented two prototypes for learning Filipino and German words by first developing a handheld AR platform. These prototypes demonstrate situated vocabulary learning. Using our AR system, students can learn words related to their current environment. We assessed the quality of these prototypes by conducting usability evaluations. For the theoretical grounding, we leveraged on multimedia learning theory to design the content. Through our handheld AR platform, we evaluated situated vocabulary learning by comparing our prototypes to a flash cards application. In the first evaluation, students scored significantly lower when using AR in an immediate post-test. However, this difference disappeared after taking into account the variability in usability scores via analysis of covariance. Taking account usability is fairer when comparing an emerging technology to traditional technology. Test scores were also not significantly different in a delayed post-test. In the second evaluation, although the post-test score and answering time of students did not differ, our results showed that they feel more satisfied and can keep their attention better when using AR. For the first time, we demonstrated situated vocabulary learning by using AR. Moreover, our preliminary study confirms the intuition that students can achieve the same score using AR, but with benefits such as ease in maintaining attention and increased satisfaction.},
  isbn      = {9784990801410},
  keywords  = {Augmented reality,Mobile learning,Situated cognition,Vocabulary learning},
}

@InProceedings{Dey2010,
  author    = {Dey, Arindam and Cunningham, Andrew and Sandor, Christian},
  title     = {{Evaluating depth perception of photorealistic Mixed Reality visualizations for occluded objects in outdoor environments}},
  booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  year      = {2010},
  abstract  = {Enabling users to accurately perceive the correct depth of occluded objects is one of the major challenges in user interfaces for Mixed Reality (MR). Therefore, several visualization techniques and user evaluations for this area have been published. Our research is focused on photorealistic X-ray type visualizations in outdoor environments. In this paper, we present an evaluation of depth perception in far-field distances through two photorealistic visualizations of occluded objects (X-ray and Melt) in the presence and absence of a depth cue. Our results show that the distance to occluded objects was underestimated in all tested conditions. This finding is curious, as it contradicts previously published results of other researchers. The Melt visualization coupled with a depth cue was the most accurate among all the experimental conditions Copyright {\textcopyright} 2010 by the Association for Computing Machinery, Inc.},
  doi       = {10.1145/1889863.1889911},
  isbn      = {9781450304412},
  keywords  = {Augmented reality,Depth cues,Depth perception,Evaluation,Handheld,Mixed reality,Photorealistic visualization},
}

@InProceedings{Krichenbauer2017a,
  author    = {Krichenbauer, Max and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu and Feiner, Steven},
  title     = {{Evaluating Positional Head-Tracking in Immersive VR for 3D Designers}},
  booktitle = {Adjunct Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2016},
  year      = {2017},
  abstract  = {With the ongoing introduction of wide-FOV VR head-worn displays into the consumer market, the application of VR 3D UIs to professional work environments is attracting increasing attention. One of the most conspicuous concepts is immersive 3D modeling and content creation. In spite of the long research history, there have been very few analyses of the effect of 3D UIs on productivity in 3D design. In this work, we explore the effect of positional head-tracking on task performance in 3D design. Previous studies have come to different conclusions on the importance of headtracking and did not investigate professional 3D modeling tools. In contrast, we performed a user study with design students using professional software on a task that closely emulates their work. Surprisingly, we did not find a significant effect of head-tracking on task-completion time, neither when using a traditional 2D mouse nor when using a pinch glove as a 3D input device.},
  doi       = {10.1109/ISMAR-Adjunct.2016.0074},
  isbn      = {9781509037407},
  keywords  = {Immersive Authoring,Virtual Reality},
}

@Article{Krichenbauer2017,
  author   = {Krichenbauer, Max and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu and Feiner, Steven},
  title    = {{Evaluating the effect of positional head-tracking on task performance in 3D modeling user interfaces}},
  journal  = {Computers and Graphics (Pergamon)},
  year     = {2017},
  issn     = {00978493},
  abstract = {With the ongoing introduction of wide-FOV VR head-worn displays into the consumer market, the application of VR 3D UIs to professional work environments is attracting increasing attention. One of the most conspicuous concepts is immersive 3D modeling and content creation. In spite of the long research history and multitude of proposed systems, there have been very few analyses of the effect of 3D UIs on productivity in 3D design. In this work, we explore the effect of positional head-tracking and its accompanying parallax depth cues on task performance in 3D object selection and transformation in a setting modeled after 3D design work. Previous studies have come to different conclusions on the importance of positional head-tracking and did not investigate professional 3D modeling tools. In contrast, we performed a user study with design students using professional software on a task that closely emulates their work. Surprisingly, we did not find a significant effect of positional head-tracking on task-completion time, neither when using a traditional 2D mouse nor when using a pinch glove as a 3D input device. Furthermore, we found that the users worked significantly faster with the mouse. We discuss possible explanations and implications for the design of 3D UIs.},
  doi      = {10.1016/j.cag.2017.04.002},
  keywords = {3D user interfaces,Augmented reality,Computer aided design},
}

@Article{Kaplan2018a,
  author   = {Kaplan, Oral and Yamamoto, Goshiro and Taketomi, Takafumi and Plopski, Alexander and Sandor, Christian and Kato, Hirokazu},
  title    = {{Exergame experience of young and old individuals under different difficulty adjustment methods}},
  journal  = {Computers},
  year     = {2018},
  issn     = {2073431X},
  abstract = {In this work, we compare the exergaming experience of young and old individuals under four difficulty adjustment methods. Physical inactivity is a leading cause of numerous health conditions including heart diseases, diabetes, cancer, and reduced life expectancy. Committing to regular physical exercise is a simple non-pharmaceutical preventive measure for maintaining good health and sustaining quality of life. Incorporating exercise into games, studies frequently used exergames as an intervention tool over the last decades to improve physical functions and to increase adherence to exercise. While task difficulty optimization is crucial to exergame design, researchers consistently overlooked age as an element which can significantly influence the nature of end results. We use the Flow State Scale to analyze the mental state of young and old individuals to compare constant difficulty with ramping, performance-based, and biofeedback-based difficulty adjustments. Our results indicate that old individuals are less likely to experience flow compared to young under the same difficulty adjustment methods. Further investigation revealed that old individuals are likely to experience flow under ramping and biofeedback-based difficulty adjustments whereas performance-based adjustments were only feasible for young.},
  doi      = {10.3390/computers7040059},
  keywords = {Difficulty adjustments,Exergames,Human-computer interaction,User experiences},
}

@InProceedings{Tonnis2005,
  author    = {T{\"{o}}nnis, Marcus and Sandor, Christian and Klinker, Gudrun and Lange, Christian and Bubb, Heiner},
  title     = {{Experimental evaluation of an augmented reality visualization for directing a car driver's attention}},
  booktitle = {Proceedings - Fourth IEEE and ACM International Symposium on Symposium on Mixed and Augmented Reality, ISMAR 2005},
  year      = {2005},
  abstract  = {With recent advances of Head-up Display technology in cars, Augmented Reality becomes interesting in supporting the driving task to guide a driver's attention. We have set up an experiment to compare two different approaches to inform the driver about dangerous situations around the car. One approach used AR to visualize the source of danger in the driver's frame of reference while the other one presented information in an exocentric frame of reference. Both approaches were evaluated in user tests. {\textcopyright} 2005 IEEE.},
  doi       = {10.1109/ISMAR.2005.31},
  isbn      = {0769524591},
}

@Article{Santos2016a,
  author   = {Santos, Marc Ericson C. and {de Souza Almeida}, Igor and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title    = {{Exploring legibility of augmented reality X-ray}},
  journal  = {Multimedia Tools and Applications},
  year     = {2016},
  issn     = {15737721},
  abstract = {Virtual objects can be visualized inside real objects using augmented reality (AR). This visualization is called AR X-ray because it gives the impression of seeing through the real object. In standard AR, virtual information is overlaid on top of the real world. To position a virtual object inside an object, AR X-ray requires partially occluding the virtual object with visually important regions of the real object. In effect, the virtual object becomes less legible compared to when it is completely unoccluded. Legibility is an important consideration for various applications of AR X-ray. In this research, we explored legibility in two implementations of AR X-ray, namely, edge-based and saliency-based. In our first experiment, we explored on the tolerable amounts of occlusion to comfortably distinguish small virtual objects. In our second experiment, we compared edge-based and saliency-based AR X-ray methods when visualizing virtual objects inside various real objects. Moreover, we benchmarked the legibility of these two methods against alpha blending. From our experiments, we observed that users have varied preferences for proper amounts of occlusion cues for both methods. The partial occlusions generated by the edge-based and saliency-based methods need to be adjusted depending on the lighting condition and the texture complexity of the occluding object. In most cases, users identify objects faster with saliency-based AR X-ray than with edge-based AR X-ray. Insights from this research can be directly applied to the development of AR X-ray applications.},
  doi      = {10.1007/s11042-015-2954-1},
  keywords = {Augmented reality,Augmented reality X-ray,Empirical study,Legibility,Visualization},
}

@InProceedings{Eck2016,
  author    = {Eck, Ulrich and Hoang, Liem and Sandor, Christian and Yamamoto, Goshiro and Taketomi, Takafumi and Kato, Hirokazu and Laga, Hamid},
  title     = {{Exploring the perception of co-location errors during tool interaction in visuo-haptic augmented reality}},
  booktitle = {Proceedings - IEEE Virtual Reality},
  year      = {2016},
  abstract  = {Co-located haptic feedback in mixed and augmented reality environments can improve realism and user performance, but it also requires careful system design and calibration. In this poster, we determine the thresholds for perceiving co-location errors through two psychophysics experiments in a typical fine-motor manipulation task. In these experiments we simulate the two fundamental ways of implementing VHAR systems: first, attaching a real tool; second, augmenting a virtual tool. We determined the just-noticeable co-location errors for position and orientation in both experiments and found that users are significantly more sensitive to co-location errors with virtual tools. Our overall findings are useful for designing visuo-haptic augmented reality workspaces and calibration procedures.},
  doi       = {10.1109/VR.2016.7504708},
  isbn      = {9781509008360},
  keywords  = {H.5.1 [Information Interfaces And Presentation]: Multimedia Information Systems - Artificial,augmented and virtual realities},
}

@InProceedings{Eck2016a,
  author    = {Eck, Ulrich and Stefan, Philipp and Laga, Hamid and Sandor, Christian and Fallavollita, Pascal and Navab, Nassir},
  title     = {{Exploring Visuo-Haptic augmented reality user interfaces for Stereo-tactic neurosurgery planning}},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year      = {2016},
  abstract  = {Stereo-tactic neurosurgery planning is a time-consuming and complex task that requires detailed understanding of the patient anatomy and the affected regions in the brain to precisely deliver the treatment and to avoid proximity to any known risk structures. Traditional user interfaces for neurosurgery planning use keyboard and mouse for interaction and visualize the medical data on a screen. Previous research, however, has shown that 3D user interfaces are more intuitive for navigating volumetric data and enable users to understand spatial relations more quickly. Furthermore, new imaging modalities and automated segmentation of relevant structures provide important information to medical experts. However, displaying such information requires frequent context switches or occludes otherwise important information. In collaboration with medical experts, we analyzed the planning workflow for stereo-tactic neurosurgery interventions and identified two tasks in the process that can be improved: volume exploration and trajectory refinement. In this paper, we present a novel 3D user interface for neurosurgery planning that is implemented using a head-mounted display and a haptic device. The proposed system improves volume exploration with bi-manual interaction to control oblique slicing of volumetric data and reduces visual clutter with the help of haptic guides that enable users to precisely target regions of interest and to avoid proximity to known risk structures.},
  doi       = {10.1007/978-3-319-43775-0_19},
  isbn      = {9783319437743},
  issn      = {16113349},
  keywords  = {Augmented reality,Haptics,Neurosurgery,Planning},
}

@TechReport{Sandor2007a,
  author    = {Sandor, Christian and Kuroki, Tsuyoshi and Uchiyama, Shinji and Yamamoto, Hiroyuki},
  title     = {{Exploring Visuo-Haptic Mixed Reality}},
  year      = {2007},
  abstract  = {In recent years, systems that allow users to see and touch virtual objects in the same space are being investigated. We refer to these systems as visuo-haptic mixed reality (VHMR) systems. Most research projects are employing a half-mirror, while few use a video see- through, head-mounted display (HMD). We have developed an HMD-based, VHMR painting application, which introduces new interaction techniques that could not be implemented with a half-mirror display. We present a user study to discuss its benefits and limitations. While we could not solve all technical problems, our work can serve as an important fundament for future research.},
  booktitle = {Technical Report PRMU2006-199 IEICE},
  issn      = {0913-5685},
  keywords  = {augmented reality,color selection,haptics,interaction techniques,mixed reality,user interfaces},
}

@Article{Mano2006,
  author   = {Mano, Max Senna and Guimar{\~{a}}es, Jos{\'{e}} Luiz Miranda and {Chicata Sutm{\"{o}}ller}, S{\"{o}}ren Franz Marian and Reiriz, Andr{\'{e}} Borba and {Chicata Sutm{\"{o}}ller}, Christian Sandor Svend and {Di Leo}, Angelo},
  title    = {{Extensive deep vein thrombosis as a complication of testicular cancer treated with the BEP protocol (bleomycin, etoposide and cisplatin): Case report}},
  journal  = {Sao Paulo Medical Journal},
  year     = {2006},
  issn     = {15163180},
  abstract = {Context: There are no reports in the literature of massive deep venous thrombosis (DVT) associated with cisplatin, bleomycin and etoposide (BEP) cancer treatment. Case Report: The patient was a 18-year-old adolescent with a nonseminomalous germ cell tumor of the right testicle, with the presence of pulmonary, liver, and massive retroperitoneal metastoses. Following radical orchiectomy, the patient started chemotherapy according to the BEP protocol (without routine prophylaxis for DVT). On day 4 of the first cycle, massive DVT was diagnosed, extending from both poplifeal veins up to the thoracic segment of the inferior vena cava. Thombolytic therapy with streptokinase was immediately diately started. On day 2 of the thrombolytic therapy, the patient developed acute renal failure, due to extension of the Thrombosis to the renal veins. Streptokinase was continued for six days and the outcome was remarkably favorable. Copyright {\textcopyright} 2006, Associa{\c{c}}{\~{a}}o Paulista de Medicina.},
  doi      = {10.1590/S1516-31802006000600009},
  keywords = {Chemotherapy,Pulmonary embolism,Testicular neoplasms,Thrombolytic agents,Venous thrombosis},
}

@InProceedings{Rovira2017,
  author    = {Rovira, Aitor and Taketomi, Takafumi and Constantine, Rompapas Damien and Kato, Hirokazu and Sandor, Christian and Ikeda, Sei},
  title     = {{EyeAR: Empiric Evaluation of a Refocusable Augmented Reality System}},
  booktitle = {Adjunct Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2016},
  year      = {2017},
  abstract  = {We present the evaluation of EyeAR, a display with refocusable content based on user's eyes measurements. We carried out a user study to validate the prototype to verify that participants cannot distinguish between real and virtual objects. Participants looked at three pillars (one of which was virtual) placed at different distances from the user. They had to guess which pillar was the virtual one while freely refocusing. The results partially confirmed that our prototype creates virtual objects that are indistinguishable from real objects.},
  doi       = {10.1109/ISMAR-Adjunct.2016.0027},
  isbn      = {9781509037407},
  keywords  = {H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial,and virtual realities; I.3.3 [Computing Methodologies]: Picture/Image Generation - Display Methods; H.1.2 [Models and Principles]: User/Machine,augmented},
}

@InProceedings{Rompapas2017,
  author    = {Rompapas, Damien Constantine and Rovira, Aitor and Ikeda, Sei and Plopski, Alexander and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title     = {{EyeAR: Refocusable Augmented Reality Content through Eye Measurements}},
  booktitle = {Adjunct Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2016},
  year      = {2017},
  abstract  = {The human visual system always focuses at a distinct depth. Therefore, objects that lie at different depths appear blurred, a phenomenon known as Depth of Field (DoF); as the user's focus depth changes, different objects come in and out of focus. Augmented Reality (AR) is a technology that superimposes computer graphics (CG) images onto a user's view of the real world. A commonly used AR display device is an Optical See-Through Head-Mounted Display (OST-HMD), enabling users to observe the real-world directly, with CG added to it. A common problem in such systems is the mismatch between the DoF properties of the user's eyes and the virtual camera used to generate CG.In this demonstration, we present an improved version of the system presented in [11] as two implementations: The first as a high quality tabletop system, the second as a component which has been integrated into the Microsoft Hololens [18].},
  doi       = {10.1109/ISMAR-Adjunct.2016.0108},
  isbn      = {9781509037407},
  issn      = {2414-4088},
  keywords  = {Augmented Reality,Depth of Field,Optical Defocus,Physically Based AR,Raytracing},
}

@InProceedings{Nakamura2018,
  author    = {Nakamura, Hiraku and Sonobe, Tatsuya and Toda, Hiroyuki and Fujisawa, Naomi and Taketomi, Takafumi and Plopski, Alexander and Sandor, Christian and Kato, Hirokazu},
  title     = {{Fusion of VSLAM/GNSS/INS for augmented reality navigation in ports}},
  booktitle = {Proceedings of the 31st International Technical Meeting of the Satellite Division of the Institute of Navigation, ION GNSS+ 2018},
  year      = {2018},
  abstract  = {Augmented Reality (AR) assisted navigation of marine vessels in port could lead to a reduction in accidents caused by human errors. To realize AR based ship navigation, it is necessary to accurately set the 3-dimensional spatial coordinates of the surroundings, to estimate the ship pose with high accuracy, and to present intuitive information based on them. In this paper, we propose an integration method of tracking information from Visual Simultaneous Localization and Mapping (VSLAM) / Global Navigation Satellite System (GNSS) / Inertial Navigation System using an Extended Kalman Filter. Our system does not require a network access, expensive Real-Time Kinematic or Precise Point Positioning. Our system is cost-effective as it utilizes only a monocular camera, plural single frequency GNSS cores, and a 6 Degree of Freedom Micro Electro Mechanical Systems Inertial Measurement Unit. To evaluate our system, we track the pose of a boat performing a berthing maneuver with our system using Accumulated Delta Range of GNSS carrier phase and with GNSS code-based positioning using Pseudo Range. Our results show that our system not only improves the scale estimation of the VSLAM system, but also the position accuracy of the ship from 0.66 m to 0.06 m on navigation coordinate. Finally, we present an intuitive AR interface that utilizes this information to support ship navigation.},
  doi       = {10.33012/2018.15942},
  isbn      = {0936406100},
}

@InProceedings{Csongei2014,
  author    = {Csongei, Michael and Hoang, Liem and Sandor, Christian and Lee, Yong Beom},
  title     = {{Global illumination for Augmented Reality on mobile phones}},
  booktitle = {Proceedings - IEEE Virtual Reality},
  year      = {2014},
  abstract  = {The goal of our work is to create highly realistic graphics for Augmented Reality on mobile phones. One of the greatest challenges for this is to provide realistic lighting of the virtual objects that matches the real world lighting. This becomes even more difficult with the limited capabilities of mobile phone GPUs. Our approach differs in the following important aspects compared to previous attempts: (1) most have relied on rasterizer approaches while our approach is based on raytracing; (2) we perform distributed rendering in order to address the limited mobile GPU capabilities; (3) we use image-based lighting from a pre-captured panorama to incorporate real world lighting. We utilize two markers: one for object tracking and one for registering the panorama. Our initial results are encouraging as the visual quality resembles real objects and also the reference renderings which were created offline. However we still need to validate our approach in human subject studies especially with regards to the trade-off between latency of remote rendering and visual quality. {\textcopyright} 2014 IEEE.},
  doi       = {10.1109/VR.2014.6802055},
  isbn      = {9781479928712},
  keywords  = {H.1.2. [Information Systems]: Models and Principles - [Human factors],H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems - [Artificial augmented and virtual realities],I.3.2. [Computer Graphics]: Graphics Systems - [Distributed/Networked Graphics]},
}

@InCollection{Asghar2015,
  author    = {Asghar, Zeeshan and Yamamoto, Goshiro and Uranishi, Yuki and Sandor, Christian and Kuroda, Tomohiro and Pulli, Petri and Kato, Hirokazu},
  title     = {{Grid-pattern indicating interface for ambient assisted living}},
  booktitle = {Recent Advances on Using Virtual Reality Technologies for Rehabilitation},
  year      = {2015},
  isbn      = {9781634840286},
  abstract  = {This chapter proposes a grid-pattern indicating interface to provide instructions remotely for supporting the daily activities of elderly people living in their homes independently. Our aim is to realize smooth and easy telecommunication between supported senior citizens at their local site and supporting caregivers who are at a remote site. Although we have used a monitoring method with video streaming where the remote caregivers indicate work steps in a conventional way, occlusion and depth perception problems were occurring. Our proposed method can provide a grid-pattern interface to remote caregivers, which could be a solution for the occlusion and depth perception problems by indicating the spatial instruction easily on a 2D input interface. In this study, a prototype has been implemented with a colour camera, a range image sensor, and projector. It also demonstrated that the proposed grid-pattern indicating interface can help caregivers show an instruction easily and intuitively.},
}

@InProceedings{Julier2015,
  author    = {Julier, Simon and Lindeman, Rob and Sandor, Christian},
  title     = {{Guest Editor's Introduction to the Special Section on the IEEE International Symposium on Mixed and Augmented Reality 2014}},
  booktitle = {IEEE Transactions on Visualization and Computer Graphics},
  year      = {2015},
  abstract  = {The IEEE International Symposium on Mixed and Augmented Reality (ISMAR) is the leading venue for publishing the latest Mixed and Augmented Reality research, applications, and technologies. This special section presents significantly extended versions of the five best papers from the IEEE ISMAR 2014 proceedings. Within the past few years, Augmented Reality (AR) has reached a critical mass in both research and commercial applications. It is now becoming truly feasible to use augmented reality to place graphics anywhere at any time. However, although the basic capabilities exist, many open research problems continue. This collection of papers considers underlying issues and technologies. IEEE ISMAR 2014 had 89 paper submissions; each paper was reviewed by at least four experts in the field. An international programcommittee of 15 ARexperts invited reviewers, led discussions, invited a rebuttal by the paper authors and prepared a consensus review. To select the final papers for publication, an online two-day PC meeting was held connecting three continents, where each paper was discussed. Thirty-five papers were accepted either as long or short publications, giving an overall acceptance rate of 40{\%}. An independent Award Committee reviewed the highest- ranked submissions again to determine the awards for Best Paper and Honorable Mention. For this special section, the authors of the award papers were invited to submit an extended version of their conference papers, with a clear focus on additional content that expands the scientific contribution of the original conference paper. A standard TVCG reviewing cycle was initiated in which all papers were reviewed, feedback was provided, and papers were revised to suit. Out of all submitted papers, less than 6{\%} appear in this TVCG Special Section.},
  doi       = {10.1109/TVCG.2015.2483298},
  issn      = {10772626},
  keywords  = {Augmented reality,Meetings,Special issues and sections,Virtual reality},
}

@Article{Polvi2018,
  author   = {Polvi, Jarkko and Taketomi, Takafumi and Moteki, Atsunori and Yoshitake, Toshiyuki and Fukuoka, Toshiyuki and Yamamoto, Goshiro and Sandor, Christian and Kato, Hirokazu},
  title    = {{Handheld Guides in Inspection Tasks: Augmented Reality versus Picture}},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2018},
  issn     = {10772626},
  abstract = {Inspection tasks focus on observation of the environment and are required in many industrial domains. Inspectors usually execute these tasks by using a guide such as a paper manual, and directly observing the environment. The effort required to match the information in a guide with the information in an environment and the constant gaze shifts required between the two can severely lower the work efficiency of inspector in performing his/her tasks. Augmented reality (AR) allows the information in a guide to be overlaid directly on an environment. This can decrease the amount of effort required for information matching, thus increasing work efficiency. AR guides on head-mounted displays (HMDs) have been shown to increase efficiency. Handheld AR (HAR) is not as efficient as HMD-AR in terms of manipulability, but is more practical and features better information input and sharing capabilities. In this study, we compared two handheld guides: an AR interface that shows 3D registered annotations, that is, annotations having a fixed 3D position in the AR environment, and a non-AR picture interface that displays non-registered annotations on static images. We focused on inspection tasks that involve high information density and require the user to move, as well as to perform several viewpoint alignments. The results of our comparative evaluation showed that use of the AR interface resulted in lower task completion times, fewer errors, fewer gaze shifts, and a lower subjective workload. We are the first to present findings of a comparative study of an HAR and a picture interface when used in tasks that require the user to move and execute viewpoint alignments, focusing only on direct observation. Our findings can be useful for AR practitioners and psychology researchers.},
  doi      = {10.1109/TVCG.2017.2709746},
  keywords = {Handheld devices,augmented reality,inspection task,user evaluation},
}

@InProceedings{Eck2013,
  author    = {Eck, Ulrich and Sandor, Christian},
  title     = {{HARP: A framework for visuo-haptic augmented reality}},
  booktitle = {Proceedings - IEEE Virtual Reality},
  year      = {2013},
  abstract  = {Visuo-Haptic Augmented Reality (VHAR) is a technology, which enables users to see and touch virtual objects. It poses unique problems to developers, including the need for precise augmentations, accurate colocation of haptic devices, and efficient processing of multiple, realtime sensor inputs to achieve low latency. We have designed and implemented the software framework HARP, which addresses these issues and simplifies the creation of haptic-enabled Augmented Reality (AR) applications. It allows developers to compose their applications on a high level of abstraction and hides most of the complexity of VHAR. Our contribution is the initial design and implementation of the framework, which we have validated in nine VHAR research projects ranging from simple interaction design to psychophysical experiments. We discuss limitations of our approach and future developments. These insights will be helpful to researchers and framework designers in the field of VHAR. {\textcopyright} 2013 IEEE.},
  doi       = {10.1109/VR.2013.6549404},
  isbn      = {9781467347952},
  keywords  = {D.2.1O [Design]: Rapid Prototyping,D.3.2 [Language Classifications]: Python, C, C++,D.3.3 [Language Constructs and Features]: Frameworks,H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems-[Artificial, augmented and virtual realities],H.5.2. [Information Interfaces and Presentation]: User Interfaces-[Haptic I/O]},
}

@InProceedings{Sherstyuk2012,
  author    = {Sherstyuk, Andrei and Dey, Arindam and Sandor, Christian},
  title     = {{Head-turning approach to eye-tracking in immersive virtual environments}},
  booktitle = {Proceedings - IEEE Virtual Reality},
  year      = {2012},
  abstract  = {Reliable and unobtrusive eye tracking remains a technical challenge for immersive virtual environment, especially when Head Mounted Displays (HMD) are used for visualization and users are allowed to move freely in the environment. In this work, we provide experimental evidence that gaze direction can be safely approximated by user head rotation, in HMD-based Virtual Reality (VR) applications, where users actively interact with the environment. We discuss the application range of our approach and consider possible extensions. {\textcopyright} 2012 IEEE.},
  doi       = {10.1109/VR.2012.6180919},
  isbn      = {9781467312462},
  keywords  = {I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction techniques,I.3.7 [Computer Graphics]: Three-dimensional Graphics and Realism - Virtual Reality},
}

@InProceedings{Macwilliams2003,
  author    = {Macwilliams, Asa and Sandor, Christian and Wagner, Martin and Bauer, Martin and Klinker, Gudrun and Bruegge, Bernd},
  title     = {{Herding sheep: Live system for distributed augmented reality}},
  booktitle = {Proceedings - 2nd IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR 2003},
  year      = {2003},
  abstract  = {In the past, architectures of augmented reality systems have been widely different and tailored to specific tasks. In this paper, we use the example of the SHEEP game to show how the structural flexibility of DWARF, our component-based distributed wearable augmented reality framework, facilitates a rapid prototyping and online development process for building, debugging and altering a complex, distributed, highly interactive AR system. The SHEEP system was designed to test and demonstrate the potential of tangible user interfaces which dynamically visualize, manipulate and control complex operations of many inter-dependent processes. SHEEP allows the users more freedom of action and forms of interaction and collaboration, following the tool metaphor that bundles software with hardware in units that are easily understandable to the user. We describe how we developed SHEEP, showing the combined evolution of framework and application, as well as the progress from rapid prototype to final demonstration system. The dynamic aspects of DWARF facilitated testing and allowed us to rapidly evaluate new technologies. SHEEP has been shown successfully at various occasions. We describe our experiences with these demos.},
  doi       = {10.1109/ISMAR.2003.1240695},
  isbn      = {0769520065},
}

@InProceedings{Hompapas2019,
  author    = {Hompapas, Damien and Sandor, Christian and Plopski, Alexander and Saakes, Daniel and Yun, Dong Hyeok and Taketomi, Takafumi and Kato, Hirokazu},
  title     = {{HoloRoyale: A Large Scale High Fidelity Augmented Reality Game}},
  booktitle = {Adjunct Proceedings - 2018 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2018},
  year      = {2019},
  abstract  = {Recent years saw an explosion in Augmented Reality (AR) experiences for consumers. These experiences can be classified based on the scale of the interactive area (room vs city/global scale), or the fidelity of the experience (high vs low) [4]. Experiences that target large areas, such as campus or world scale [6], [7], commonly have only rudimentary interactions with the physical world, and suffer from registration errors and jitter. We classify these experiences as large scale and low fidelity. On the other hand, various room sized experiences [5], [8] feature realistic interaction of virtual content with the real world. We classify these experiences as small scale and high fidelity.},
  doi       = {10.1109/ISMAR-Adjunct.2018.00120},
  isbn      = {9781538675922},
  keywords  = {Human-centered Computing,Mixed/Augmented Reality},
}

@InProceedings{Sandor2005a,
  author    = {Sandor, Christian and Olwal, Alex and Bell, Blaine and Feiner, Steven},
  title     = {{Immersive mixed-reality configuration of hybrid user interfaces}},
  booktitle = {Proceedings - Fourth IEEE and ACM International Symposium on Symposium on Mixed and Augmented Reality, ISMAR 2005},
  year      = {2005},
  abstract  = {Information in hybrid user interfaces can be spread over a variety of different, but complementary, displays, with which users interact through a potentially equally varied range of interaction devices. Since the exact configuration of these displays and devices may not be known in advance, it is desirable for users to he able to reconfigure at runtime the data flow between interaction devices and objects on the displays. To make this possible, we present the design and implementation of a prototype mixed reality system that allows users to immersively reconfigure a running hybrid user interface. {\textcopyright} 2005 IEEE.},
  doi       = {10.1109/ISMAR.2005.37},
  isbn      = {0769524591},
}

@Article{Yamamoto2017,
  author   = {Yamamoto, Goshiro and Sampaio, Luiz and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu and Kuroda, Tomohiro},
  title    = {{Imperceptible on-screen markers for mobile interaction on public large displays}},
  journal  = {IEICE Transactions on Information and Systems},
  year     = {2017},
  issn     = {17451361},
  abstract = {We present a novel method to enable users to experience mobile interaction with digital content on external displays by embedding markers imperceptibly on the screen. Our method consists of two parts: marker embedding on external displays and marker detection. To embed markers, similar to previous work, we display complementary colors in alternating frames, which are selected by considering L∗a∗b color space in order to make the markers harder for humans to detect. Our marker detection process does not require mobile devices to be synchronized with the display, while certain constraints for the relation between camera and display update rate need to be fulfilled. In this paper, we have conducted three experiments. The results show 1) selecting complementary colors in the a∗b∗ color plane maximizes imperceptibility, 2) our method is extremely robust when used with static contents and can handle animated contents up to certain optical flow levels, and 3) our method was proved to work well in case of small movements, but large movements can lead to loss of tracking.},
  doi      = {10.1587/transinf.2016PCP0015},
  keywords = {Human Visual Perception,Imperceptible Marker,Unsynchronized Capture},
}

@InProceedings{Avery2009,
  author   = {Avery, Benjamin and Sandor, Christian and Thomas, Bruce H.},
  title    = {{Improving Spatial Perception for Augmented Reality X-Ray Vision}},
  year     = {2009},
  abstract = {Augmented reality x-ray vision allows users to see through walls and view real occluded objects and locations. We present an augmented reality x-ray vision system that employs multiple view modes to support new visualizations that provide depth cues and spatial awareness to users. The edge overlay visualization provides depth cues to make hidden objects appear to be behind walls, rather than floating in front of them. Utilizing this edge overlay, the tunnel cut-out visualization provides details about occluding layers between the user and remote location. Inherent limitations of these visualizations are addressed by our addition of view modes allowing the user to obtain additional detail by zooming in, or an overview of the environment via an overhead exocentric view.},
  doi      = {10.1109/vr.2009.4811002},
}

@InProceedings{Kaplan2017,
  author    = {Kaplan, Oral and Yamamoto, Goshiro and Yoshitake, Yasuhide and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title     = {{In-situ visualization of pedaling forces on cycling training videos}},
  booktitle = {2016 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2016 - Conference Proceedings},
  year      = {2017},
  abstract  = {Over the last decades, visual representations of data has been a commonly used medium to bolster human cognition in performance evaluation of professional athletes. However, the current approaches to these visualizations still build upon the paper based principles of initial designs with solid backgrounds. Due to this situation, same visualizations usually fail to provide explicit information about the physical characteristics of the scenario that the data was captured, such as the form of athletes. In this work, we present a data visualization method which combines visual representations of cyclist's pedaling with correlated frames of indoor training videos. We designed a prototype system which allows us to superimpose various pedaling visualizations onto simultaneously captured training videos of cyclists (Figure 1). The results of user studies we conducted with twelve professional cyclists confirmed their interest in new possibilities emerging from intuitive data visualizations. We also received valuable feedback about the feasible benefits of our approach over traditional approaches, such as reduced cognitive overload in understanding visualizations. We conclude by discussing the future implementations and application areas of our approach and further need of adjusting it to distinct training scenarios.},
  doi       = {10.1109/SMC.2016.7844371},
  isbn      = {9781509018970},
}

@InProceedings{Etal.BauerM.HilingesO.MacWilliamsA.2003,
  author    = {{et al. Bauer, M., Hilinges, O., MacWilliams, A.} and Bauer, M and Hilinges, Otmar and MacWilliams, Asa and Sandor, Christian and Wagner, Martin and Klinker, Gudrun and Newman, Joseph and Reitmayr, Gerhard and Fahmy, Tamer and Pintaric, Thomas and Schmalstieg, Dieter},
  title     = {{Integrating Studierstube and DWARF}},
  booktitle = {Proceedings of the International Workshop on Software Technology for Augmented Reality Systems (STARS 2003)},
  year      = {2003},
  abstract  = {Studierstube and DWARF are modular Augmented Reality frameworks, each with distinct advantages in different application areas. Both can easily be extended by adding new components. In this case study we show how new components can act as a bridge between the frameworks. By facilitating the exchange of basic data types such as pose and user input, the frameworks become interoperable. This allows us to build new applications leveraging the advantages of both frameworks, while fostering cooperation between research groups.},
}

@InProceedings{Bauer2003,
  author    = {Bauer, M and Hilinges, Otmar and MacWilliams, Asa and Sandor, Christian and Wagner, Martin and Klinker, Gudrun and Newman, Joseph and Reitmayr, Gerhard and Fahmy, Tamer and Pintaric, Thomas and Schmalstieg, Dieter},
  title     = {{Integrating Studierstube and DWARF}},
  booktitle = {Proceedings of the International Workshop on Software Technology for Augmented Reality Systems (STARS 2003)},
  year      = {2003},
  abstract  = {Studierstube and DWARF are modular Augmented Reality frameworks, each with distinct advantages in different application areas. Both can easily be extended by adding new components. In this case study we show how new components can act as a bridge between the frameworks. By facilitating the exchange of basic data types such as pose and user input, the frameworks become interoperable. This allows us to build new applications leveraging the advantages of both frameworks, while fostering cooperation between research groups.},
}

@InCollection{Ty2019,
  author = {Ty, Jayzon and Inoue, Naoki and Plopski, Alexander and Okahashi, Sayaka and Sandor, Christian and Hsu, Hsiu-Yun and Kuo, Li-Chieh and Su, Fong-Chin and Kato, Hirokazu},
  title  = {{Integration of Augmented Reality with Pressing Evaluation and Training System for Finger Force Training}},
  year   = {2019},
  isbn   = {9783030220143},
  doi    = {10.1007/978-3-030-22015-0_45},
  issn   = {16113349},
}

@InProceedings{Hilliges2006,
  author    = {Hilliges, Otmar and Sandor, Christian and Klinker, Gudrun},
  title     = {{Interactive prototyping for ubiquitous augmented reality user interfaces}},
  booktitle = {International Conference on Intelligent User Interfaces, Proceedings IUI},
  year      = {2006},
  abstract  = {User interfaces for ubiquitous augmented reality incorporate a wide variety of concepts such as multi-modal, multi-user, multi-device aspects and new input/output devices. In this paper we present a twofold approach that consists of an execution engine for ubiquitous augmented reality user interfaces and a runtime development environment that enables rapid prototyping and live system adaption for such advanced user interfaces.},
  doi       = {10.1145/1111449.1111512},
  isbn      = {1595932879},
  keywords  = {Augmented reality,Multi-modal,Multi-user,Software architectures,Tangible user interfaces,Ubiquitous computing,Visual programming},
}

@Article{Sandor2015a,
  author  = {Sandor, Christian and Lindeman, Robert and Mayol-Cuevas, Walterio},
  title   = {{ISMAR 2015 PC Welcome Message}},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year    = {2015},
  issn    = {10772626},
  doi     = {10.1109/TVCG.2015.2472756},
}

@InProceedings{Sandor2016,
  author    = {Sandor, Christian},
  title     = {{Keynote at international display workshop 2016: Breaking the barriers to true augmented reality}},
  booktitle = {23rd International Display Workshops in conjunction with Asia Display, IDW/AD 2016},
  year      = {2016},
  abstract  = {In recent years, Augmented Reality (AR) and especially Virtual Reality have gained considerable commercial traction. I believe that soon, AR can become indistinguishable from reality; a concept I call True AR. This keynote addresses questions including: Why is True AR desirable? How can we test if something is True AR? How can we implement True AR?.},
  isbn      = {9781510845510},
  keywords  = {Augmented reality,Augmented reality x-ray,Haptics,Light field display,Turing test},
}

@InProceedings{Meng2013,
  author    = {Meng, Ma and Fallavollita, Pascal and Blum, Tobias and Eck, Ulrich and Sandor, Christian and Weidert, Simon and Waschke, Jens and Navab, Nassir},
  title     = {{Kinect for interactive AR anatomy learning}},
  booktitle = {2013 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2013},
  year      = {2013},
  abstract  = {Education of anatomy is a challenging but crucial element in educating medical professionals, but also for general education of pupils. Our research group has previously developed a prototype of an Augmented Reality (AR) magic mirror which allows intuitive visualization of realistic anatomical information on the user. However, the current overlay is imprecise as the magic mirror depends on the skeleton output from Kinect. These imprecisions affect the quality of education and learning. Hence, together with clinicians we have defined bone landmarks which users can touch easily on their body while standing in front of the sensor. We demonstrate that these landmarks allow the proper deformation of medical data within the magic mirror and onto the human body, resulting in a more precise augmentation. {\textcopyright} 2013 IEEE.},
  doi       = {10.1109/ISMAR.2013.6671803},
  isbn      = {9781479928699},
  keywords  = {Anatomy Learning,Augmented Reality,Kinect},
}

@InCollection{Sandor2006,
  author    = {Sandor, Christian and Klinker, Gudrun},
  title     = {{Lessons learned in designing ubiquitous augmented reality user interfaces}},
  booktitle = {Emerging Technologies of Augmented Reality: Interfaces and Design},
  year      = {2006},
  isbn      = {9781599040660},
  abstract  = {Ubiquitous augmented reality (UAR) is an emerging human-computer interaction technology, arising from the convergence of augmented reality and ubiquitous computing. In UAR, visualizations can augment the real world with digital information. Interactions can follow a tangible metaphor. Both should adapt according to the user's context and are distributed on a possibly changing set of devices. Current research problems for user interfaces in UAR are software infrastructures, authoring tools, and a supporting design process. We present case studies of how we have used a systematic design space analysis to carefully narrow the amount of available design options. The next step in our approach is to use interactive, possibly immersive tools to support interdisciplinary brainstorming sessions. Several tools are presented. We conclude by summarizing the lessons we have learned while applying our method. {\textcopyright} 2007, Idea Group Inc.},
  doi       = {10.4018/978-1-59904-066-0.ch011},
}

@Article{Dey2014,
  author   = {Dey, Arindam and Sandor, Christian},
  title    = {{Lessons learned: Evaluating visualizations for occluded objects in handheld augmented reality}},
  journal  = {International Journal of Human Computer Studies},
  year     = {2014},
  issn     = {10959300},
  abstract = {Handheld devices like smartphones and tablets have emerged as one of the most promising platforms for Augmented Reality (AR). The increased usage of these portable handheld devices has enabled handheld AR applications to reach the end-users; hence, it is timely and important to seriously consider the user experience of such applications. AR visualizations for occluded objects enable an observer to look through objects. AR visualizations have been predominantly evaluated using Head-Worn Displays (HWDs), handheld devices have rarely been used. However, unless we gain a better understanding of the perceptual and cognitive effects of handheld AR systems, effective interfaces for handheld devices cannot be designed. Similarly, human perception of AR systems in outdoor environments, which provide a higher degree of variation than indoor environments, has only been insufficiently explored. In this paper, we present insights acquired from five experiments we performed using handheld devices in outdoor locations. We provide design recommendations for handheld AR systems equipped with visualizations for occluded objects. Our key conclusions are the following: (1) Use of visualizations for occluded objects improves the depth perception of occluded objects akin to non-occluded objects. (2) To support different scenarios, handheld AR systems should provide multiple visualizations for occluded objects to complement each other. (3) Visual clutter in AR visualizations reduces the visibility of occluded objects and deteriorates depth judgment; depth judgment can be improved by providing clear visibility of the occluded objects. (4) Similar to virtual reality interfaces, both egocentric and exocentric distances are underestimated in handheld AR. (5) Depth perception will improve if handheld AR systems can dynamically adapt their geometric field of view (GFOV) to match the display field of view (DFOV). (6) Large handheld displays are hard to carry and use; however, they enable users to better grasp the depth of multiple graphical objects that are presented simultaneously. {\textcopyright} 2014 Elsevier Ltd.},
  doi      = {10.1016/j.ijhcs.2014.04.001},
  keywords = {Augmented reality,Experiments,Handheld devices,Outdoor environments,Visualizations for occluded objects},
}

@InProceedings{Sandor2009,
  author = {Sandor, Christian and Kitahara, Itaru and Reitmayr, Gerhard and Feiner, Steven and Ohta, Yuichi},
  title  = {{Lets go out: Research in outdoor mixed and augmented reality}},
  year   = {2009},
  doi    = {10.1109/ismar.2009.5336446},
}

@InProceedings{Akiyama2018,
  author    = {Akiyama, Ryo and Yamamoto, Goshiro and Amano, Toshiyuki and Taketomi, Takafumi and Plopski, Alexander and Sandor, Christian and Kato, Hirokazu},
  title     = {{Light Projection-Induced Illusion for Controlling Object Color}},
  booktitle = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
  year      = {2018},
  abstract  = {Using projection mapping, we can control the appearance of realworld objects by projecting colored light onto them. Because a projector can only add illumination to the scene, only a limited color gamut can be presented through projection mapping. In this paper we describe how the controllable color gamut can be extended by accounting for human perception and visual illusions. In particular, we induce color constancy to control what color space observers will perceive. In this paper, we explain the concept of our approach, and show first results of our system.},
  doi       = {10.1109/VR.2018.8446481},
  isbn      = {9781538633656},
  keywords  = {Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality,Computing methodologies-Computer graphics-Graphics systems and interfaces-Perception},
}

@Article{Julier2014,
  author   = {Julier, Simon and Lindeman, Robert W. and Sandor, Christian},
  title    = {{Message from the science {\&} technology paper chairs}},
  journal  = {ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings},
  year     = {2014},
  abstract = {Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
  doi      = {10.1109/ISMAR.2014.6948390},
  isbn     = {9781479961849},
}

@InProceedings{Lugtenberg2018,
  author    = {Lugtenberg, Geert and H{\"{u}}rst, Wolfgang and Rosa, Nina and Sandor, Christian and Plopski, Alexander and Taketomi, Takafumi and Kato, Hirokazu},
  title     = {{Multimodal Augmented Reality – Augmenting Auditory-Tactile Feedback to Change the Perception of Thickness}},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year      = {2018},
  abstract  = {With vision being a primary sense of humans, we often first estimate the physical properties of objects by looking at them. However, when in doubt, for example, about the material they are made of or its structure, it is natural to apply other senses, such as haptics by touching them. Aiming at the ultimate goal of achieving a full-sensory augmented reality experience, we present an initial study focusing on multimodal feedback when tapping an object to estimate the thickness of its material. Our results indicate that we can change the perception of thickness of stiff objects by modulating acoustic stimuli. For flexible objects, which have a more distinctive tactile characteristic, adding vibratory responses when tapping on thick objects can make people perceive them as thin. We also identified that in the latter case, adding congruent acoustic stimuli does not further enhance the illusion but worsens it.},
  doi       = {10.1007/978-3-319-73603-7_30},
  isbn      = {9783319736020},
  issn      = {16113349},
  keywords  = {Augmented reality,Multimodal AR,Multimodal perception},
}

@InProceedings{Jung2018a,
  author = {Jung, Sungchul and Bruder, Gerd and Wisniewski, Pamela J. and Sandor, Christian and Hughes, Charles E.},
  title  = {{Over My Hand}},
  year   = {2018},
  doi    = {10.1145/3267782.3267920},
}

@InProceedings{Jung2018,
  author    = {Jung, Sungchul and Bruder, Gerd and Wisniewski, Pamela J. and Sandor, Christian and Hughes, Charles E.},
  title     = {{Over my hand: Using a personalized hand in VR to improve object size estimation, body ownership, and presence}},
  booktitle = {SUI 2018 - Proceedings of the Symposium on Spatial User Interaction},
  year      = {2018},
  abstract  = {When estimating the distance or size of an object in the real world, we often use our own body as a metric; this strategy is called body-based scaling. However, object size estimation in a virtual environment presented via a head-mounted display differs from the physical world due to technical limitations such as narrow field of view and low fidelity of the virtual body when compared to one's real body. In this paper, we focus on increasing the fidelity of a participant's body representation in virtual environments with a personalized hand using personalized characteristics and a visually faithful augmented virtuality approach. To investigate the impact of the personalized hand, we compared it against a generic virtual hand and measured effects on virtual body ownership, spatial presence, and object size estimation. Specifically, we asked participants to perform a perceptual matching task that was based on scaling a virtual box on a table in front of them. Our results show that the personalized hand not only increased virtual body ownership and spatial presence, but also supported participants in correctly estimating the size of a virtual object in the proximity of their hand.},
  doi       = {10.1145/3267782.3267920},
  isbn      = {9781450357081},
  keywords  = {Personalized Virtual Body,Presence,Virtual Body Ownership Illusion,Virtual Object Size Manipulation,Virtual Reality},
}

@InProceedings{Akiyama2019,
  author = {Akiyama, Ryo and Yamamoto, Goshiro and Amano, Toshiyuki and Taketomi, Takafumi and Sandor, Christian and Plopski, Alexander and Kato, Hirokazu},
  title  = {{Perceptual Appearance Control by Projection-Induced Illusion}},
  year   = {2019},
  doi    = {10.1109/vr.2019.8798347},
  isbn   = {9781728113777},
}

@InProceedings{Marner2009,
  author    = {Marner, Michael R. and Thomas, Bruce H. and Sandor, Christian},
  title     = {{Physical-virtual tools for spatial augmented reality user interfaces}},
  booktitle = {Science and Technology Proceedings - IEEE 2009 International Symposium on Mixed and Augmented Reality, ISMAR 2009},
  year      = {2009},
  abstract  = {This paper presents a new user interface methodology for Spatial Augmented Reality systems. The methodology is based on a set of physical tools that are overloaded with logical functions. Visual feedback presents the logical mode of the tool to the user by projecting graphics onto the physical tools. This approach makes the tools malleable in their functionality, with this change conveyed to the user by changing the projected information. Our prototype application implements a two handed technique allowing an industrial designer to digitally airbrush onto an augmented physical model, masking the paint using a virtualized stencil. {\textcopyright}2009 IEEE.},
  doi       = {10.1109/ISMAR.2009.5336458},
  isbn      = {9781424453900},
  keywords  = {Spatial augmented reality,User interfaces},
}

@Article{Eck2015,
  author   = {Eck, Ulrich and Pankratz, Frieder and Sandor, Christian and Klinker, Gudrun and Laga, Hamid},
  title    = {{Precise Haptic Device Co-Location for Visuo-Haptic Augmented Reality}},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2015},
  issn     = {10772626},
  abstract = {Visuo-haptic augmented reality systems enable users to see and touch digital information that is embedded in the real world. PHANToM haptic devices are often employed to provide haptic feedback. Precise co-location of computer-generated graphics and the haptic stylus is necessary to provide a realistic user experience. Previous work has focused on calibration procedures that compensate the non-linear position error caused by inaccuracies in the joint angle sensors. In this article we present a more complete procedure that additionally compensates for errors in the gimbal sensors and improves position calibration. The proposed procedure further includes software-based temporal alignment of sensor data and a method for the estimation of a reference for position calibration, resulting in increased robustness against haptic device initialization and external tracker noise. We designed our procedure to require minimal user input to maximize usability. We conducted an extensive evaluation with two different PHANToMs, two different optical trackers, and a mechanical tracker. Compared to state-of-the-art calibration procedures, our approach significantly improves the co-location of the haptic stylus. This results in higher fidelity visual and haptic augmentations, which are crucial for fine-motor tasks in areas such as medical training simulators, assembly planning tools, or rapid prototyping applications.},
  doi      = {10.1109/TVCG.2015.2480087},
  keywords = {Calibration,Haptic interfaces,Phantoms,Sensors,Target tracking,Transforms,Virtual reality},
}

@InProceedings{Kaplan2018b,
  author    = {Kaplan, Oral and Yamamoto, Goshiro and Taketomi, Takafumi and Yoshitake, Yasuhide and Plopski, Alexander and Sandor, Christian and Kato, Hirokazu},
  title     = {{Promoting short-term gains in physical exercise through digital media creation}},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year      = {2018},
  abstract  = {Although regular physical exercise is associated with various health benefits, low rates of adherence remain as an intricate problem. In this paper, we discuss a new emotional facilitator named productivity for increasing adherence to regular physical exercise by promoting short-term gains. We introduce how it can be utilized in digital exertion games to encourage physical activity and maintain high intrinsic motivation. We define how it can be used to encourage regular practice of physical exercise and support users in maintaining high levels of intrinsic motivation. We believe inclusion of design ideas disclosed in this paper will lead to more engaging experiences with higher adherence to physical exercise.},
  doi       = {10.1007/978-3-319-76270-8_19},
  isbn      = {9783319762692},
  issn      = {16113349},
  keywords  = {Digital exergames,Emotional facilitators,Exercise motivation,Physical exercise adherence},
}

@Article{Nguyen2010,
  author   = {Nguyen, Thanh and Sandor, Christian and Park, Joseph J.},
  title    = {{PTAMM-Plus: Refactoring and Extending PTAMM}},
  journal  = {In Proceedings of International Conference on Artificial Reality and Telexistence},
  year     = {2010},
  abstract = {Augmented Reality (AR) is a promising candidate for the next gen- eration of human computer interaction. However, a great number of obstacles have to be overcome in order to makeARbecome widely- acceptable. Among those obstacles, tracking is one of the ma- jor well-known challenges. The state-of-the-art in AR proved that PTAMM is one of the most promising tracking systems. However, it is quite difficult to develop AR applications based on PTAMM system because of its monolithic architecture. Therefore, modu- larizing it for seamless integration is needed. On the other hand, PTAMM distorts graphic results instead of undistorting the video images, the result of which is a distorted augmented view. In this paper, we present PTAMM-Plus, which refactored and extended PTAMM by developing a well-formed API, re-arranging threads, and improving the undistortion mechanism. Using PTAMM-Plus, AR developers may easily build AR applications independently on top of PTAMM. By rearranging threads and undistortion mecha- nism, presented PTAMM-Plus also enhanced performance and ac- curacy. In this paper, a testing oriented approach in software devel- opment was applied when implementing the PTAMM-Plus.},
}

@InCollection{Livingston2013,
  author    = {Livingston, Mark A. and Dey, Arindam and Sandor, Christian and Thomas, Bruce H.},
  title     = {{Pursuit of “X-Ray Vision” for Augmented Reality}},
  booktitle = {Human Factors in Augmented Reality Environments},
  year      = {2013},
  abstract  = {The ability to visualize occluded objects or people offers tremendous potential to users of augmented reality (AR). This is especially true in mobile applications for urban environments, in which navigation and other operations are hindered by the urban infrastructure. This “X-ray vision” feature has intrigued and challenged AR system designers for many years, with only modest progress in demonstrating a useful and usable capability. The most obvious challenge is to the human visual system, which is being asked to interpret a very unnatural perceptual metaphor. We review the perceptual background to understand how the visual system infers depth and how these assumptions are or are not met by augmented reality displays. In response to these challenges, several visualization metaphors have been proposed; we survey these in light of the perceptual background. Because augmented reality systems are user-centered, it is important to evaluate how well these visualization metaphors enable users to perform tasks that benefit from X-ray vision. We summarize studies reported in the literature. Drawing upon these analyses, we offer suggestions for future research on this tantalizing capability.},
  doi       = {10.1007/978-1-4614-4205-9_4},
}

@Article{Jung2016,
  author   = {Jung, Sungchul and Sandor, Christian and Wisniewski, Pamela and Hughes, Charles E},
  title    = {{RealME: The influence of a personalized body representation on the illusion of virtual body ownership}},
  journal  = {International Conference on Artificial Reality and Telexistence Eurographics Symposium on Virtual Environments (ICAT/EGVE)},
  year     = {2016},
  abstract = {Figure 1: Overall experiment environment. In this experiment, we provided two type of body reflection through a virtual mirror while the partic-ipants looked at a virtual hand and arm from a first person perspective. (a) Visually personalized body reflection where the clothes and shape are identical to those of the participant seen in (c). (b) Generic Avatar body representation. (d) and (e) Two levels of hand representation – (d) Fully modeled limb from shoulder to hand. (e) Arm removed disconnected hand. ABSTRACT The study presented in this paper extends earlier research involving body continuity by investigating if the presence of real body cues (legs that look like and move like one's own) alters one's sense of immersion in a virtual environment. The main hypothesis is that real body cues increase one's sense of physical presence and body ownership, even when those body parts are not essential to the ac-tivity on which one is focused. To test this hypothesis, we devel-oped an experiment that uses a virtual human hand and arm that are directly observable but clearly synthetic, and a lower body seen through a virtual mirror, where the legs are sometimes visually ac-curate and personalized, and other times accurate in movement but not in appearance. Only the virtual right hand and arm play a part in our scenario, and so the lower body, despite sometimes appearing realistic, is largely irrelevant, except in its influence on perception. By looking at combinations of arm-hand continuity (2 conditions), freedom or lack of it to move the hand (2 conditions), and realism or lack of it of the virtually reflected lower body (2 conditions), we are able to study the effects of each combination on presence and body ownership, critical features in virtual environments involving a virtual surrogate.},
  keywords = {Artificial augmented—virtual realities,H51 [Information interfaces and presentation],Index Terms},
}

@InProceedings{Jung2017,
  author    = {Jung, Sungchul and Wisniewski, Pamela J. and Sandor, Christian and Hughes, Charles E.},
  title     = {{RealME: The influence of body and hand representations on body ownership and presence}},
  booktitle = {SUI 2017 - Proceedings of the 2017 Symposium on Spatial User Interaction},
  year      = {2017},
  abstract  = {The study presented in this paper extends earlier research involving body continuity by investigating if the presence of real body cues (legs that look like and move like one's own) alters one's sense of immersion in a virtual environment. e main hypothesis is that real body cues increase one's sense of body ownership and spatial presence, even when those body parts are not essential to the activity on which one is focused. To test this hypothesis, we developed an experiment that uses a virtual human hand and arm that are directly observable but clearly synthetic, and a lower body seen through a virtual mirror, where the legs are sometimes visually accurate and personalized, and other times accurate in movement but not in appearance. e virtual right hand and arm are the focus of our scenario; the lower body, only visible in the mirror, is largely irrelevant to the task, providing only perceptually contextual information. By looking at combinations of arm-hand continuity (2 conditions), freedom or lack of it to move the hand (2 conditions), and realism or lack of it of the virtually reected lower body (2 conditions), we are able to study the eects of each combination on the perceptions of body ownership and presence, critical features in virtual environments involving a virtual surrogate.},
  doi       = {10.1145/3131277.3132186},
  isbn      = {9781450354868},
  keywords  = {Body Continuity,Human Computer Interaction,Presence,User Study,Virtual Body Ownership,Virtual Reality},
}

@InProceedings{Asghar2017,
  author    = {Asghar, Zeeshan and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu and Pulli, Petri},
  title     = {{Remote assistance for elderly to find hidden objects in a kitchen}},
  booktitle = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
  year      = {2017},
  abstract  = {Remote assistive technologies are one of the most promising solutions for an aging society in the future. This paper describes a design of a remote assistive system to guide elderly to find and recognize hidden objects in a kitchen through ubiquitous technologies utilizing sensing and light projection. These intelligent technologies can play a vital role in taking care of the elderly with cognitive impairments when the caregiver's lives or work far away. The main goal of this research is to provide visual guidance to elderly to overcoming the deficits of initiation, planning, attention and memory deficits while performing kitchen-based activities of daily living such as locating and identifying items for cooking. In a standard kitchen where objects can be placed in open and closed spaces, it is difficult for elderly with cognitive impairment to find and locate objects that are invisible and sometimes hidden behind other objects. In this situation the RFID technology can directly provide the location of the items and projection technology can display the image of the object at the exact location. An initial prototype has been developed and a user study with twelve elderly people has been conducted. The initial results show that the visual guidance makes the task of finding and identifying objects easier and simpler. Additionally, results show promise for further development and system can be used for other kitchen activities.},
  doi       = {10.1007/978-3-319-49655-9_1},
  isbn      = {9783319496542},
  issn      = {18678211},
  keywords  = {Caregiver,Elderly,Projection,RFID,Remote assistance},
}

@Article{NAKAMURA2019,
  author  = {NAKAMURA, Hiraku and SONOBE, Tatsuya and TODA, Hiroyuki and FUJISAWA, Naomi and TAKETOMI, Takafumi and PLOPSKI, Alexander and SANDOR, Christian and KATO, Hirokazu},
  title   = {{Robust Positioning Method for Berthing Support Using Fusion of Tightly Coupled GNSS Carrier Phase with IMU and Visual Odometry着桟支援のためのGNSS搬送波位相とIMUの密結合と視覚オドメトリの融合による位置推定のロバスト化}},
  journal = {Journal of the Institute of Positioning, Navigation and Timing of Japan},
  year    = {2019},
  issn    = {2185-2952},
  doi     = {10.5266/ipntj.10.1},
}

@Article{Akiyama2019a,
  author  = {Akiyama, Ryo and Yamamoto, Goshiro and Amano, Toshiyuki and Taketomi, Takafumi and Plopski, Alexander and Sandor, Christian and Kato, Hirokazu},
  title   = {{Robust Reflectance Estimation for Projection-Based Appearance Control in a Dynamic Light Environment}},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year    = {2019},
  issn    = {1077-2626},
  doi     = {10.1109/tvcg.2019.2940453},
}

@InProceedings{Porter2009,
  author    = {Porter, Shane and Marner, Michael R. and Eck, Ulrich and Sandor, Christian and Thomas, Bruce H.},
  title     = {{Rundle lantern in miniature: Simulating large scale non-planar displays}},
  booktitle = {ACM International Conference Proceeding Series},
  year      = {2009},
  abstract  = {The Rundle Lantern is a central fixture of the Adelaide CBD; a full color display four stories high, spanning two sides of a building. This paper describes our approach to rapid prototyping interactive content for use on the Rundle Lantern. We have created a physical small scale model of the Lantern, and employed spatial augmented reality to project content onto the mockup at the same resolution of the Lantern, to produce a similar appearance as the real Lantern. We developed an application to automatically quantize the pixelation of images to match the Rundle Lantern's relatively low pixel resolution. This approach allows new content to easily be developed and prototyped on a non-planar display with the same spatial proportions as the real Lantern; albeit on a much smaller scale. The prototype system is combined with software for the Apple iPhone so that interactive content can be developed. We have used our system to develop new visual content and several multiplayer games suitable for use on the Rundle Lantern. {\textcopyright} ACM 2009.},
  doi       = {10.1145/1690388.1690445},
  isbn      = {9781605588643},
  keywords  = {Large display,Mixed reality,Prototyping,Rundle lantern,Simulation,Smartphone interaction,Spatial augmented reality},
}

@InProceedings{Oshima2016,
  author    = {Oshima, Kohei and Moser, Kenneth R. and Rompapas, Damien Constantine and Swan, J. Edward and Ikeda, Sei and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title     = {{SharpView: Improved clarity of defocussed content on optical see-through head-mounted displays}},
  booktitle = {Proceedings - IEEE Virtual Reality},
  year      = {2016},
  abstract  = {A common factor among current generation optical see-through augmented reality systems is fixed focal distance to virtual content. In this work, we investigate the issue of focus blur, in particular, the blurring caused by simultaneously viewing virtual content and physical objects in the environment at differing focal distances. We examine the application of dynamic sharpening filters as a straight forward, system independent, means for mitigating this effect improving the clarity of defocused AR content. We assess the utility of this method, termed SharpView, by employing an adjustment experiment in which users actively apply varying amounts of sharpening to reduce the perception of blur in AR content. Our experimental results validate the ability of our SharpView model to improve the visual clarity of focus blurred content, with optimal performance at focal differences well suited for near field AR applications.},
  doi       = {10.1109/VR.2016.7504749},
  isbn      = {9781509008360},
  keywords  = {H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial, augmented, and virtual realities,I.4.4 [Image Processing and Computer Vision]: Restoration - Wiener filtering},
}

@InProceedings{Oshima2015,
  author    = {Oshima, Kohei and Rompapas, Damien Constantine and Moser, Kenny and Swan, Ed and Ikeda, Sei and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title     = {{SharpView: Improved Legibility of Defocussed Content on Optical See-Through Head-Mounted Displays}},
  booktitle = {Demo under submission for: ACM International Symposium on Mixed and Augmented Reality},
  year      = {2015},
  abstract  = {Augmented Reality (AR) makes it possible to insert computer generated information and objects onto a user's view of the real-world. Optical See-Through (OST) Head-Mounted Displays (HMDs) are becoming an increasingly common medium for presenting AR content, with the added beneﬁt of enabling users to observe the real-world directly, through transparent display material, alongside computer graphics (CG) content. A problem in OST HMD systems, though, is the existence of focal rivalry resulting from the distance disparity between real world objects and the virtual screen of the HMD. Due to the focal limitations of the human eye, this focal discrepancy results in the CG image appearing blurred when real objects are ﬁxated upon. The goal of our SharpView system is to ameliorate this problem through sharpening. SharpView utilizes a variable sharpening ﬁlter, based on the eye's Point Spread Function (PSF), to process the CG imagery before it is displayed. We derive the PSF from three parameters in real-time: pupil size, ﬁxation distance, and the focal distance of the virtual display screen. Our demonstration allows users to experience the focal disparity problem in OST HMD hardware and view the improved legibility of CG content provided by the SharpView technique.},
  keywords  = {Augmented Reality, Optical-See-Through HeadMounted},
}

@InProceedings{Sandor2002,
  author    = {Sandor, Christian and Wagner, Martin and Macwilliams, Asa and Bauer, Martin and Klinker, Gudrun},
  title     = {{SHEEP: The Shared Environment Entertainment Pasture}},
  booktitle = {Demo at IEEE and ACM International Symposium on Mixed and Augmented Reality ISMAR 2002},
  year      = {2002},
  abstract  = {In this demo, we use a multiplayer shepherding game to explore the possibilities of mutimodal, multiuser interaction with wearable computing in an intelligent environment. The game is centered around a table with a beamerprojected pastoral landscape. Players can use different intuitive interaction technologies (beamer, screen, HMD, touchscreen, speech, gestures) offered by the mobile and stationary computers. Building on our DWARF framework, the system uses peer-to-peer, dynamically cooperating services to integrate different mobile devices (including spectators laptops) into the game.},
  isbn      = {4989490851},
  keywords  = {ad hoc services,augmented reality,environments,frameworks,intelligent,multimodal interaction},
}

@InProceedings{Ng2018,
  author    = {Ng, Gary and Shin, Joon Gi and Plopski, Alexander and Sandor, Christian and Saakes, Daniel},
  title     = {{Situated game level editing in augmented reality}},
  booktitle = {TEI 2018 - Proceedings of the 12th International Conference on Tangible, Embedded, and Embodied Interaction},
  year      = {2018},
  abstract  = {Level editors let end-users create custom levels and content within a given video game. In this paper, we explore the concept and design of Augmented reality game level editors. These new types of editors are not only spatial and embodied, but also situated, as they enable users to tailor games to the unique characteristics and emotional value of their own space. We present the design and implementation of a prototype level editor that runs on the Microsoft HoloLens. The editor enables users to add virtual content in their homes and add interactions through spatial trigger-action game-logic programming. We had pairs of students create games with the prototype and play each other's games. They reported that games are fun to make, play, and watch others play. Based on the design and evaluation, we propose guidelines for Augmented reality game-authoring tools for end users.},
  doi       = {10.1145/3173225.3173230},
  isbn      = {9781450355681},
  keywords  = {Augmented Reality,Game Design,Level Editor,Spatial Programming,Trigger-Action Programming},
}

@Article{Polvi2016,
  author   = {Polvi, Jarkko and Taketomi, Takafumi and Yamamoto, Goshiro and Dey, Arindam and Sandor, Christian and Kato, Hirokazu},
  title    = {{SlidAR: A 3D positioning method for SLAM-based handheld augmented reality}},
  journal  = {Computers and Graphics (Pergamon)},
  year     = {2016},
  issn     = {00978493},
  abstract = {Handheld Augmented Reality (HAR) has the potential to introduce Augmented Reality (AR) to large audiences due to the widespread use of suitable handheld devices. However, many of the current HAR systems are not considered very practical and they do not fully answer to the needs of the users. One of the challenging areas in HAR is the in-situ AR content creation where the correct and accurate positioning of virtual objects to the real world is fundamental. Due to the hardware limitations of handheld devices and possible restrictions in the environment, the correct 3D positioning of objects can be difficult to achieve we are unable to use AR markers or correctly map the 3D structure of the environment. We present SlidAR, a 3D positioning for Simultaneous Localization And Mapping (SLAM) based HAR systems. SlidAR utilizes 3D ray-casting and epipolar geometry for virtual object positioning. It does not require a perfect 3D reconstruction of the environment nor any virtual depth cues. We have conducted a user experiment to evaluate the efficiency of SlidAR method against an existing device-centric positioning method that we call HoldAR. Results showed that SlidAR was significantly faster, required significantly less device movement, and also got significantly better subjective evaluation from the test participants. SlidAR also had higher positioning accuracy, although not significantly.},
  doi      = {10.1016/j.cag.2015.10.013},
  keywords = {3D manipulation,3D positioning,Handheld augmented reality,SLAM,User evaluation},
}

@InProceedings{Cameron2011,
  author    = {Cameron, Brad and Sandor, Christian and Mickan, Peter},
  title     = {{Social semiotic analysis of the design space of augmented reality}},
  booktitle = {2011 IEEE International Symposium on Mixed and Augmented Reality - Arts, Media, and Humanities, ISMAR-AMH 2011},
  year      = {2011},
  abstract  = {Text is one of the multimodal components used in many augmented reality (AR) applications. Fulfilling the design goals of an application depends, to a large extent, on the successful design of these components. This paper examines the role of text in AR user interfaces from a social semiotic and linguistic perspective. Since analysis of the text in AR applications has mostly focused on aspects such as legibility, placement, occlusion-related issues and exploration of methods to overcome other visual challenges, we suggest that such a social semiotic and linguistic analysis of the textual content can provide insights into user interface and application design for AR researchers and developers. Our analysis suggests that the role of text in AR can differ, and depends, at least in part, on the type of text-image interaction of which it is a part. {\textcopyright} 2011 IEEE.},
  doi       = {10.1109/ISMAR-AMH.2011.6093666},
  isbn      = {9781467300575},
  keywords  = {Augmented Reality,Multimodality,Social Semiotics,User Interface Design},
}

@InProceedings{Sandor2019,
  author    = {Sandor, Christian and Nakamura, Hiraku},
  title     = {{SoliScratch: A Radar Interface for Scratch DJs}},
  booktitle = {Adjunct Proceedings - 2018 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2018},
  year      = {2019},
  abstract  = {Scratching is a DJ (Disk Jockey) technique for producing rhythmic sounds by moving a record back and forth. It requires a high level of manual dexterity and its difficulty has been likened to playing an instrument. Our goals are: first, ease of use, so that novice users can start scratching immediately; second, to enable users to scratch while walking. We employ millimeter wave radar sensing technology to replace record movements with mid-air gestures that are captured at 1 kHz. We developed our systems in several iterations with professional DJs. The accompanying movie shows that we could achieve our goals.},
  doi       = {10.1109/ISMAR-Adjunct.2018.00129},
  isbn      = {9781538675922},
}

@Article{Moser2015,
  author   = {Moser, Kenneth and Itoh, Yuta and Oshima, Kohei and Swan, J. Edward and Klinker, Gudrun and Sandor, Christian},
  title    = {{Subjective Evaluation of a Semi-Automatic Optical See-Through Head-Mounted Display Calibration Technique}},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2015},
  issn     = {10772626},
  abstract = {With the growing availability of optical see-through (OST) head-mounted displays (HMDs) there is a present need for robust, uncomplicated, and automatic calibration methods suited for non-expert users. This work presents the results of a user study which both objectively and subjectively examines registration accuracy produced by three OST HMD calibration methods: (1) SPAAM, (2) Degraded SPAAM, and (3) Recycled INDICA, a recently developed semi-automatic calibration method. Accuracy metrics used for evaluation include subject provided quality values and error between perceived and absolute registration coordinates. Our results show all three calibration methods produce very accurate registration in the horizontal direction but caused subjects to perceive the distance of virtual objects to be closer than intended. Surprisingly, the semi-automatic calibration method produced more accurate registration vertically and in perceived object distance overall. User assessed quality values were also the highest for Recycled INDICA, particularly when objects were shown at distance. The results of this study confirm that Recycled INDICA is capable of producing equal or superior on-screen registration compared to common OST HMD calibration methods. We also identify a potential hazard in using reprojection error as a quantitative analysis technique to predict registration accuracy. We conclude with discussing the further need for examining INDICA calibration in binocular HMD systems, and the present possibility for creation of a closed-loop continuous calibration method for OST Augmented Reality.},
  doi      = {10.1109/TVCG.2015.2391856},
  keywords = {Calibration,INDICA,OST HMD,SPAAM,eye tracking,user study},
}

@Article{Sandor2016a,
  author  = {Sandor, Christian and Teather, Robert and Suma, Evan and Johnsen, Kyle},
  title   = {{SUI 2016 welcome message}},
  journal = {SUI 2016 - Proceedings of the 2016 Symposium on Spatial User Interaction},
  year    = {2016},
  isbn    = {9781450340687},
}

@InProceedings{Dey2012,
  author    = {Dey, Arindam and Jarvis, Graeme and Sandor, Christian and Reitmayr, Gerhard},
  title     = {{Tablet versus phone: Depth perception in handheld augmented reality}},
  booktitle = {ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers},
  year      = {2012},
  abstract  = {Augmented Reality (AR) applications on mobile devices like smartphones and tablet computers have become increasingly popular. In this paper, for the first time in the AR domain, we present: (1) the influence of different handheld displays and (2) the exocentric depth perception. Unlike egocentric depth perception, exocentric depth perception has not been investigated in AR. {\textcopyright} 2012 IEEE.},
  doi       = {10.1109/ISMAR.2012.6402556},
  isbn      = {9781467346603},
  keywords  = {Augmented Reality,Depth Perception,Handheld Displays,Outdoor Environment,User Evaluation,X-ray Visualization},
}

@InProceedings{Santos2016b,
  author    = {Santos, Marc Ericson C. and Rompapas, Damien Constantine and Nishiki, Yoshinari and Taketomi, Takafumi and Yamamoto, Goshiro and Sandor, Christian and Kato, Hirokazu},
  title     = {{The COMPASS framework for digital entertainment: Discussing augmented reality activities for scouts}},
  booktitle = {ACM International Conference Proceeding Series},
  year      = {2016},
  abstract  = {Entertainment is challenging to observe, especially with children, due to limited analytical tools. In response, we present a modified framework for entertainment computing, COMPASS - COmbined Mental, PhysicAl, Social and Spatial factors, which we use to analyze augmented reality activities for cub scouts.},
  doi       = {10.1145/3001773.3001799},
  isbn      = {9781450347730},
  keywords  = {Augmented reality,Children-computer interaction,Evaluation framework,User experience},
}

@InProceedings{Pucihar2019,
  author    = {Pucihar, Klen {\v{C}}opi{\v{c}} and Huerst, Wolfgang and Kato, Hirokazu and Sandor, Christian and Plopski, Alexander and Leiva, Luis A. and Kljun, Matjaz{\v{c}} and Taketomi, Takafumi},
  title     = {{The missing interface: Micro-gestures on augmented objects}},
  booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
  year      = {2019},
  abstract  = {Augmenting arbitrary physical objects with digital content leads to the missing interface problem, because those objects were never designed to incorporate such digital content and so they lack a user interface. A review of related work reveals that current approaches fail due to limited detection fidelity and spatial resolution. Our proposal, based on Google Soli's radar sensing technology, is designed to detect micro-gestures on objects with sub-millimeter precision. Preliminary results with a custom gesture set show that Soli's core features and traditional machine learning models (Random Forest and Support Vector Machine) do not lead to robust recognition accuracy, and so more advanced techniques should be used instead, possibly incorporating additional sensor features.},
  doi       = {10.1145/3290607.3312986},
  isbn      = {9781450359719},
  keywords  = {Augmented Reality,Google Soli,Micro-gesture Recognition,Millimeter-wave Radar},
}

@InProceedings{Eck2010,
  author    = {Eck, Ulrich and Sandor, Christian},
  title     = {{Tint: Towards a pure python augmented reality framework}},
  booktitle = {Proceedings of the Third Workshop on Software Engineering and Architectures for Realtime Interactive Systems},
  year      = {2010},
  abstract  = {This paper describes our software framework, called TINT, which is targeted towards rapid development of augmented reality appli- cations. It is entirely written in the Python programming language and optimized with compiled modules to achieve realtime perfor- mance without sacrificing simplicity and maintainability. The de- sign goal for TINT is, to make it possible to develop applications and framework components in pure Python code. This increases the productivity of the developer and is less error-prone. TINT implements a set of components: dataflow network with hardware sensors, video-capturing, record / playback functionality, presentation library with compositing, networking, and application classes. In this paper we give a high level overview of TINT and compare it with existing frameworks and libraries. We also elaborate in detail on our techniques to support realtime augmented reality applica- tions written in pure Python and give some examples its usage},
}

@InProceedings{Santos2015a,
  author    = {Santos, Marc Ericson C. and Taketomi, Takafumi and Yamamoto, Goshiro and Rodrigo, Ma Mercedes T. and Sandor, Christian and Kato, Hirokazu},
  title     = {{Toward guidelines for designing handheld augmented reality in learning support}},
  booktitle = {Workshop Proceedings of the 23rd International Conference on Computers in Education, ICCE 2015},
  year      = {2015},
  abstract  = {Developing systems using emerging technology such as augmented reality is difficult because there are limited guidelines to inform developers during the design process. In particular, there are no established guidelines for learning support systems based on handheld augmented reality. To gather such design guidelines, we first summarize existing guidelines for handheld augmented reality in other fields of application. We then provide our synthesis of these guidelines into five design guidelines. We share our own experience of how we observed these guidelines in developing FlipPin -A handheld augmented reality system for learning new vocabulary. We then propose an additional guideline based on our experience.},
  isbn      = {9784990801472},
  keywords  = {Design guidelines,Handheld augmented reality,Learning support,Mobile devices},
}

@Article{Santos2015,
  author   = {Santos, Marc Ericson C. and Polvi, Jarkko and Taketomi, Takafumi and Yamamoto, Goshiro and Sandor, Christian and Kato, Hirokazu},
  title    = {{Toward Standard Usability Questionnaires for Handheld Augmented Reality}},
  journal  = {IEEE Computer Graphics and Applications},
  year     = {2015},
  issn     = {02721716},
  abstract = {Usability evaluations are important to improving handheld augmented reality (HAR) systems. However, no standard questionnaire considers perceptual and ergonomic issues found in HAR. The authors performed a systematic literature review to enumerate these issues. Based on these issues, they created a HAR usability scale that consists of comprehensibility and manipulability scales. These scales measure general system usability, ease of understanding the information presented, and ease of handling the device. The questionnaires' validity and reliability were evaluated in four experiments, and the results show that the questionnaires consistently correlate with other subjective and objective measures of usability. The questionnaires also have good reliability based on the Cronbach's alpha. Researchers and professionals can directly use these questionnaires to evaluate their own HAR applications or modify them with the insights presented in this article.},
  doi      = {10.1109/MCG.2015.94},
  keywords = {augmented reality,computer graphics,handheld augmented reality,information interfaces,usability engineering,virtual reality},
}

@InProceedings{Kulas2004,
  author    = {Kulas, Christian and Sandor, Christian and Klinker, Gudrun},
  title     = {{Towards a Development Methodology for Augmented Reality User Interfaces}},
  booktitle = {Proc. of the International Workshop exploring the Design and Engineering of Mixed Reality Systems - MIXER 2004},
  year      = {2004},
  abstract  = {In this paper we describe why we believe that the development of Augmented Reality user interfaces requires special attention and cannot be efficiently handled with neither existing tools nor traditional development processes. A new methodology comprising both a new process and better tools might be the best action to take. A requirement analysis on issues regarding the process, the user groups involved, and the supportive tools for Augmented Reality user interface development is presented. This opens up a number of research challenges covering the tools, the process and the methodology as a whole. A new development process which is a first attempt to meet the newly found challenges is briefly outlined. This process relies on high parallelism and extends previously learned insights with usability evaluation matters. Following, our complementary proposed tool set gets introduced in detail. This set again profited mostly from new tools fitting in the usability engineering realm, which so far has been mostly ignored in the field of Augmented Reality. First steps towards a development methodology for the creation of Augmented Reality user interfaces, tackling the found requirements, are thereby made. Finally, our planed future steps are shown, meant to bring the development methodology further along, by solving important, but achievable, remaining challenges.},
}

@InProceedings{Krichenbauer2014,
  author    = {Krichenbauer, Max and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
  title     = {{Towards augmented reality user interfaces in 3D media production}},
  booktitle = {ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings},
  year      = {2014},
  abstract  = {For this demo, we present an Augmented Reality (AR) User Interface (UI) for the 3D design software Autodesk Maya, aimed at professional media creation. A user wears a head-mounted display (HMD) and thin cotton gloves which allow him to interact with virtual 3D models in the work area. Additional viewers can see the video stream on a projector and thus share the users view. Both head and hand positions are tracked from the HMD video stream, and an inertial measurement unit (IMU) and conductive materials on the gloves allow interaction with virtual objects. This system is built using Autodesk Maya - a professional 3D software package commonly used in the media industry - and aims to fulfill the requirements of professional 3D design work which we identified in our paper of the same title. While still an early prototype, it was already tested with media professionals to evaluate our approach.},
  doi       = {10.1109/ISMAR.2014.6948484},
  isbn      = {9781479961849},
}

@Article{Rompapas2019,
  author   = {Rompapas, Damien Constantine and Sandor, Christian and Plopski, Alexander and Saakes, Daniel and Shin, Joongi and Taketomi, Takafumi and Kato, Hirokazu},
  title    = {{Towards large scale high fidelity collaborative augmented reality}},
  journal  = {Computers {\&} Graphics},
  year     = {2019},
  issn     = {00978493},
  abstract = {In recent years, there has been an increasing amount of Collaborative Augmented Reality (CAR) experiences, classifiable by the deployed scale and the fidelity of the experience. In this paper, we create HoloRoyale, the first large scale high fidelity (LSHF) CAR experience. We do this by first exploring the LSHF CAR design space, drawing on technical implementations and design aspects from AR and video games. We then create and implement a software architecture that improves the accuracy of synchronized poses between multiple users. Finally, we apply our target experience and technical implementation to the explored design space. A core design component of HoloRoyale is the use of visual repellers as crowd control elements to guide players away from undesired areas. To evaluate the effectiveness of the employed visual repellers in a LSHF CAR context we conducted a user study, deploying HoloRoyale in a 12.500 m2 area. The results from the user study suggest that visual repellers are effective crowd control elements that do not significantly impact the user's overall immersion. Overall our main contribution is the exploration of a design space, discussing several means to address the challenges of LSHF CAR, the creation of a system capable of LSHF CAR interactions along with an experience that has been fitted to the design space, and an indepth study that verifies a key design aspect for LSHF CAR. As such, our work is the first to explore the domain of LSHF CAR and provides insight into designing experiences in other AR domains.},
  doi      = {10.1016/j.cag.2019.08.007},
}

@InProceedings{Kaplan2018,
  author    = {Kaplan, Oral and Yarnarnoto, Goshiro and Taketomi, Takafumi and Yoshltake, Yasuhide and Plopski, Alexander and Sandor, Christian and Sandor, Christian},
  title     = {{Towards Situated Knee Trajectory Visualization for Self Analysis in Cycling}},
  booktitle = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
  year      = {2018},
  abstract  = {Inflammation, stiffness, and swelling are frequently reported symptoms of patellar tendinitis among cyclists; making knee pain a consistently observed overuse injury in cycling. In this paper, we investigate the applicability of a knee trajectory visualization to self-Analysis for increasing awareness of movement patterns leading to injuries. We briefly explain overuse injuries and patellar instability, describe the experiments we did with cyclists for gathering requirements, and finally illustrate an augmented reality concept. We also show two different types of visualizations with participant opinions; one being conventional and other being a video-based one and discuss how situated visualizations can be utilized for improving self awareness to injury causes.},
  doi       = {10.1109/VR.2018.8446212},
  isbn      = {9781538633656},
  keywords  = {Human-centered computing,Information visualization,Mixed / augmented reality,Visualization design and evaluation methods},
}

@InProceedings{Caluya2018,
  author    = {Caluya, Nicko R. and Plopski, Alexander and Ty, Jayzon F. and Sandor, Christian and Taketomi, Takafumi and Kato, Hirokazu},
  title     = {{Transferability of Spatial Maps: Augmented Versus Virtual Reality Training}},
  booktitle = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
  year      = {2018},
  abstract  = {Work space simulations help trainees acquire skills necessary to perform their tasks efficiently without disrupting the workflow, forgetting important steps during a procedure, or the location of important information. This training can be conducted in Augmented and Virtual Reality (AR, VR) to enhance its effectiveness and speed. When the skills are transferred to the actual application, it is referred to as positive training transfer. However, thus far, it is unclear which training, AR or VR, achieves better results in terms of positive training transfer. We compare the effectiveness of AR and VR for spatial memory training in a control-room scenario, where users have to memorize the location of buttons and information displays in their surroundings. We conducted a within-subject study with 16 participants and evaluated the impact the training had on short-Term and long-Term memory. Results of our study show that VR outperformed AR when tested in the same medium after the training. In a memory transfer test conducted two days later AR outperformed VR. Our findings have implications on the design of future training scenarios and applications.},
  doi       = {10.1109/VR.2018.8447561},
  isbn      = {9781538633656},
  keywords  = {Evaluation/methodology,H.5.1-Information Interfaces and Presentation: Multimedia Information Systems-Artificial,H.5.2-Information Interfaces and Presentation: Multimedia Information Systems-Ergonomics,Theory and methods,and virtual realities,augmented},
}

@InProceedings{Eckhoff2018,
  author    = {Eckhoff, Daniel and Sandor, Christian and Lins, Christian and Eck, Ulrich and Kalkofen, Denis and Hein, Andreas},
  title     = {{TutAR: Augmented Reality Tutorials for Hands-only Procedures}},
  booktitle = {Proceedings of the 16th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry},
  year      = {2018},
  abstract  = {With Augmented Reality (AR) on Optical See-Through Head-Mounted Displays (OST-HMD), users can observe the real world and computer graphics at the same time. In this work, we present the design and implementation of TutAR, a pipeline that semi-automatically creates AR tutorials of 2D RGB videos with hands-only procedures such as cardiopulmonary resuscitation (CPR). TutAR extracts relevant 3D hand motion from the input video. The derived hand motion will be displayed as an animated 3D hand relative to the human body and plays synchronously with the motion in the video on an OST-HMD.},
  doi       = {10.1145/3284398.3284399},
  isbn      = {9781450360876},
  keywords  = {Augmented Reality,Medical Education,Motion extraction,Video Tutorials},
}

@InProceedings{Eckhoff2018a,
  author    = {Eckhoff, Daniel and Sandor, Christian and Kalkoten, Denis and Eck, Ulrich and Lins, Christian and Hein, Andreas},
  title     = {{TutAR: Semi-Automatic Generation of Augmented Reality Tutorials for Medical Education}},
  booktitle = {Adjunct Proceedings of the IEEE International Conference on Mixed and Augmented Reality},
  year      = {2018},
  abstract  = {With Augmented Reality (AR) on Optical-See-Through-Head-Mounted Displays (OST-HMD), users can observe the real world and computer graphics at the same time. In this work, we present TutAR, a pipeline that semi-automatically creates AR tutorials out of 2D RGB videos. TutAR extracts relevant 3D hand motion from the input video. The derived motion will be displayed as an animated 3D hand relative to the human body and plays synchronously with the motion in the video on an OST-HMD.},
  doi       = {10.1109/ISMAR-Adjunct.2018.00131},
  isbn      = {9781538675922},
  keywords  = {Augmented Reality,Motion extraction,Video tutorials},
}

@InProceedings{Santos2014,
  author    = {Santos, Marc Ericson C. and Polvi, Jarkko and Taketomi, Takafumi and Yamamoto, Goshiro and Sandor, Christian and Kato, Hirokazu},
  title     = {{Usability scale for handheld augmented reality}},
  booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  year      = {2014},
  abstract  = {Handheld augmented reality (HAR) applications must be carefully designed and improved based on user feedback to sustain commercial use. However, no standard questionnaire considers perceptual and ergonomic issues found in HAR. We address this issue by creating a HAR Usability Scale (HARUS). To create HARUS, we performed a systematic literature review to enumerate user-reported issues in HAR applications. Based on these issues, we created a questionnaire measuring manipulability - the ease of handling the HAR system, and comprehensibility - the ease of understanding the information presented by HAR. We then provide evidences of validity and reliability of the HARUS questionnaire by applying it to three experiments. The results show that HARUS consistently correlates with other subjective and objective measures of usability, thereby supporting its concurrent validity. Moreover, HARUS obtained a good Cronbach's alpha in all three experiments, thereby demonstrating internally consistency. HARUS, as well as its decomposition into individual manipulability and comprehensibility scores, are evaluation tools that researchers and professionals can use to analyze their HAR applications. By providing such a tool, they can gain quality feedback from users to improve their HAR applications towards commercial success.},
  doi       = {10.1145/2671015.2671019},
  isbn      = {9781450332538},
  keywords  = {Augmented reality,Evaluation method,Handheld devices,Usability,User studies},
}

@InProceedings{Cook2018,
  author    = {Cook, Trey and Phillips, Nate and Massey, Kristen and Plopski, Alexander and Sandor, Christian},
  title     = {{User Preference for SharpView-Enhanced Virtual Text during Non-Fixated Viewing}},
  booktitle = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
  year      = {2018},
  abstract  = {For optical see-Through head-mounted displays, the mismatch between a display's focal length and the real world scene inadvertently prevents users from simultaneously focusing on the presented virtual content and the scene. It has been shown that it is possible to ameliorate the out-of-focus blur for images with a known focus distance, by applying an algorithm called Sharp View. However, it remains unclear if Sharp View also improves the readability and clarity of text rendered on the display. In this study, we investigate whether users reported increased text clarity when Sharp View was applied to a text label, and how the focal demand of the display, the focal distance to real world content, and gaze condition affect the result. Our results indicate that, in non-fixated viewing, there is a significant user preference for Sharp View-enhanced text strings.},
  doi       = {10.1109/VR.2018.8446058},
  isbn      = {9781538633656},
}

@InProceedings{Elsayed2013,
  author    = {Elsayed, Neven A.M. and Sandor, Christian and Laga, Hamid},
  title     = {{Visual analytics in Augmented Reality}},
  booktitle = {2013 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2013},
  year      = {2013},
  abstract  = {In the last decade, Augmented Reality has become more mature and is widely adopted on mobile devices. Exploring the available information of a user's environment is one of the key applications. However, current mobile Augmented Reality interfaces are very limited compared to the recently emerging big data exploration tools for desktop computers. Our vision is to bring powerful Visual Analytic tools to mobile Augmented Reality. {\textcopyright} 2013 IEEE.},
  doi       = {10.1109/ISMAR.2013.6671817},
  isbn      = {9781479928699},
  keywords  = {context-based,mixed reality,visualisation},
}

@InProceedings{Sandor2004,
  author    = {Sandor, Christian and Bell, Blaine and Olwal, Alex and Temiyabutr, Surabhan and Feiner, Steven},
  title     = {{Visual end user configuration of hybrid user interfaces}},
  booktitle = {Proceedings of the ACM SIGMM 2004 Workshop on Effective Telepresence - ETP 2004},
  year      = {2004},
  abstract  = {Hybrid user interfaces are a promising paradigm for human-computer interaction, employing a range of displays and devices. However, most experimental hybrid user interfaces use a relatively rigid configuation. Our demo explores the possibilities of end users configuring the setup of a hybrid user interface, using novel interaction techniques and visualizations, based on a shared augmented reality. Copyright 2004 ACM.},
  doi       = {10.1145/1026776.1026796},
  isbn      = {1581139330},
  keywords  = {DWARF,End User Programming,Hybrid User Interfaces,View management},
}

@InProceedings{White2012,
  author   = {White, Sean and Kalkofen, Denis and Sandor, Christian},
  title    = {{Visualization in mixed reality environments}},
  year     = {2012},
  abstract = {Mixed and Augmented Reality displays extend the user's perception with computer generated information. This information is typically registered in three-dimensional space, and related to objects and places in the physical world. While individual annotation of objects has historically been a topic of MR research, visualization incorporating multiple related data points or models provides a variety of new research challenges in systems and techniques. For example, photorealistic augmented reality visualization presents data by adapting additionally presented imagery to the real world condition while illustrative visualization techniques aim at enhancing the understanding of augmented scenarios by carefully combining and mediating real and virtual data. Situated visualization techniques present virtual representations of data in relevant locations in the physical scene. A challenge in many of these techniques is the need to correctly communicate the relationships between physical imagery and virtual data.},
  doi      = {10.1109/ismar.2011.6162861},
}

@InCollection{Kalkofen2011,
  author    = {Kalkofen, Denis and Sandor, Christian and White, Sean and Schmalstieg, Dieter},
  title     = {{Visualization Techniques for Augmented Reality}},
  booktitle = {Handbook of Augmented Reality},
  year      = {2011},
  abstract  = {Visualizations in real world environments benefit from the visual interaction between real and virtual imagery. However, compared to traditional visualizations, a number of problems have to be solved in order to achieve effective visualizations within Augmented Reality (AR). This chapter provides an overview of techniques to handle the main obstacles in AR visualizations. It discusses spatial integration of virtual objects within real world environments, techniques to rearrange objects within mixed environments, and visualizations which adapt to its environmental context.},
  doi       = {10.1007/978-1-4614-0064-6_3},
}

@Article{Swan2017,
  author   = {Swan, J. Edward and Kuparinen, Liisa and Rapson, Scott and Sandor, Christian},
  title    = {{Visually Perceived Distance Judgments: Tablet-Based Augmented Reality Versus the Real World}},
  journal  = {International Journal of Human-Computer Interaction},
  year     = {2017},
  issn     = {15327590},
  abstract = {Does visually perceived distance differ when objects are viewed in augmented reality (AR), as opposed to the real world? What are the differences? These questions are theoretically interesting, and the answers are important for the development of many tablet- and phone-based AR applications, including mobile AR navigation systems. This article presents a thorough literature review of distance judgment experimental protocols, and results from several areas of perceptual psychology. In addition to distance judgments of real and virtual objects, this section also discusses previous work in measuring the geometry of virtual picture space and considers how this work might be relevant to tablet AR. Then, the article presents the results of two experiments. In each experiment, observers bisected egocentric distances of 15 and 30 m in tablet-based AR and in the real world, in both indoor corridor and outdoor field environments. In AR, observers bisected the distances to virtual humans, while in the real world, they bisected the distances to real humans. This is the first reported research that directly compares distance judgments of real and virtual objects in a tablet AR system. Four key findings were: (1) In AR, observers expanded midpoint intervals at 15 m, but compressed midpoints at 30 m. (2) Observers were accurate in the real world. (3) The environmental setting—corridor or open field—had no effect. (4) The picture perception literature is important in understanding how distances are likely judged in tablet-based AR. Taken together, these findings suggest the depth distortions that AR application developers should expect with mobile and especially tablet-based AR.},
  doi      = {10.1080/10447318.2016.1265783},
}

@InProceedings{Eck2013a,
  author    = {Eck, Ulrich and Sandor, Christian and Laga, Hamid},
  title     = {{Visuo-Haptic Augmented Reality runtime environment for medical training}},
  booktitle = {2013 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2013},
  year      = {2013},
  abstract  = {During the last decade, Visuo-Haptic Augmented Reality (VHAR) systems have emerged that enable users to see and touch digital information that is embedded in the real world. They pose unique problems to developers, including the need for precise augmentations, accurate colocation of haptic devices, and efficient concurrent processing of multiple, realtime sensor inputs to achieve low latency. We think that this complexity is one of the main reasons, why VHAR technology has only been used in few user interface research projects. The proposed project's main objective is to pioneer the development of a widely applicable VHAR runtime environment, which meets the requirements of realtime, low latency operation with precise co-location, haptic interaction with deformable bodies, and realistic rendering, while reducing the overall cost and complexity for developers. A further objective is to evaluate the benefits of VHAR user interfaces with a focus on medical training applications, so that creators of future medical simulators or other haptic applications recognize the potential of VHAR. {\textcopyright} 2013 IEEE.},
  doi       = {10.1109/ISMAR.2013.6671816},
  isbn      = {9781479928699},
  keywords  = {augmented reality,dataflow architectures,haptic interaction,medical training,mixed reality,physically-based simulation},
}

@InProceedings{Sandor2007,
  author    = {Sandor, Christian and Uchiyama, Shinji and Yamamoto, Hiroyuki},
  title     = {{Visuo-haptic systems: Half-mirrors considered harmful}},
  booktitle = {Proceedings - Second Joint EuroHaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, World Haptics 2007},
  year      = {2007},
  abstract  = {In recent years, systems that allow users to see and touch virtual objects in the same space (visuo-haptic systems) are being investigated. Most research projects are employing a half-mirror, while few use a video see-through, head-mounted display (HMD). The work presented in this paper points out advantages of the HMD-based approach. First, we present an experiment that analyzes human performance in a target acquisition task. We have compared a half-mirror system with an HMD system. Our main finding is, that a half-mirror significantly reduces performance. Second, we present an HMD-based painting application, which introduces new interaction techniques that could not be implemented with a half-mirror display. We believe that our findings could inspire other researchers, employing a half-mirror, to reconsider their approach. {\textcopyright} 2007 IEEE.},
  doi       = {10.1109/WHC.2007.125},
  isbn      = {0769527388},
}

@Article{Sandor2015,
  author  = {Sandor, Christian and Lindeman, Robert and Mayol-Cuevas, Walterio},
  title   = {{Welcome message from the ISMAR 2015 science and technology program chairs}},
  journal = {Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2015},
  year    = {2015},
  doi     = {10.1109/ISMAR.2015.5},
  isbn    = {9781467376600},
}

@Article{Thomas2009,
  author   = {Thomas, Bruce H. and Sandor, Christian},
  title    = {{What wearable augmented reality can do for you}},
  journal  = {IEEE Pervasive Computing},
  year     = {2009},
  issn     = {15361268},
  abstract = {This paper focuses on the present and near-future possibilities and technologies for wearable augmented reality. The paper provides an overview of the concept of wearable augmented reality and describes a current state-of-the-art system Tinmith. A number of benefits are presented, such as in situ information presentation, hands-free operation, ability to multitask, and navigation aids. The following five major application domains are examined as a means of highlighting the effectiveness of this technology: consumer help, entertainment, manufacturing, medicine, and navigation. The article provides some insights into technological hurdles yet to be overcome.},
  doi      = {10.1109/MPRV.2009.38},
  keywords = {And virtual realities,Augmented,Computer System Implementation: Wearable Computers,Information Interfaces and Representation (HCI): Artificial},
}

@Comment{jabref-meta: databaseType:bibtex;}
